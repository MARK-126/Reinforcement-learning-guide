# Gu√≠a de Inicio R√°pido

¬°Bienvenido! Esta gu√≠a te ayudar√° a empezar con Reinforcement Learning en 30 minutos.

## üöÄ Configuraci√≥n R√°pida (5 minutos)

### 1. Clonar el Repositorio

```bash
git clone https://github.com/MARK-126/Reinforcement-learning-guide.git
cd Reinforcement-learning-guide
```

### 2. Crear Entorno Virtual

```bash
# Crear entorno
python -m venv venv

# Activar (Linux/Mac)
source venv/bin/activate

# Activar (Windows)
venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

Esto instalar√°:
- `gymnasium` - Ambientes de RL
- `torch` - Deep Learning
- `numpy` - Computaci√≥n num√©rica
- `matplotlib` - Visualizaci√≥n
- Y m√°s...

### 4. Verificar Instalaci√≥n

```bash
python -c "import gymnasium; import torch; print('‚úì Listo para empezar!')"
```

## üìñ Tu Primera Sesi√≥n de RL (25 minutos)

### Parte 1: Entender los Conceptos (10 minutos)

Lee estos archivos en orden:

1. **[Introducci√≥n al RL](01_fundamentos/introduccion.md)** (5 min)
   - ¬øQu√© es RL?
   - Agente, ambiente, recompensa
   - Exploraci√≥n vs Explotaci√≥n

2. **[MDPs B√°sicos](01_fundamentos/mdp.md)** (5 min - solo introducci√≥n)
   - Estados y acciones
   - La propiedad de Markov

### Parte 2: Ejecutar tu Primer Agente (10 minutos)

```bash
# Navegar al ejemplo de CartPole
cd 04_ejemplos/cartpole

# Ejecutar Q-Learning
python cartpole_qlearning.py
```

Esto:
1. ‚úÖ Entrenar√° un agente por 500 episodios (~2 minutos)
2. ‚úÖ Mostrar√° progreso en consola
3. ‚úÖ Evaluar√° el agente entrenado
4. ‚úÖ Generar√° gr√°ficos de resultados

**Observa**:
- C√≥mo la recompensa mejora con el tiempo
- C√≥mo epsilon decae (menos exploraci√≥n)
- El agente aprende a balancear el poste

### Parte 3: Entender el C√≥digo (5 minutos)

Abre `cartpole_qlearning.py` y observa:

```python
# 1. Crear agente
agent = QLearningAgent(
    n_actions=2,        # Izquierda o derecha
    alpha=0.1,          # Learning rate
    gamma=0.99,         # Discount factor
    epsilon=1.0         # Exploration rate
)

# 2. Entrenar
for episode in range(n_episodes):
    state = env.reset()
    
    for step in range(max_steps):
        # Seleccionar acci√≥n
        action = agent.get_action(state)
        
        # Ejecutar acci√≥n
        next_state, reward, done = env.step(action)
        
        # Actualizar Q-values (¬°AQU√ç EST√Å LA MAGIA!)
        agent.update(state, action, reward, next_state, done)
        
        state = next_state
        if done:
            break
```

**Conceptos clave**:
- **Q-Learning**: Aprende valor de estado-acci√≥n
- **Œµ-greedy**: Balance exploraci√≥n/explotaci√≥n
- **Update**: Ecuaci√≥n de Bellman para mejorar estimaciones

## üéØ Pr√≥ximos Pasos

### Opci√≥n A: Profundizar en Teor√≠a (Recomendado)

1. Lee [MDPs](01_fundamentos/mdp.md) completo
2. Lee [Ecuaciones de Bellman](01_fundamentos/bellman.md)
3. Lee [Value Functions](01_fundamentos/value_policy.md)

**Tiempo**: 2-3 horas  
**Resultado**: Entender√°s la base matem√°tica

### Opci√≥n B: M√°s Pr√°ctica

1. Modifica `cartpole_qlearning.py`:
   ```python
   # Experimenta cambiando:
   alpha = 0.5        # ¬øM√°s r√°pido?
   gamma = 0.95       # ¬øDiferente?
   epsilon_decay = 0.99  # ¬øM√°s exploraci√≥n?
   ```

2. Prueba otros ambientes:
   ```python
   env = gym.make('FrozenLake-v1')  # M√°s simple
   env = gym.make('MountainCar-v0')  # M√°s dif√≠cil
   ```

3. Implementa SARSA:
   - Ve a `02_algoritmos_clasicos/temporal_difference/sarsa.py`
   - Compara con Q-Learning

**Tiempo**: 2-3 horas  
**Resultado**: Intuici√≥n pr√°ctica de RL

### Opci√≥n C: Deep RL

1. Ejecuta DQN en CartPole:
   ```bash
   cd 03_deep_rl/dqn
   python dqn_basic.py
   ```

2. Compara con Q-Learning:
   - ¬øCu√°l es m√°s r√°pido?
   - ¬øCu√°l obtiene mejor resultado?
   - ¬øPor qu√©?

**Tiempo**: 1-2 horas  
**Resultado**: Introducci√≥n a Deep RL

## üõ†Ô∏è Herramientas √ötiles

### Visualizar Agente Entrenado

```python
import gymnasium as gym

env = gym.make('CartPole-v1', render_mode='human')
# ... entrenar agente ...

# Visualizar
for episode in range(5):
    state, _ = env.reset()
    done = False
    
    while not done:
        action = agent.get_action(state, training=False)
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

env.close()
```

### Debuggear Q-Values

```python
# Ver Q-values para un estado
state = (0, 0, 0, 0)  # Estado ejemplo
print(f"Q-values: {agent.Q[state]}")
print(f"Mejor acci√≥n: {np.argmax(agent.Q[state])}")
```

### Guardar/Cargar Agente

```python
import pickle

# Guardar
with open('agent.pkl', 'wb') as f:
    pickle.dump(agent.Q, f)

# Cargar
with open('agent.pkl', 'rb') as f:
    agent.Q = pickle.load(f)
```

## üìö Recursos de Referencia R√°pida

### Ecuaciones Importantes

**Q-Learning Update**:
```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
```

**SARSA Update**:
```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ Q(s',a') - Q(s,a)]
```

**Œµ-greedy**:
```python
if random() < epsilon:
    return random_action()
else:
    return argmax(Q[state])
```

### Hiperpar√°metros T√≠picos

| Par√°metro | S√≠mbolo | Valor T√≠pico | Descripci√≥n |
|-----------|---------|--------------|-------------|
| Learning rate | Œ± | 0.1 - 0.01 | Qu√© tan r√°pido aprende |
| Discount factor | Œ≥ | 0.99 - 0.95 | Importancia del futuro |
| Epsilon start | Œµ‚ÇÄ | 1.0 | Exploraci√≥n inicial |
| Epsilon end | Œµ_min | 0.01 | Exploraci√≥n final |
| Epsilon decay | - | 0.995 | Velocidad de decaimiento |

## ‚ùì Troubleshooting

### "No module named 'gymnasium'"

```bash
pip install gymnasium
```

### El agente no aprende

1. **Verifica hiperpar√°metros**: Œ± muy bajo o Œ≥ muy alto
2. **Exploraci√≥n**: Aseg√∫rate que Œµ est√© decayendo
3. **Recompensas**: Verifica que las recompensas tengan sentido
4. **Episodios**: Tal vez necesitas m√°s episodios

### Entrenamiento muy lento

1. **Reduce episodios**: Empieza con 100-200
2. **Discretizaci√≥n**: Si usas Q-Learning, reduce bins
3. **Max steps**: Limita steps por episodio

### Resultados inconsistentes

Esto es normal en RL! Para resultados m√°s estables:

```python
# Fija random seed
import random
import numpy as np

random.seed(42)
np.random.seed(42)
```

## üéì Plan de 7 D√≠as

### D√≠a 1: Setup y Primer Agente
- ‚úÖ Configurar entorno
- ‚úÖ Ejecutar CartPole
- ‚úÖ Entender el c√≥digo

### D√≠a 2: Teor√≠a Fundamental
- üìñ Leer fundamentos completos
- üìñ Hacer ejercicios mentales

### D√≠a 3: Q-Learning Profundo
- üíª Implementar Q-Learning desde cero
- üíª Probar en FrozenLake

### D√≠a 4: Comparar Algoritmos
- üíª SARSA vs Q-Learning
- üìä Comparar resultados

### D√≠a 5: Intro a Deep RL
- üíª Ejecutar DQN
- üìñ Entender diferencias

### D√≠a 6: Proyecto Personal
- üöÄ Resolver un ambiente nuevo
- üöÄ Experimentar con hiperpar√°metros

### D√≠a 7: Documentar y Compartir
- üìù Escribir sobre lo aprendido
- üåü Compartir resultados

## üåü Consejos de Expertos

1. **Empieza simple**: No saltes directo a Deep RL
2. **Visualiza**: Grafica todo (rewards, Q-values, pol√≠ticas)
3. **Experimenta**: Cambia hiperpar√°metros y observa
4. **Lee c√≥digo**: Las implementaciones son mejores maestros que teor√≠a sola
5. **S√© paciente**: RL es dif√≠cil, los agentes no siempre aprenden a la primera

## üìû Ayuda

¬øAtascado? 
- üêõ [Reporta un issue](https://github.com/MARK-126/Reinforcement-learning-guide/issues)
- üí¨ Consulta [CONTRIBUTING.md](CONTRIBUTING.md)
- üìö Lee la [documentaci√≥n completa](README.md)

---

**¬°Feliz aprendizaje! üöÄü§ñ**

La mejor forma de aprender RL es implementando. No te preocupes si no entiendes todo al principio, ¬°eso es completamente normal!
