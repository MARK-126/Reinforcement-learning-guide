# GuÃ­a Completa de Reinforcement Learning

Una guÃ­a completa y estructurada para aprender Reinforcement Learning (Aprendizaje por Refuerzo) desde los fundamentos hasta implementaciones avanzadas.

## ğŸ“š Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Requisitos Previos](#requisitos-previos)
3. [InstalaciÃ³n](#instalaciÃ³n)
4. [Notebooks Interactivos (Ejercicios Fill-in-the-Blank)](#-notebooks-interactivos-ejercicios-fill-in-the-blank)
5. [Estructura del Repositorio](#estructura-del-repositorio)
6. [Ruta de Aprendizaje](#ruta-de-aprendizaje)
7. [Recursos Adicionales](#recursos-adicionales)

## ğŸ¯ IntroducciÃ³n

El Reinforcement Learning es una rama del machine learning donde un agente aprende a tomar decisiones interactuando con un ambiente. A travÃ©s de prueba y error, el agente recibe recompensas o penalizaciones y aprende a maximizar las recompensas a largo plazo.

Este repositorio estÃ¡ diseÃ±ado para proporcionar:
- **TeorÃ­a sÃ³lida**: Conceptos fundamentales explicados claramente
- **Implementaciones prÃ¡cticas**: CÃ³digo funcional de algoritmos clÃ¡sicos y modernos
- **Ejemplos completos**: Proyectos aplicados a problemas reales
- **Recursos curados**: Referencias a papers, libros y cursos

## ğŸ“‹ Requisitos Previos

### Conocimientos Recomendados
- **Python**: ProgramaciÃ³n bÃ¡sica a intermedia
- **MatemÃ¡ticas**:
  - Ãlgebra lineal (vectores, matrices)
  - CÃ¡lculo (derivadas, optimizaciÃ³n)
  - Probabilidad y estadÃ­stica
- **Machine Learning bÃ¡sico**: Conceptos de supervised learning (opcional pero Ãºtil)

### Software Necesario
- Python 3.8 o superior
- pip o conda para gestiÃ³n de paquetes
- Jupyter Notebook (opcional, para ejemplos interactivos)

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone https://github.com/MARK-126/Reinforcement-learning-guide.git
cd Reinforcement-learning-guide
```

### 2. Crear entorno virtual (recomendado)
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Verificar instalaciÃ³n
```bash
python -c "import gym; import torch; print('âœ“ InstalaciÃ³n exitosa')"
```

## ğŸ““ Notebooks Interactivos (Ejercicios Fill-in-the-Blank)

Este repositorio incluye **notebooks interactivos estilo Coursera/DeepLearning.AI** para los algoritmos principales. Estos notebooks estÃ¡n diseÃ±ados con una pedagogÃ­a guiada que incluye:

### CaracterÃ­sticas de los Notebooks

âœ… **Explicaciones intercaladas**: TeorÃ­a mezclada con cÃ³digo
âœ… **Ejercicios fill-in-the-blank**: Completa el cÃ³digo en secciones marcadas
âœ… **Tests automÃ¡ticos**: Valida tu implementaciÃ³n con outputs esperados
âœ… **Visualizaciones embebidas**: GrÃ¡ficos para entender el aprendizaje
âœ… **Formato guiado**: Paso a paso desde conceptos hasta implementaciÃ³n completa

### Notebooks Disponibles

#### 02_algoritmos_clasicos/temporal_difference/
- **Q_Learning_Exercise.ipynb**: Implementa Q-Learning desde cero
  - Ejercicio 1: InicializaciÃ³n de Q-table
  - Ejercicio 2: PolÃ­tica epsilon-greedy
  - Ejercicio 3: Regla de actualizaciÃ³n Q-Learning
  - Ejercicio 4: Loop de entrenamiento completo
  - Tests: `q_learning_tests.py`

- **SARSA_Exercise.ipynb**: Implementa SARSA y compÃ¡ralo con Q-Learning
  - Ejercicio 1: Regla de actualizaciÃ³n SARSA (on-policy)
  - Ejercicio 2: Loop de entrenamiento SARSA
  - ComparaciÃ³n: CliffWalking environment (SARSA vs Q-Learning)
  - Tests: `sarsa_tests.py`

#### 03_deep_rl/dqn/
- **DQN_Exercise.ipynb**: Deep Q-Network con PyTorch
  - Ejercicio 1: Arquitectura de Q-Network
  - Ejercicio 2: Experience Replay Buffer
  - Ejercicio 3: CÃ¡lculo de TD Loss
  - Ejercicio 4: Loop de entrenamiento DQN
  - Tests: `dqn_tests.py`

### CÃ³mo Usar los Notebooks

1. **Abrir el notebook**:
   ```bash
   jupyter notebook 02_algoritmos_clasicos/temporal_difference/Q_Learning_Exercise.ipynb
   ```

2. **Leer las explicaciones**: Cada secciÃ³n tiene teorÃ­a y contexto

3. **Completar los ejercicios**: Busca los comentarios:
   ```python
   # GRADED FUNCTION: nombre_funcion

   def nombre_funcion(params):
       # (approx. 3 lines)
       # Instrucciones claras de quÃ© hacer

       # YOUR CODE STARTS HERE


       # YOUR CODE ENDS HERE

       return resultado
   ```

4. **Ejecutar los tests**: Cada ejercicio tiene una celda de test:
   ```python
   nombre_funcion_test(nombre_funcion)
   ```

5. **Ver el resultado**: Los tests te dirÃ¡n si tu implementaciÃ³n es correcta

### Estructura de un Ejercicio

```python
# GRADED FUNCTION: q_learning_update

def q_learning_update(Q, state, action, reward, next_state, done, alpha, gamma):
    """
    Update Q-table using Q-Learning rule.

    Arguments:
    Q -- Q-table, numpy array of shape (n_states, n_actions)
    state -- current state
    action -- action taken
    reward -- reward received
    next_state -- next state
    done -- boolean, True if next_state is terminal
    alpha -- learning rate
    gamma -- discount factor

    Returns:
    Q -- updated Q-table
    td_error -- TD error (for tracking)
    """
    # (approx. 5-7 lines)
    # Step 1: Get current Q-value
    # Step 2: Calculate TD target (if done: r, else: r + gamma * max Q(s',a'))
    # Step 3: Calculate TD error
    # Step 4: Update Q-value

    # YOUR CODE STARTS HERE
    current_q = Q[state, action]

    if done:
        target_q = reward
    else:
        target_q = reward + gamma * np.max(Q[next_state])

    td_error = target_q - current_q
    Q[state, action] = current_q + alpha * td_error
    # YOUR CODE ENDS HERE

    return Q, td_error
```

### Tests AutomÃ¡ticos

Cada funciÃ³n tiene tests que verifican:
- âœ… **Tipo de datos**: Â¿Devuelve numpy array?
- âœ… **Forma (shape)**: Â¿Dimensiones correctas?
- âœ… **Valores**: Â¿CÃ¡lculos correctos con casos de prueba?

Ejemplo de output de test:
```
Testing q_learning_update...
Non-terminal update:
  Q[0,0] before: 0.0000
  Q[0,0] after: 0.1450
  TD error: 1.4500

Terminal update:
  Q[0,0] after: 0.1000
  TD error: 1.0000

âœ… All tests passed!
```

### Ventajas del Formato Interactivo

1. **Aprendizaje Activo**: Escribes cÃ³digo, no solo lo lees
2. **Feedback Inmediato**: Los tests te dicen si vas bien
3. **ProgresiÃ³n Guiada**: De simple a complejo, paso a paso
4. **ValidaciÃ³n Rigurosa**: Tests con valores esperados exactos
5. **VisualizaciÃ³n**: GrÃ¡ficos para entender el comportamiento

### Diferencia con Archivos `.py` Existentes

| Aspecto | Archivos `.py` | Notebooks `_Exercise.ipynb` |
|---------|----------------|----------------------------|
| CÃ³digo | Completo, listo para ejecutar | Ejercicios para completar |
| Uso | Referencia, lectura | Aprendizaje interactivo |
| Tests | No incluidos | Tests automÃ¡ticos integrados |
| TeorÃ­a | Docstrings | Explicaciones markdown extensas |
| PÃºblico | Avanzado | Principiantes/Intermedios |

**RecomendaciÃ³n**:
- ğŸ“š **Principiantes**: Empiecen con los notebooks `_Exercise.ipynb`
- ğŸ”§ **Avanzados**: Usen los archivos `.py` como referencia o para proyectos

## ğŸ“ Estructura del Repositorio

```
Reinforcement-learning-guide/
â”‚
â”œâ”€â”€ 01_fundamentos/              # Conceptos bÃ¡sicos de RL
â”‚   â”œâ”€â”€ introduccion.md          # QuÃ© es RL, historia, aplicaciones
â”‚   â”œâ”€â”€ mdp.md                   # Procesos de DecisiÃ³n de Markov
â”‚   â”œâ”€â”€ bellman.md               # Ecuaciones de Bellman
â”‚   â””â”€â”€ value_policy.md          # Value functions y polÃ­ticas
â”‚
â”œâ”€â”€ 02_algoritmos_clasicos/      # MÃ©todos tabulares y clÃ¡sicos
â”‚   â”œâ”€â”€ dynamic_programming/     # ProgramaciÃ³n dinÃ¡mica
â”‚   â”‚   â”œâ”€â”€ policy_iteration.py
â”‚   â”‚   â””â”€â”€ value_iteration.py
â”‚   â”œâ”€â”€ monte_carlo/             # MÃ©todos Monte Carlo
â”‚   â”‚   â”œâ”€â”€ mc_prediction.py
â”‚   â”‚   â””â”€â”€ mc_control.py
â”‚   â””â”€â”€ temporal_difference/     # TD Learning
â”‚       â”œâ”€â”€ sarsa.py
â”‚       â”œâ”€â”€ q_learning.py
â”‚       â””â”€â”€ expected_sarsa.py
â”‚
â”œâ”€â”€ 03_deep_rl/                  # Deep Reinforcement Learning
â”‚   â”œâ”€â”€ dqn/                     # Deep Q-Networks
â”‚   â”‚   â”œâ”€â”€ dqn_basic.py
â”‚   â”‚   â”œâ”€â”€ double_dqn.py
â”‚   â”‚   â””â”€â”€ dueling_dqn.py
â”‚   â”œâ”€â”€ policy_gradient/         # MÃ©todos de gradiente de polÃ­tica
â”‚   â”‚   â”œâ”€â”€ reinforce.py
â”‚   â”‚   â”œâ”€â”€ actor_critic.py
â”‚   â”‚   â””â”€â”€ a3c.py
â”‚   â””â”€â”€ advanced/                # Algoritmos avanzados
â”‚       â”œâ”€â”€ ppo.py               # Proximal Policy Optimization
â”‚       â”œâ”€â”€ sac.py               # Soft Actor-Critic
â”‚       â””â”€â”€ td3.py               # Twin Delayed DDPG
â”‚
â”œâ”€â”€ 04_ejemplos/                 # Proyectos completos
â”‚   â”œâ”€â”€ cartpole/                # Balance de pÃ©ndulo invertido
â”‚   â”œâ”€â”€ lunar_lander/            # Aterrizaje lunar
â”‚   â”œâ”€â”€ atari/                   # Juegos de Atari
â”‚   â””â”€â”€ custom_env/              # Crear ambientes personalizados
â”‚
â”œâ”€â”€ 05_recursos/                 # Material adicional
â”‚   â”œâ”€â”€ papers.md                # Papers fundamentales
â”‚   â”œâ”€â”€ libros.md                # Libros recomendados
â”‚   â”œâ”€â”€ cursos.md                # Cursos online
â”‚   â””â”€â”€ datasets.md              # Datasets y benchmarks
â”‚
â”œâ”€â”€ utils/                       # Utilidades y helpers
â”‚   â”œâ”€â”€ plotting.py              # VisualizaciÃ³n de resultados
â”‚   â”œâ”€â”€ wrappers.py              # Gym wrappers Ãºtiles
â”‚   â””â”€â”€ replay_buffer.py         # Experience replay
â”‚
â”œâ”€â”€ requirements.txt             # Dependencias del proyecto
â”œâ”€â”€ .gitignore                   # Archivos a ignorar
â””â”€â”€ README.md                    # Este archivo
```

## ğŸ“ Ruta de Aprendizaje

### Nivel 1: Fundamentos (2-3 semanas)
1. **Semana 1**: Conceptos bÃ¡sicos
   - Â¿QuÃ© es RL? Agente, ambiente, recompensa
   - Procesos de DecisiÃ³n de Markov (MDPs)
   - Value functions y polÃ­ticas
   - Ecuaciones de Bellman

2. **Semana 2-3**: MÃ©todos tabulares
   - ProgramaciÃ³n dinÃ¡mica
   - MÃ©todos Monte Carlo
   - Temporal Difference Learning (TD)
   - Q-Learning y SARSA

### Nivel 2: Deep RL (4-6 semanas)
3. **Semana 4-5**: Deep Q-Learning
   - Redes neuronales bÃ¡sicas con PyTorch
   - DQN (Deep Q-Network)
   - Experience Replay
   - Target Networks
   - Variantes: Double DQN, Dueling DQN

4. **Semana 6-8**: Policy Gradient Methods
   - REINFORCE
   - Actor-Critic
   - Advantage Actor-Critic (A2C/A3C)

### Nivel 3: Avanzado (6-8 semanas)
5. **Semana 9-12**: Algoritmos modernos
   - PPO (Proximal Policy Optimization)
   - DDPG (Deep Deterministic Policy Gradient)
   - TD3 (Twin Delayed DDPG)
   - SAC (Soft Actor-Critic)

6. **Semana 13-16**: TÃ³picos especiales
   - Multi-agent RL
   - Model-based RL
   - Hierarchical RL
   - Inverse RL

### Proyectos PrÃ¡cticos Sugeridos
- [ ] Implementar Q-Learning para resolver un GridWorld
- [ ] Entrenar un agente DQN para jugar CartPole
- [ ] Resolver LunarLander con PPO
- [ ] Crear un ambiente custom y entrenar un agente
- [ ] Implementar un algoritmo desde un paper

## ğŸ“– Recursos Adicionales

### Libros Fundamentales
- **"Reinforcement Learning: An Introduction"** - Sutton & Barto (2018)
  - El libro de texto definitivo en RL
  - [VersiÃ³n gratuita online](http://incompleteideas.net/book/the-book-2nd.html)

- **"Deep Reinforcement Learning Hands-On"** - Maxim Lapan
  - Enfoque prÃ¡ctico con PyTorch
  
- **"Algorithms for Reinforcement Learning"** - Csaba SzepesvÃ¡ri
  - Perspectiva matemÃ¡tica rigurosa

### Cursos Online
- **David Silver's RL Course** (DeepMind)
  - [Videos en YouTube](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)
  
- **CS285: Deep RL** - UC Berkeley (Sergey Levine)
  - [PÃ¡gina del curso](http://rail.eecs.berkeley.edu/deeprlcourse/)

- **Spinning Up in Deep RL** - OpenAI
  - [DocumentaciÃ³n interactiva](https://spinningup.openai.com/)

### Papers Fundamentales
- **DQN**: Mnih et al. (2015) - "Human-level control through deep reinforcement learning"
- **A3C**: Mnih et al. (2016) - "Asynchronous Methods for Deep Reinforcement Learning"
- **PPO**: Schulman et al. (2017) - "Proximal Policy Optimization Algorithms"
- **SAC**: Haarnoja et al. (2018) - "Soft Actor-Critic"

### Comunidades y Foros
- [r/reinforcementlearning](https://www.reddit.com/r/reinforcementlearning/) - Subreddit activo
- [RL Discord](https://discord.gg/xhfNqQv) - Comunidad en Discord
- Stack Overflow con tag `reinforcement-learning`

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas! Si deseas mejorar este repositorio:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/mejora`)
3. Commit tus cambios (`git commit -m 'AÃ±adir nueva explicaciÃ³n'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Abre un Pull Request

## ğŸ“ Notas

- Los ejemplos estÃ¡n diseÃ±ados para ejecutarse en CPU, pero muchos se benefician de GPU
- Algunos ambientes (Atari) requieren instalaciÃ³n adicional de ROMs
- Se recomienda usar Google Colab para entrenamientos largos si no tienes GPU local

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¤ Autor

**MARK-126**

---

â­ Si este repositorio te resulta Ãºtil, considera darle una estrella!

**Â¡Feliz aprendizaje de Reinforcement Learning!** ğŸš€ğŸ¤–