# CONTRIBUTING.md

隆Gracias por tu inter茅s en contribuir a esta gu铆a de Reinforcement Learning! 

## C贸mo Contribuir

### Reportar Errores

Si encuentras un error en el c贸digo, documentaci贸n, o explicaciones:

1. Verifica que el error no haya sido reportado antes en [Issues](https://github.com/MARK-126/Reinforcement-learning-guide/issues)
2. Crea un nuevo issue con:
   - Descripci贸n clara del error
   - Pasos para reproducirlo (si aplica)
   - Comportamiento esperado vs actual
   - Capturas de pantalla si es relevante

### Sugerir Mejoras

驴Tienes ideas para mejorar el contenido?

1. Abre un issue describiendo tu sugerencia
2. Explica por qu茅 ser铆a 煤til
3. Si es posible, proporciona ejemplos o referencias

### Contribuir con C贸digo

#### 1. Fork y Clone

```bash
# Fork el repositorio en GitHub, luego:
git clone https://github.com/TU_USUARIO/Reinforcement-learning-guide.git
cd Reinforcement-learning-guide
```

#### 2. Crea una Rama

```bash
git checkout -b feature/mi-contribucion
```

#### 3. Realiza tus Cambios

- **C贸digo**: Sigue el estilo existente (PEP 8 para Python)
- **Documentaci贸n**: Escribe en espa帽ol, claro y conciso
- **Comentarios**: Explica el "por qu茅", no solo el "qu茅"

#### 4. Prueba tus Cambios

```bash
# Aseg煤rate de que el c贸digo funciona
python tu_archivo.py

# Si a帽adiste dependencias, actualiza requirements.txt
pip freeze > requirements.txt
```

#### 5. Commit y Push

```bash
git add .
git commit -m "Descripci贸n clara de tus cambios"
git push origin feature/mi-contribucion
```

#### 6. Abre un Pull Request

1. Ve a tu fork en GitHub
2. Click en "Compare & pull request"
3. Describe tus cambios detalladamente
4. Espera revisi贸n y feedback

## Gu铆as de Estilo

### Python

- **PEP 8**: Sigue las convenciones de Python
- **Docstrings**: Usa docstrings para funciones y clases
- **Type hints**: Cuando sea posible, a帽ade type hints

```python
def train_agent(env: gym.Env, episodes: int = 1000) -> List[float]:
    """
    Entrena un agente en el ambiente dado.
    
    Args:
        env: Ambiente de Gymnasium
        episodes: N煤mero de episodios de entrenamiento
    
    Returns:
        Lista de recompensas por episodio
    """
    pass
```

### Markdown

- **Encabezados**: Usa jerarqu铆a clara (# > ## > ###)
- **C贸digo**: Especifica el lenguaje en bloques de c贸digo
- **Enlaces**: Usa enlaces descriptivos
- **Listas**: Consistente (- o 1. 2. 3.)

### Estructura de Archivos

```
/algoritmo/
 README.md           # Explicaci贸n del algoritmo
 algorithm.py        # Implementaci贸n
 example.py          # Ejemplo de uso
```

## Tipos de Contribuciones Bienvenidas

###  Documentaci贸n
- Mejorar explicaciones existentes
- A帽adir ejemplos
- Traducir contenido
- Corregir errores tipogr谩ficos

###  C贸digo
- Implementar algoritmos faltantes
- Optimizar c贸digo existente
- A帽adir tests
- Mejorar visualizaciones

###  Contenido Educativo
- Tutoriales nuevos
- Ejercicios pr谩cticos
- Notebooks de Jupyter
- Diagramas y visualizaciones

###  Correcciones
- Bugs en c贸digo
- Errores en matem谩ticas
- Enlaces rotos
- Formato inconsistente

## Contenido que Buscamos

### Algoritmos
- M茅todos tabulares (Monte Carlo, TD, etc.)
- Deep RL (Rainbow DQN, A2C, etc.)
- Policy Gradient avanzados
- Model-based RL
- Multi-agent RL
- Meta-RL

### Ejemplos
- Ambientes cl谩sicos (CartPole, LunarLander, etc.)
- Custom environments
- Aplicaciones reales
- Visualizaciones interactivas

### Recursos
- Papers importantes
- Implementaciones de referencia
- Datasets
- Benchmarks

## Lo que NO Aceptamos

- C贸digo copiado sin atribuci贸n
- Contenido plagiado
- Implementaciones sin documentaci贸n
- C贸digo que no funciona
- Contenido ofensivo o inapropiado

## Proceso de Revisi贸n

1. **Revisi贸n inicial** (1-3 d铆as): Verificamos que el PR cumple requisitos b谩sicos
2. **Revisi贸n t茅cnica** (3-7 d铆as): Revisamos c贸digo y contenido
3. **Feedback**: Te daremos feedback constructivo
4. **Iteraci贸n**: Har谩s cambios basados en feedback
5. **Merge**: Una vez aprobado, hacemos merge

## Reconocimientos

Todos los contribuidores ser谩n reconocidos en:
- README.md (secci贸n de contribuidores)
- Historial de commits de Git
- Release notes

## Preguntas

驴Tienes preguntas? 
- Abre un [issue de discusi贸n](https://github.com/MARK-126/Reinforcement-learning-guide/issues)
- Etiqu茅talo como "question"

## C贸digo de Conducta

### Nuestro Compromiso

Crear un ambiente acogedor y respetuoso para todos.

### Comportamientos Esperados

- Ser respetuoso con diferentes puntos de vista
- Aceptar cr铆ticas constructivas
- Enfocarse en lo mejor para la comunidad
- Mostrar empat铆a hacia otros miembros

### Comportamientos Inaceptables

- Lenguaje ofensivo o inapropiado
- Trolling o comentarios insultantes
- Acoso p煤blico o privado
- Publicar informaci贸n privada de otros sin permiso

## Licencia

Al contribuir, aceptas que tus contribuciones ser谩n licenciadas bajo la MIT License del proyecto.

---

隆Gracias por contribuir a hacer de esta la mejor gu铆a de RL en espa帽ol! 
