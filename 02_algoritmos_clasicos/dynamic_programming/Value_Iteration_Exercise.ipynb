{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration Algorithm\n",
    "\n",
    "Welcome to the Value Iteration assignment! This is one of the fundamental algorithms in Dynamic Programming for solving MDPs. By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Understand the Bellman Optimality Equation and how it differs from the Bellman Expectation Equation\n",
    "* Implement the value update (Bellman backup) for Value Iteration\n",
    "* Extract an optimal policy from the optimal value function\n",
    "* Build a complete Value Iteration solver\n",
    "* Understand when Value Iteration converges and why\n",
    "\n",
    "## Dynamic Programming: The Foundation\n",
    "\n",
    "**Dynamic Programming (DP)** requires:\n",
    "- ✅ Full knowledge of the MDP (model-based)\n",
    "- ✅ Can compute exact solutions\n",
    "- ❌ Computationally expensive for large state spaces\n",
    "- ❌ Requires environment dynamics (not always available)\n",
    "\n",
    "## Value Iteration: Key Ideas\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "$$V^*(s) = \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma V^*(s')\\right]$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize $V(s)$ arbitrarily for all states\n",
    "2. Repeat until convergence:\n",
    "   - For each state $s$:\n",
    "     - $V(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n",
    "3. Extract policy: $\\pi(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n",
    "\n",
    "**Key Insight:** Value Iteration combines policy evaluation and policy improvement into a single update!\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*bEhGLE8N0z_kDT6h_qjW8w.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "## Important Note on Submission\n",
    "\n",
    "Please ensure:\n",
    "1. No extra print statements\n",
    "2. No extra code cells\n",
    "3. Function parameters unchanged\n",
    "4. No global variables in graded functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - GridWorld Environment](#2)\n",
    "- [3 - Value Function Initialization](#3)\n",
    "    - [Exercise 1 - initialize_value_function](#ex-1)\n",
    "- [4 - Bellman Optimality Backup](#4)\n",
    "    - [Exercise 2 - bellman_optimality_backup](#ex-2)\n",
    "- [5 - Policy Extraction](#5)\n",
    "    - [Exercise 3 - extract_policy](#ex-3)\n",
    "- [6 - Value Iteration Sweep](#6)\n",
    "    - [Exercise 4 - value_iteration_sweep](#ex-4)\n",
    "- [7 - Complete Value Iteration](#7)\n",
    "    - [Exercise 5 - value_iteration](#ex-5)\n",
    "- [8 - Testing and Visualization](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from value_iteration_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - GridWorld Environment\n",
    "\n",
    "We'll use FrozenLake as our test environment. It's a simple 4x4 gridworld:\n",
    "\n",
    "```\n",
    "SFFF  (S: start, F: frozen, H: hole, G: goal)\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "- **States**: 16 positions (4x4 grid)\n",
    "- **Actions**: 4 (left=0, down=1, right=2, up=3)\n",
    "- **Rewards**: +1 for reaching goal, 0 otherwise\n",
    "- **Transitions**: Deterministic (is_slippery=False)\n",
    "\n",
    "**MDP Components:**\n",
    "- $\\mathcal{S}$: Set of 16 states\n",
    "- $\\mathcal{A}$: Set of 4 actions\n",
    "- $p(s',r|s,a)$: Transition dynamics (accessible via `env.P`)\n",
    "- $\\gamma$: Discount factor (we'll use 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"States: {n_states}\")\n",
    "print(f\"Actions: {n_actions}\")\n",
    "print(f\"\\nTransition dynamics available via env.P\")\n",
    "print(f\"Example - State 0, Action 2 (right):\")\n",
    "print(f\"env.P[0][2] = {env.P[0][2]}\")\n",
    "print(f\"\\nFormat: [(probability, next_state, reward, done)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Value Function Initialization\n",
    "\n",
    "The value function $V(s)$ represents the expected cumulative reward starting from state $s$ and following the optimal policy.\n",
    "\n",
    "**Initialization strategies:**\n",
    "- **Zeros**: Conservative, slower convergence\n",
    "- **Random**: Can help explore different convergence paths\n",
    "- **Optimistic**: Initialize with high values to encourage exploration during policy extraction\n",
    "\n",
    "For Value Iteration, we typically initialize to zeros, except terminal states which are always 0.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - initialize_value_function\n",
    "\n",
    "Implement value function initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_value_function\n",
    "\n",
    "def initialize_value_function(n_states, terminal_states=None, init_value=0.0):\n",
    "    \"\"\"\n",
    "    Initialize value function for all states.\n",
    "    \n",
    "    Arguments:\n",
    "    n_states -- number of states\n",
    "    terminal_states -- list of terminal state indices (value always 0)\n",
    "    init_value -- initial value for non-terminal states\n",
    "    \n",
    "    Returns:\n",
    "    V -- numpy array of shape (n_states,) with initialized values\n",
    "    \"\"\"\n",
    "    # (approx. 3-5 lines)\n",
    "    # 1. Create array of init_value for all states\n",
    "    # 2. Set terminal states to 0 (if terminal_states is provided)\n",
    "    # Hint: Use np.ones() or np.full() for initialization\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "V = initialize_value_function(16, terminal_states=[5, 7, 11, 12, 15], init_value=0.0)\n",
    "print(\"Value function shape:\", V.shape)\n",
    "print(\"Value function:\\n\", V.reshape(4, 4))\n",
    "print(f\"\\nTerminal states (5,7,11,12,15) should be 0: {V[[5,7,11,12,15]]}\")\n",
    "\n",
    "# Run the grader\n",
    "initialize_value_function_test(initialize_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Bellman Optimality Backup\n",
    "\n",
    "The **Bellman optimality backup** is the core of Value Iteration. For a single state $s$, we compute:\n",
    "\n",
    "$$V(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma V(s')\\right]$$\n",
    "\n",
    "**Breaking it down:**\n",
    "1. For each action $a$ from state $s$:\n",
    "   - Look up possible transitions: $p(s',r|s,a)$ (from `env.P[s][a]`)\n",
    "   - For each possible next state $s'$:\n",
    "     - Compute: $p \\times (r + \\gamma V(s'))$\n",
    "   - Sum over all next states: $Q(s,a) = \\sum_{s'} \\ldots$\n",
    "2. Take maximum over all actions: $V(s) = \\max_a Q(s,a)$\n",
    "\n",
    "**Environment dynamics format:**\n",
    "```python\n",
    "env.P[state][action] = [(probability, next_state, reward, done), ...]\n",
    "```\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - bellman_optimality_backup\n",
    "\n",
    "Implement the Bellman optimality backup for a single state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: bellman_optimality_backup\n",
    "\n",
    "def bellman_optimality_backup(env, V, state, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Perform Bellman optimality backup for a single state.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment with env.P (transition dynamics)\n",
    "    V -- current value function, numpy array of shape (n_states,)\n",
    "    state -- state to update\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    new_value -- updated value for this state (scalar)\n",
    "    best_action -- action that achieves max value (for debugging)\n",
    "    \"\"\"\n",
    "    # (approx. 10-12 lines)\n",
    "    # 1. Initialize list to store Q-values for each action\n",
    "    # 2. For each action:\n",
    "    #    a. Get transitions: env.P[state][action]\n",
    "    #    b. For each (prob, next_state, reward, done) in transitions:\n",
    "    #       - Compute: prob * (reward + gamma * V[next_state])\n",
    "    #       - Sum these values to get Q(state, action)\n",
    "    # 3. Find max Q-value and corresponding action\n",
    "    # 4. Return max Q-value as new_value\n",
    "    \n",
    "    # Hint: env.P[state][action] gives list of (probability, next_state, reward, done)\n",
    "    # Hint: Use np.argmax() to find best action\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return new_value, best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "V_test = np.zeros(16)\n",
    "V_test[15] = 1.0  # Goal state has value 1\n",
    "\n",
    "# Test backup for state 14 (next to goal)\n",
    "new_value, best_action = bellman_optimality_backup(env, V_test, state=14, gamma=0.99)\n",
    "print(f\"State 14 (next to goal):\")\n",
    "print(f\"  New value: {new_value:.4f}\")\n",
    "print(f\"  Best action: {best_action} (1=down toward goal)\")\n",
    "print(f\"  Expected: ~0.99 (reward 0 + gamma * V[15])\")\n",
    "\n",
    "# Run the grader\n",
    "bellman_optimality_backup_test(bellman_optimality_backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "State 14 (next to goal):\n",
    "  New value: 0.9900\n",
    "  Best action: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Policy Extraction\n",
    "\n",
    "Once we have the optimal value function $V^*$, we can extract the optimal policy:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma V^*(s')\\right]$$\n",
    "\n",
    "This is similar to the Bellman backup, but instead of taking the **max value**, we take the **argmax action**.\n",
    "\n",
    "**Policy types:**\n",
    "- **Deterministic**: $\\pi(s)$ returns a single action\n",
    "- **Stochastic**: $\\pi(a|s)$ returns probability distribution over actions\n",
    "\n",
    "We'll implement a deterministic policy.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - extract_policy\n",
    "\n",
    "Extract the greedy policy from a value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: extract_policy\n",
    "\n",
    "def extract_policy(env, V, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Extract greedy policy from value function.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    V -- value function, numpy array of shape (n_states,)\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    policy -- numpy array of shape (n_states,) with best action for each state\n",
    "    \"\"\"\n",
    "    # (approx. 8-10 lines)\n",
    "    # 1. Initialize policy array\n",
    "    # 2. For each state:\n",
    "    #    a. Compute Q(s,a) for all actions (similar to bellman_backup)\n",
    "    #    b. Select action with highest Q-value\n",
    "    #    c. Store in policy array\n",
    "    # \n",
    "    # Hint: You can reuse logic from bellman_optimality_backup\n",
    "    # Hint: For each action, compute sum over transitions\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "V_test = np.zeros(16)\n",
    "V_test[15] = 1.0\n",
    "\n",
    "policy = extract_policy(env, V_test, gamma=0.99)\n",
    "print(\"Policy shape:\", policy.shape)\n",
    "print(\"Policy (as 4x4 grid):\")\n",
    "print(policy.reshape(4, 4))\n",
    "print(\"\\nAction mapping: 0=left, 1=down, 2=right, 3=up\")\n",
    "print(\"\\nState 14 should choose action 2 (right toward goal)\")\n",
    "print(f\"Policy[14] = {policy[14]}\")\n",
    "\n",
    "# Run the grader\n",
    "extract_policy_test(extract_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Value Iteration Sweep\n",
    "\n",
    "A **sweep** means updating the value function for **all states** once. \n",
    "\n",
    "In Value Iteration, we have two update strategies:\n",
    "\n",
    "**1. Synchronous (two arrays):**\n",
    "```python\n",
    "V_new = np.copy(V)\n",
    "for s in all_states:\n",
    "    V_new[s] = max_a sum(...)\n",
    "V = V_new\n",
    "```\n",
    "\n",
    "**2. Asynchronous (in-place):**\n",
    "```python\n",
    "for s in all_states:\n",
    "    V[s] = max_a sum(...)  # Update immediately\n",
    "```\n",
    "\n",
    "Asynchronous often converges faster! We'll implement the in-place version.\n",
    "\n",
    "**Convergence check:**\n",
    "- Measure max change: $\\delta = \\max_s |V_{new}(s) - V_{old}(s)|$\n",
    "- Stop when $\\delta < \\theta$ (threshold)\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - value_iteration_sweep\n",
    "\n",
    "Perform one complete sweep over all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: value_iteration_sweep\n",
    "\n",
    "def value_iteration_sweep(env, V, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Perform one sweep of value iteration (update all states).\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    V -- current value function, numpy array of shape (n_states,)\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    V -- updated value function\n",
    "    delta -- maximum change in value function (for convergence check)\n",
    "    \"\"\"\n",
    "    # (approx. 6-8 lines)\n",
    "    # 1. Initialize delta = 0\n",
    "    # 2. For each state:\n",
    "    #    a. Store old value: v_old = V[state]\n",
    "    #    b. Compute new value using bellman_optimality_backup\n",
    "    #    c. Update V[state] = new_value (in-place!)\n",
    "    #    d. Update delta = max(delta, |new_value - v_old|)\n",
    "    # 3. Return updated V and delta\n",
    "    \n",
    "    # Hint: Use your bellman_optimality_backup function\n",
    "    # Hint: delta tracks the largest change across all states\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "V_test = initialize_value_function(16, terminal_states=[5,7,11,12,15])\n",
    "\n",
    "print(\"Initial V (first row):\", V_test[:4])\n",
    "\n",
    "V_updated, delta = value_iteration_sweep(env, V_test, gamma=0.99)\n",
    "print(f\"After 1 sweep:\")\n",
    "print(f\"  V (first row): {V_updated[:4]}\")\n",
    "print(f\"  Delta: {delta:.6f}\")\n",
    "\n",
    "# Run a few more sweeps\n",
    "for i in range(3):\n",
    "    V_updated, delta = value_iteration_sweep(env, V_updated, gamma=0.99)\n",
    "    print(f\"After {i+2} sweeps: delta = {delta:.6f}\")\n",
    "\n",
    "# Run the grader\n",
    "value_iteration_sweep_test(value_iteration_sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Complete Value Iteration\n",
    "\n",
    "Now we put it all together! The complete Value Iteration algorithm:\n",
    "\n",
    "```\n",
    "Initialize V(s) for all s\n",
    "repeat:\n",
    "    Δ ← 0\n",
    "    for each state s:\n",
    "        v ← V(s)\n",
    "        V(s) ← max_a Σ p(s',r|s,a)[r + γV(s')]\n",
    "        Δ ← max(Δ, |v - V(s)|)\n",
    "until Δ < θ\n",
    "```\n",
    "\n",
    "**Convergence:**\n",
    "- Value Iteration is guaranteed to converge to $V^*$\n",
    "- Convergence is geometric: error decreases by factor of $\\gamma$ each sweep\n",
    "- Typical threshold: $\\theta = 10^{-6}$ to $10^{-4}$\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - value_iteration\n",
    "\n",
    "Implement the complete Value Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: value_iteration\n",
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    gamma -- discount factor\n",
    "    theta -- convergence threshold\n",
    "    max_iterations -- maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    V -- optimal value function\n",
    "    policy -- optimal policy\n",
    "    n_iterations -- number of iterations until convergence\n",
    "    deltas -- list of delta values (for plotting convergence)\n",
    "    \"\"\"\n",
    "    # (approx. 12-15 lines)\n",
    "    # 1. Initialize value function (hint: use your initialize_value_function)\n",
    "    #    FrozenLake holes: [5,7,11,12], goal: [15]\n",
    "    # 2. Initialize deltas list to track convergence\n",
    "    # 3. Loop until convergence or max_iterations:\n",
    "    #    a. Perform one sweep (hint: use your value_iteration_sweep)\n",
    "    #    b. Store delta in deltas list\n",
    "    #    c. If delta < theta: break (converged!)\n",
    "    # 4. Extract optimal policy (hint: use your extract_policy)\n",
    "    # 5. Return V, policy, n_iterations, deltas\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return V, policy, n_iterations, deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Testing and Visualization\n",
    "\n",
    "Let's run the complete Value Iteration algorithm and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Value Iteration\n",
    "print(\"Running Value Iteration on FrozenLake...\\n\")\n",
    "V_optimal, policy_optimal, n_iter, deltas = value_iteration(\n",
    "    env, gamma=0.99, theta=1e-6, max_iterations=1000\n",
    ")\n",
    "\n",
    "print(f\"Converged in {n_iter} iterations\\n\")\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V_optimal.reshape(4, 4))\n",
    "print(\"\\nOptimal Policy (0=L, 1=D, 2=R, 3=U):\")\n",
    "print(policy_optimal.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Delta over iterations\n",
    "ax1.plot(deltas, linewidth=2)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Delta (max value change)')\n",
    "ax1.set_title('Value Iteration Convergence')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Value function heatmap\n",
    "V_grid = V_optimal.reshape(4, 4)\n",
    "im = ax2.imshow(V_grid, cmap='viridis')\n",
    "ax2.set_title('Optimal Value Function')\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_yticks(range(4))\n",
    "\n",
    "# Add value annotations\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        text = ax2.text(j, i, f'{V_grid[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"white\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"1. Delta decreases geometrically (linear on log scale)\")\n",
    "print(f\"2. Values are highest near the goal (state 15)\")\n",
    "print(f\"3. Hole states (5,7,11,12) have value 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy with arrows\n",
    "def plot_policy(policy, title=\"Optimal Policy\"):\n",
    "    \"\"\"Plot policy as arrows on grid.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Create grid\n",
    "    grid = np.zeros((4, 4))\n",
    "    ax.imshow(grid, cmap='Blues', alpha=0.3)\n",
    "    \n",
    "    # Arrow directions\n",
    "    arrows = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "    \n",
    "    # Plot arrows\n",
    "    for state in range(16):\n",
    "        if state in [5, 7, 11, 12]:  # Holes\n",
    "            continue\n",
    "        i, j = state // 4, state % 4\n",
    "        action = policy[state]\n",
    "        ax.text(j, i, arrows[action], ha='center', va='center', \n",
    "                fontsize=30, color='darkblue')\n",
    "    \n",
    "    # Mark special states\n",
    "    ax.text(0, 0, 'S', ha='left', va='top', fontsize=12, color='green', weight='bold')\n",
    "    ax.text(3, 3, 'G', ha='right', va='bottom', fontsize=12, color='red', weight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_policy(policy_optimal, \"Optimal Policy from Value Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the policy\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    \"\"\"Evaluate policy by running episodes.\"\"\"\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = int(policy[state])\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if reward == 1:\n",
    "                successes += 1\n",
    "                break\n",
    "    \n",
    "    return successes / n_episodes\n",
    "\n",
    "success_rate = evaluate_policy(env, policy_optimal, n_episodes=100)\n",
    "print(f\"\\nPolicy Evaluation (100 episodes):\")\n",
    "print(f\"Success rate: {success_rate * 100:.1f}%\")\n",
    "print(f\"\\nFor deterministic FrozenLake, optimal policy should reach 100%!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Value Iteration from scratch! Here's what you've learned:\n",
    "\n",
    "✅ How to initialize value functions properly\n",
    "\n",
    "✅ The Bellman optimality backup equation\n",
    "\n",
    "✅ How to extract greedy policies from value functions\n",
    "\n",
    "✅ The complete Value Iteration algorithm\n",
    "\n",
    "✅ How to check for convergence\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Value Iteration** combines policy evaluation and improvement into one update\n",
    "2. **Convergence** is guaranteed and geometric (fast!)\n",
    "3. **Model-based**: Requires full knowledge of MDP dynamics\n",
    "4. **Optimal**: Finds the true optimal policy for the MDP\n",
    "5. **In-place updates** often converge faster than synchronous\n",
    "\n",
    "### Value Iteration vs Q-Learning:\n",
    "\n",
    "| Aspect | Value Iteration | Q-Learning |\n",
    "|--------|----------------|------------|\n",
    "| Knowledge | Requires model | Model-free |\n",
    "| Updates | All states per iteration | One state per step |\n",
    "| Convergence | Guaranteed, fast | Guaranteed (with conditions) |\n",
    "| Scalability | Poor (large state spaces) | Better (sampling) |\n",
    "| Optimality | Exact optimal | Converges to optimal |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Learn about **Policy Iteration** (evaluates full policy before improvement)\n",
    "- Compare convergence speed: Policy Iteration vs Value Iteration\n",
    "- See how **Monte Carlo** methods work without a model\n",
    "- Understand why **Q-Learning** is preferred for large/unknown MDPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
