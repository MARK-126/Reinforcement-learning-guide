{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration - Interactive Exercise\n",
    "\n",
    "Welcome! In this notebook, you will implement **Policy Iteration**, one of the fundamental algorithms in Dynamic Programming for Reinforcement Learning.\n",
    "\n",
    "## What is Policy Iteration?\n",
    "\n",
    "Policy Iteration is an algorithm that finds the optimal policy by alternating between two steps:\n",
    "1. **Policy Evaluation**: Compute the value function V^π for the current policy π\n",
    "2. **Policy Improvement**: Update the policy to be greedy with respect to V^π\n",
    "\n",
    "Unlike Value Iteration (which updates values and policies simultaneously), Policy Iteration fully evaluates each policy before improving it.\n",
    "\n",
    "## Key Differences from Value Iteration\n",
    "\n",
    "| Aspect | Value Iteration | Policy Iteration |\n",
    "|--------|----------------|------------------|\n",
    "| Update | Bellman Optimality | Bellman Expectation + Greedy |\n",
    "| Convergence | Value function | Policy (can converge in fewer iterations) |\n",
    "| Per Iteration | Faster (1 sweep) | Slower (multiple sweeps for evaluation) |\n",
    "| Total Iterations | More iterations | Fewer iterations |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the two-phase structure of Policy Iteration\n",
    "- Implement policy evaluation using the Bellman Expectation equation\n",
    "- Implement policy improvement\n",
    "- Combine both to create the complete Policy Iteration algorithm\n",
    "- Compare Policy Iteration with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from policy_iteration_tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment: FrozenLake\n",
    "\n",
    "We'll use the same FrozenLake environment as Value Iteration for easy comparison.\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"Number of states: {n_states}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "print(f\"\\nAction meanings: 0=Left, 1=Down, 2=Right, 3=Up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Initialize Policy\n",
    "\n",
    "In Policy Iteration, we start with an initial policy. A common choice is a **uniform random policy** that selects each action with equal probability.\n",
    "\n",
    "**Task**: Initialize a stochastic policy where each action has equal probability.\n",
    "\n",
    "**Policy Representation**: \n",
    "- Shape: (n_states, n_actions)\n",
    "- policy[s, a] = probability of taking action a in state s\n",
    "- For uniform random: policy[s, a] = 1/n_actions for all s, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_policy\n",
    "\n",
    "def initialize_policy(n_states, n_actions):\n",
    "    \"\"\"\n",
    "    Initialize a uniform random policy.\n",
    "    \n",
    "    Arguments:\n",
    "    n_states -- number of states\n",
    "    n_actions -- number of actions\n",
    "    \n",
    "    Returns:\n",
    "    policy -- numpy array of shape (n_states, n_actions) with uniform probabilities\n",
    "    \"\"\"\n",
    "    # (approx. 1 line)\n",
    "    # Create a matrix where each row sums to 1.0 (valid probability distribution)\n",
    "    # Hint: Use np.ones() and divide by n_actions\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "initialize_policy_test(initialize_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Policy Evaluation Step\n",
    "\n",
    "Policy evaluation computes V^π using the **Bellman Expectation Equation**:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "This is different from Value Iteration which uses the Bellman **Optimality** Equation (with max).\n",
    "\n",
    "**Task**: Implement one iteration of policy evaluation for a single state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: policy_evaluation_step\n",
    "\n",
    "def policy_evaluation_step(env, V, policy, state, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Perform one step of policy evaluation for a single state.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- OpenAI Gym environment\n",
    "    V -- current value function, numpy array of shape (n_states,)\n",
    "    policy -- current policy, numpy array of shape (n_states, n_actions)\n",
    "    state -- state to evaluate\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    new_value -- updated value for the state\n",
    "    \"\"\"\n",
    "    # (approx. 8-10 lines)\n",
    "    # 1. Initialize new_value = 0\n",
    "    # 2. For each action:\n",
    "    #    a. Get action probability: policy[state, action]\n",
    "    #    b. Get transitions: env.P[state][action]\n",
    "    #    c. For each (prob, next_state, reward, done):\n",
    "    #       - Compute: prob * (reward + gamma * V[next_state])\n",
    "    #       - Add to action_value\n",
    "    #    d. Add: policy[state, action] * action_value to new_value\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "policy_evaluation_step_test(policy_evaluation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Full Policy Evaluation\n",
    "\n",
    "Now we need to evaluate the policy until convergence. We sweep through all states repeatedly until the value function stops changing significantly.\n",
    "\n",
    "**Convergence Criterion**: Stop when max|V_new - V_old| < theta\n",
    "\n",
    "**Task**: Implement full policy evaluation with convergence check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: policy_evaluation\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=0.99, theta=1e-8, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a policy until convergence.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- OpenAI Gym environment\n",
    "    policy -- policy to evaluate, numpy array of shape (n_states, n_actions)\n",
    "    gamma -- discount factor\n",
    "    theta -- convergence threshold\n",
    "    max_iterations -- maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    V -- value function for the policy, numpy array of shape (n_states,)\n",
    "    iterations -- number of iterations until convergence\n",
    "    \"\"\"\n",
    "    # (approx. 12-15 lines)\n",
    "    # 1. Initialize V = zeros\n",
    "    # 2. For iteration in range(max_iterations):\n",
    "    #    a. delta = 0\n",
    "    #    b. For each state:\n",
    "    #       - old_value = V[state]\n",
    "    #       - new_value = policy_evaluation_step(...)\n",
    "    #       - V[state] = new_value\n",
    "    #       - delta = max(delta, abs(old_value - new_value))\n",
    "    #    c. If delta < theta: break\n",
    "    # 3. Return V and number of iterations\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return V, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "policy_evaluation_test(policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Policy Improvement\n",
    "\n",
    "After evaluating the current policy, we improve it by making it **greedy** with respect to the value function:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "This is similar to extracting policy in Value Iteration, but now we use V^π instead of V*.\n",
    "\n",
    "**Task**: Implement policy improvement that returns a deterministic greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: policy_improvement\n",
    "\n",
    "def policy_improvement(env, V, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Improve policy by making it greedy with respect to V.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- OpenAI Gym environment\n",
    "    V -- value function, numpy array of shape (n_states,)\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    new_policy -- improved policy, numpy array of shape (n_states, n_actions)\n",
    "                  deterministic: new_policy[s, best_action] = 1.0, others = 0.0\n",
    "    \"\"\"\n",
    "    # (approx. 12-15 lines)\n",
    "    # 1. Get n_states and n_actions from env\n",
    "    # 2. Initialize new_policy = zeros(n_states, n_actions)\n",
    "    # 3. For each state:\n",
    "    #    a. Initialize action_values = zeros(n_actions)\n",
    "    #    b. For each action:\n",
    "    #       - Get transitions: env.P[state][action]\n",
    "    #       - For each (prob, next_state, reward, done):\n",
    "    #         * Compute: prob * (reward + gamma * V[next_state])\n",
    "    #       - Store in action_values[action]\n",
    "    #    c. Find best_action = argmax(action_values)\n",
    "    #    d. Set new_policy[state, best_action] = 1.0\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "policy_improvement_test(policy_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Complete Policy Iteration\n",
    "\n",
    "Now let's combine policy evaluation and policy improvement into the complete Policy Iteration algorithm!\n",
    "\n",
    "**Algorithm**:\n",
    "1. Initialize policy (e.g., uniform random)\n",
    "2. Repeat:\n",
    "   - **Policy Evaluation**: Compute V^π\n",
    "   - **Policy Improvement**: Make policy greedy w.r.t. V^π\n",
    "   - **Check**: If policy didn't change, we've found the optimal policy\n",
    "3. Return optimal policy and value function\n",
    "\n",
    "**Task**: Implement the complete Policy Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: policy_iteration\n",
    "\n",
    "def policy_iteration(env, gamma=0.99, theta=1e-8, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Solve an MDP using Policy Iteration.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- OpenAI Gym environment\n",
    "    gamma -- discount factor\n",
    "    theta -- convergence threshold for policy evaluation\n",
    "    max_iterations -- maximum number of policy iterations\n",
    "    \n",
    "    Returns:\n",
    "    policy -- optimal policy, numpy array of shape (n_states, n_actions)\n",
    "    V -- optimal value function, numpy array of shape (n_states,)\n",
    "    iterations -- number of policy iterations\n",
    "    \"\"\"\n",
    "    # (approx. 12-15 lines)\n",
    "    # 1. Get n_states and n_actions\n",
    "    # 2. Initialize policy using initialize_policy()\n",
    "    # 3. For iteration in range(max_iterations):\n",
    "    #    a. Evaluate current policy: V = policy_evaluation(...)\n",
    "    #    b. Improve policy: new_policy = policy_improvement(...)\n",
    "    #    c. Check if policy changed:\n",
    "    #       - If np.array_equal(policy, new_policy): break (converged)\n",
    "    #       - Else: policy = new_policy\n",
    "    # 4. Return policy, V, and number of iterations\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy, V, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "policy_iteration_test(policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Compare with Value Iteration\n",
    "\n",
    "Let's compare Policy Iteration with Value Iteration on the same environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Policy Iteration\n",
    "policy, V, pi_iterations = policy_iteration(env)\n",
    "\n",
    "print(f\"Policy Iteration converged in {pi_iterations} iterations\")\n",
    "print(f\"\\nOptimal Value Function:\")\n",
    "print(V.reshape(4, 4))\n",
    "\n",
    "# Extract deterministic policy (action with highest probability)\n",
    "action_map = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "deterministic_policy = np.argmax(policy, axis=1)\n",
    "\n",
    "print(f\"\\nOptimal Policy:\")\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        state = i * 4 + j\n",
    "        print(action_map[deterministic_policy[state]], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "**Policy Iteration Characteristics**:\n",
    "- Typically converges in **fewer iterations** than Value Iteration\n",
    "- Each iteration is **more expensive** (full policy evaluation)\n",
    "- Guarantees **policy convergence** (not just value convergence)\n",
    "- Often preferred when policy convergence is more important than value accuracy\n",
    "\n",
    "**When to use Policy Iteration vs Value Iteration**:\n",
    "- **Policy Iteration**: When you need the exact optimal policy quickly\n",
    "- **Value Iteration**: When you need approximate values quickly, or in continuous/large state spaces\n",
    "\n",
    "Both algorithms are guaranteed to converge to the optimal policy for finite MDPs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Policy Iteration! You now understand:\n",
    "- ✅ The two-phase structure of Policy Iteration\n",
    "- ✅ Policy evaluation using Bellman Expectation\n",
    "- ✅ Policy improvement using greedy action selection\n",
    "- ✅ How to combine them for the complete algorithm\n",
    "- ✅ Differences between Policy Iteration and Value Iteration\n",
    "\n",
    "**Next Steps**: Try Policy Gradient methods like REINFORCE for large/continuous state spaces!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
