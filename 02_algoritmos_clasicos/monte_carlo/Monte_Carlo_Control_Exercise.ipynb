{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control\n",
    "\n",
    "Welcome to Monte Carlo Control! This is a model-free RL method that learns from complete episodes. By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Understand how Monte Carlo methods differ from Dynamic Programming and TD Learning\n",
    "* Generate episodes using an ε-greedy policy\n",
    "* Calculate returns (cumulative rewards) from episodes\n",
    "* Implement first-visit Monte Carlo for Q-value estimation\n",
    "* Build a complete Monte Carlo Control algorithm\n",
    "\n",
    "## Monte Carlo Methods: Key Concepts\n",
    "\n",
    "**Monte Carlo = Learning from complete episodes**\n",
    "\n",
    "| Method | Updates | Requires Model | Bootstrap |\n",
    "|--------|---------|----------------|------------|\n",
    "| **DP** | All states | ✅ Yes | ✅ Yes |\n",
    "| **MC** | Visited states | ❌ No | ❌ No |\n",
    "| **TD** | One step | ❌ No | ✅ Yes |\n",
    "\n",
    "**Key Ideas:**\n",
    "- Learn from **complete episodes** (trajectory from start to end)\n",
    "- Use **actual returns** $G_t$ instead of bootstrapping\n",
    "- No model required (model-free)\n",
    "- Can only be applied to **episodic tasks**\n",
    "\n",
    "**Monte Carlo Update:**\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha\\left[G_t - Q(s,a)\\right]$$\n",
    "\n",
    "Where $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$ is the actual return.\n",
    "\n",
    "## Important Note on Submission\n",
    "\n",
    "Please ensure:\n",
    "1. No extra print statements\n",
    "2. No extra code cells\n",
    "3. Function parameters unchanged\n",
    "4. No global variables in graded functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Episode Generation](#2)\n",
    "    - [Exercise 1 - generate_episode](#ex-1)\n",
    "- [3 - Return Calculation](#3)\n",
    "    - [Exercise 2 - calculate_returns](#ex-2)\n",
    "- [4 - Q-Value Update](#4)\n",
    "    - [Exercise 3 - update_q_values](#ex-3)\n",
    "- [5 - Complete MC Control](#5)\n",
    "    - [Exercise 4 - monte_carlo_control](#ex-4)\n",
    "- [6 - Testing and Comparison](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from monte_carlo_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Episode Generation\n",
    "\n",
    "Monte Carlo learns from **episodes**: sequences of (state, action, reward) tuples from start to terminal state.\n",
    "\n",
    "**Episode structure:**\n",
    "```\n",
    "Episode = [(s0, a0, r1), (s1, a1, r2), ..., (sT, aT, rT+1)]\n",
    "```\n",
    "\n",
    "We use **ε-greedy** policy for exploration:\n",
    "- With probability ε: random action\n",
    "- With probability 1-ε: greedy action (argmax Q)\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - generate_episode\n",
    "\n",
    "Generate one complete episode using ε-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: generate_episode\n",
    "\n",
    "def generate_episode(env, Q, epsilon=0.1, max_steps=100):\n",
    "    \"\"\"\n",
    "    Generate one episode using epsilon-greedy policy.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    Q -- Q-table (defaultdict), Q[state][action]\n",
    "    epsilon -- exploration rate\n",
    "    max_steps -- maximum steps per episode\n",
    "    \n",
    "    Returns:\n",
    "    episode -- list of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    # (approx. 12-15 lines)\n",
    "    # 1. Initialize episode list\n",
    "    # 2. Reset environment to get initial state\n",
    "    # 3. Loop until done or max_steps:\n",
    "    #    a. Choose action using epsilon-greedy:\n",
    "    #       - Random with prob epsilon\n",
    "    #       - Argmax Q[state] with prob 1-epsilon\n",
    "    #    b. Take action, get next_state and reward\n",
    "    #    c. Append (state, action, reward) to episode\n",
    "    #    d. Update state\n",
    "    #    e. If done, break\n",
    "    # 4. Return episode\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "episode = generate_episode(env, Q, epsilon=0.5)\n",
    "print(f\"Episode length: {len(episode)}\")\n",
    "print(f\"First 3 steps: {episode[:3]}\")\n",
    "print(f\"Format: (state, action, reward)\")\n",
    "\n",
    "generate_episode_test(generate_episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Return Calculation\n",
    "\n",
    "The **return** $G_t$ is the cumulative discounted reward from time $t$:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "**Efficient computation (backward):**\n",
    "```python\n",
    "G = 0\n",
    "for t in reversed(range(T)):\n",
    "    G = rewards[t] + gamma * G\n",
    "    returns[t] = G\n",
    "```\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - calculate_returns\n",
    "\n",
    "Calculate returns for each time step in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: calculate_returns\n",
    "\n",
    "def calculate_returns(episode, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate returns for each step in episode.\n",
    "    \n",
    "    Arguments:\n",
    "    episode -- list of (state, action, reward) tuples\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    returns -- list of returns, returns[t] = G_t\n",
    "    \"\"\"\n",
    "    # (approx. 6-8 lines)\n",
    "    # 1. Initialize returns list (same length as episode)\n",
    "    # 2. Initialize G = 0\n",
    "    # 3. Loop backwards through episode (from T-1 to 0):\n",
    "    #    a. Get reward at time t: episode[t][2]\n",
    "    #    b. Update G = reward + gamma * G\n",
    "    #    c. Store returns[t] = G\n",
    "    # 4. Return returns list\n",
    "    \n",
    "    # Hint: Use reversed(range(len(episode)))\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "test_episode = [(0, 1, 0), (1, 2, 0), (2, 1, 1)]  # Last reward = 1\n",
    "returns = calculate_returns(test_episode, gamma=0.9)\n",
    "\n",
    "print(\"Episode rewards: [0, 0, 1]\")\n",
    "print(f\"Returns: {returns}\")\n",
    "print(f\"\\nExpected:\")\n",
    "print(f\"  G[2] = 1\")\n",
    "print(f\"  G[1] = 0 + 0.9*1 = 0.9\")\n",
    "print(f\"  G[0] = 0 + 0.9*0.9 = 0.81\")\n",
    "\n",
    "calculate_returns_test(calculate_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Q-Value Update\n",
    "\n",
    "**First-visit Monte Carlo**: Update Q(s,a) only the first time (s,a) is visited in an episode.\n",
    "\n",
    "**Update rule:**\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha\\left[G - Q(s,a)\\right]$$\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "For each (state, action) in episode:\n",
    "    If first visit to (state, action):\n",
    "        G = return from that point\n",
    "        Q(state, action) += alpha * (G - Q(state, action))\n",
    "```\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - update_q_values\n",
    "\n",
    "Update Q-values using first-visit Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_q_values\n",
    "\n",
    "def update_q_values(Q, episode, returns, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Update Q-values using first-visit Monte Carlo.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-table (defaultdict)\n",
    "    episode -- list of (state, action, reward) tuples\n",
    "    returns -- list of returns for each time step\n",
    "    alpha -- learning rate\n",
    "    \n",
    "    Returns:\n",
    "    Q -- updated Q-table\n",
    "    \"\"\"\n",
    "    # (approx. 8-10 lines)\n",
    "    # 1. Initialize set to track visited (state, action) pairs\n",
    "    # 2. For each time step t in episode:\n",
    "    #    a. Get state, action from episode[t]\n",
    "    #    b. Check if (state, action) already visited\n",
    "    #    c. If first visit:\n",
    "    #       - Get return G from returns[t]\n",
    "    #       - Update: Q[state][action] += alpha * (G - Q[state][action])\n",
    "    #       - Add (state, action) to visited set\n",
    "    # 3. Return Q\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "Q_test = defaultdict(lambda: np.zeros(4))\n",
    "test_episode = [(0, 1, 0), (1, 2, 0), (2, 1, 1)]\n",
    "test_returns = [0.81, 0.9, 1.0]\n",
    "\n",
    "Q_test = update_q_values(Q_test, test_episode, test_returns, alpha=0.1)\n",
    "print(f\"Q[0][1] after update: {Q_test[0][1]:.4f}\")\n",
    "print(f\"Expected: 0 + 0.1*(0.81 - 0) = 0.0810\")\n",
    "\n",
    "update_q_values_test(update_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Complete Monte Carlo Control\n",
    "\n",
    "**Monte Carlo Control Algorithm:**\n",
    "```\n",
    "Initialize Q(s,a) arbitrarily\n",
    "For each episode:\n",
    "    Generate episode using ε-greedy policy\n",
    "    Calculate returns G_t for each t\n",
    "    For each (s,a) in episode (first-visit):\n",
    "        Q(s,a) ← Q(s,a) + α[G - Q(s,a)]\n",
    "```\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - monte_carlo_control\n",
    "\n",
    "Implement complete Monte Carlo Control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: monte_carlo_control\n",
    "\n",
    "def monte_carlo_control(env, n_episodes=5000, alpha=0.1, gamma=0.99,\n",
    "                       epsilon=0.1, epsilon_decay=0.999, epsilon_min=0.01):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    n_episodes -- number of episodes\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    epsilon -- initial exploration rate\n",
    "    epsilon_decay -- decay rate for epsilon\n",
    "    epsilon_min -- minimum epsilon\n",
    "    \n",
    "    Returns:\n",
    "    Q -- learned Q-table\n",
    "    policy -- greedy policy extracted from Q\n",
    "    rewards_history -- list of total rewards per episode\n",
    "    \"\"\"\n",
    "    # (approx. 15-18 lines)\n",
    "    # 1. Initialize Q as defaultdict\n",
    "    # 2. Initialize rewards_history\n",
    "    # 3. For each episode:\n",
    "    #    a. Generate episode using your generate_episode function\n",
    "    #    b. Calculate returns using your calculate_returns function\n",
    "    #    c. Update Q-values using your update_q_values function\n",
    "    #    d. Calculate total reward (sum of rewards in episode)\n",
    "    #    e. Store reward in history\n",
    "    #    f. Decay epsilon\n",
    "    # 4. Extract greedy policy from Q\n",
    "    # 5. Return Q, policy, rewards_history\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, policy, rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Testing and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Monte Carlo Control\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "print(\"Training Monte Carlo Control...\\n\")\n",
    "Q, policy, rewards = monte_carlo_control(\n",
    "    env, n_episodes=5000, alpha=0.1, gamma=0.99,\n",
    "    epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.01\n",
    ")\n",
    "\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Final success rate (last 100): {np.mean(rewards[-100:]):.3f}\")\n",
    "\n",
    "# Visualize\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, alpha=0.3, label='Episode reward')\n",
    "plt.plot(range(window-1, len(rewards)), moving_avg, \n",
    "         label=f'Moving average ({window})', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Monte Carlo Control Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Monte Carlo Control! Here's what you've learned:\n",
    "\n",
    "✅ How to generate episodes with ε-greedy policies\n",
    "\n",
    "✅ How to calculate returns efficiently\n",
    "\n",
    "✅ First-visit Monte Carlo updates\n",
    "\n",
    "✅ Complete MC Control algorithm\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Model-free**: MC doesn't need environment dynamics\n",
    "2. **Episodic**: Requires complete episodes (can't do infinite horizon)\n",
    "3. **No bootstrapping**: Uses actual returns, not estimates\n",
    "4. **High variance**: Returns can vary a lot between episodes\n",
    "5. **Unbiased**: Converges to true Q-values (no bootstrapping bias)\n",
    "\n",
    "### Monte Carlo vs Other Methods:\n",
    "\n",
    "| Method | Model | Bootstrap | Variance | Bias | Episodes |\n",
    "|--------|-------|-----------|----------|------|----------|\n",
    "| **MC** | Free | No | High | None | Required |\n",
    "| **TD** | Free | Yes | Low | Small | Not required |\n",
    "| **DP** | Based | Yes | None | None | Not required |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Compare MC with Q-Learning on same environment\n",
    "- Learn about **every-visit MC** vs first-visit\n",
    "- Explore **off-policy MC** with importance sampling\n",
    "- Understand **n-step methods** (hybrid between MC and TD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
