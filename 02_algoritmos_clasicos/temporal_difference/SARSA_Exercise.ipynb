{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Algorithm\n",
    "\n",
    "Welcome to the SARSA assignment! SARSA (State-Action-Reward-State-Action) is an **on-policy** TD control algorithm. By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Understand the difference between on-policy and off-policy learning\n",
    "* Implement the SARSA update rule\n",
    "* Compare SARSA with Q-Learning\n",
    "* Understand when to use SARSA vs Q-Learning\n",
    "\n",
    "## SARSA vs Q-Learning: The Key Difference\n",
    "\n",
    "**Q-Learning (Off-Policy)**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right]$$\n",
    "\n",
    "**SARSA (On-Policy)**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma Q(s',a') - Q(s,a)\\right]$$\n",
    "\n",
    "The difference:\n",
    "- **Q-Learning**: Uses $\\max_{a'} Q(s',a')$ (best possible action)\n",
    "- **SARSA**: Uses $Q(s',a')$ (action actually taken)\n",
    "\n",
    "This makes SARSA more conservative and safer in risky environments!\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*vq3cnSPORN6YZAHCf9uAEA.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "## Important Note on Submission\n",
    "\n",
    "Please ensure:\n",
    "1. No extra print statements\n",
    "2. No extra code cells\n",
    "3. Function parameters unchanged\n",
    "4. No global variables in graded functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - SARSA Update](#2)\n",
    "    - [Exercise 1 - sarsa_update](#ex-1)\n",
    "- [3 - SARSA Training Loop](#3)\n",
    "    - [Exercise 2 - train_sarsa](#ex-2)\n",
    "- [4 - Comparison: SARSA vs Q-Learning](#4)\n",
    "    - [4.1 - CliffWalking Environment](#4-1)\n",
    "    - [4.2 - Train Both Algorithms](#4-2)\n",
    "    - [4.3 - Compare Results](#4-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from sarsa_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - SARSA Update\n",
    "\n",
    "The SARSA algorithm follows this sequence:\n",
    "1. Start in state $s$\n",
    "2. Choose action $a$ using policy (e.g., ε-greedy)\n",
    "3. Take action $a$, observe reward $r$ and next state $s'$\n",
    "4. **Choose next action $a'$ using the same policy** (key difference!)\n",
    "5. Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$\n",
    "6. $s \\leftarrow s'$, $a \\leftarrow a'$\n",
    "\n",
    "**Why is this important?**\n",
    "- SARSA learns about the policy it's actually following (including exploration)\n",
    "- More conservative: avoids risky paths during exploration\n",
    "- Q-Learning learns about the optimal policy (ignoring exploration)\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - sarsa_update\n",
    "\n",
    "Implement the SARSA update rule. Note that unlike Q-Learning, you need the next action $a'$ that will actually be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sarsa_update\n",
    "\n",
    "def sarsa_update(Q, state, action, reward, next_state, next_action, done, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Update Q-table using SARSA rule.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-table, numpy array of shape (n_states, n_actions)\n",
    "    state -- current state\n",
    "    action -- action taken\n",
    "    reward -- reward received\n",
    "    next_state -- next state\n",
    "    next_action -- next action that WILL be taken (key difference from Q-Learning!)\n",
    "    done -- boolean, True if next_state is terminal\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    Q -- updated Q-table\n",
    "    td_error -- TD error\n",
    "    \"\"\"\n",
    "    # (approx. 5-7 lines)\n",
    "    # Step 1: Get current Q-value Q(s,a)\n",
    "    # Step 2: Calculate TD target\n",
    "    #         If done: target = reward\n",
    "    #         Else: target = reward + gamma * Q(s', a')  <- Use next_action, not max!\n",
    "    # Step 3: Calculate TD error\n",
    "    # Step 4: Update Q-value\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "Q_test = np.zeros((4, 2))\n",
    "Q_test[1] = [0.5, 0.3]\n",
    "\n",
    "# SARSA uses the actual next action (not max)\n",
    "Q_updated, td_error = sarsa_update(\n",
    "    Q_test.copy(), state=0, action=0, reward=1.0,\n",
    "    next_state=1, next_action=1, done=False,  # next_action=1 (not max which is 0)\n",
    "    alpha=0.1, gamma=0.9\n",
    ")\n",
    "\n",
    "print(f\"SARSA update (using next_action=1):\")\n",
    "print(f\"  Q[0,0] = {Q_updated[0, 0]:.4f}\")\n",
    "print(f\"  TD error = {td_error:.4f}\")\n",
    "print(f\"  (Q-Learning would use max Q[1] = 0.5, SARSA uses Q[1,1] = 0.3)\")\n",
    "\n",
    "# Run the grader\n",
    "sarsa_update_test(sarsa_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output (approximately):**\n",
    "```\n",
    "SARSA update (using next_action=1):\n",
    "  Q[0,0] = 0.1270\n",
    "  TD error = 1.2700\n",
    "```\n",
    "Note: This is different from Q-Learning (0.145) because SARSA uses the actual next action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - SARSA Training Loop\n",
    "\n",
    "The SARSA training loop is similar to Q-Learning with one key difference:\n",
    "\n",
    "```\n",
    "For each episode:\n",
    "    Initialize s\n",
    "    Choose a using ε-greedy from Q(s,·)\n",
    "    For each step:\n",
    "        Take action a, observe r, s'\n",
    "        Choose a' using ε-greedy from Q(s',·)  <- Must choose before update!\n",
    "        Update Q(s,a) using (s, a, r, s', a')  <- Use actual a', not max\n",
    "        s ← s', a ← a'\n",
    "```\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - train_sarsa\n",
    "\n",
    "Implement the SARSA training loop. Pay attention to when you choose the next action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (same as Q-Learning)\n",
    "def epsilon_greedy_action(Q, state, n_actions, epsilon):\n",
    "    \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def initialize_q_table(n_states, n_actions, init_value=0.0):\n",
    "    \"\"\"Initialize Q-table.\"\"\"\n",
    "    return np.ones((n_states, n_actions)) * init_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_sarsa\n",
    "\n",
    "def train_sarsa(env, n_episodes=1000, alpha=0.1, gamma=0.99,\n",
    "                epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                max_steps=100):\n",
    "    \"\"\"\n",
    "    Train SARSA agent.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    n_episodes -- number of episodes\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    epsilon -- initial exploration rate\n",
    "    epsilon_decay -- epsilon decay rate\n",
    "    epsilon_min -- minimum epsilon\n",
    "    max_steps -- max steps per episode\n",
    "    \n",
    "    Returns:\n",
    "    Q -- trained Q-table\n",
    "    rewards_history -- list of rewards per episode\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = initialize_q_table(n_states, n_actions)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    # Training loop (approx. 20-25 lines)\n",
    "    # Key difference from Q-Learning:\n",
    "    #   1. Choose initial action BEFORE the step loop\n",
    "    #   2. Choose next_action BEFORE calling sarsa_update\n",
    "    #   3. Use next_action in sarsa_update (not max!)\n",
    "    #   4. Set action = next_action for next iteration\n",
    "    \n",
    "    # For each episode:\n",
    "    #   1. Reset environment, get initial state\n",
    "    #   2. Choose initial action using epsilon_greedy\n",
    "    #   3. For each step:\n",
    "    #      a. Take action, observe reward and next_state\n",
    "    #      b. Choose next_action using epsilon_greedy (BEFORE update!)\n",
    "    #      c. Update Q using sarsa_update with next_action\n",
    "    #      d. state = next_state, action = next_action\n",
    "    #      e. If done, break\n",
    "    #   4. Decay epsilon\n",
    "    #   5. Store episode reward\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Comparison: SARSA vs Q-Learning\n",
    "\n",
    "Let's compare SARSA and Q-Learning on the **CliffWalking** environment!\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - CliffWalking Environment\n",
    "\n",
    "```\n",
    "┌─────────────┐\n",
    "│ · · · · · G │  G = Goal (+1)\n",
    "│ S C C C C C │  S = Start\n",
    "└─────────────┘  C = Cliff (-100)\n",
    "```\n",
    "\n",
    "**The Dilemma:**\n",
    "- **Shortest path**: Walk along the cliff (risky during exploration!)\n",
    "- **Safe path**: Walk along the top (longer but safer)\n",
    "\n",
    "**Expected behavior:**\n",
    "- **Q-Learning**: Learns the optimal (risky) path along the cliff\n",
    "- **SARSA**: Learns a safer path away from the cliff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Train Both Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Q-Learning from previous exercise\n",
    "def train_q_learning(env, n_episodes=1000, alpha=0.1, gamma=0.99,\n",
    "                     epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                     max_steps=100):\n",
    "    \"\"\"Q-Learning training (for comparison).\"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = initialize_q_table(n_states, n_actions)\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy_action(Q, state, n_actions, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-Learning update (uses max)\n",
    "            current_q = Q[state, action]\n",
    "            if done:\n",
    "                target_q = reward\n",
    "            else:\n",
    "                target_q = reward + gamma * np.max(Q[next_state])\n",
    "            Q[state, action] = current_q + alpha * (target_q - current_q)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        rewards_history.append(episode_reward)\n",
    "    \n",
    "    return Q, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CliffWalking environment\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "print(\"Training both algorithms on CliffWalking...\")\n",
    "print(\"This will show the key difference between SARSA (safe) and Q-Learning (risky)\\n\")\n",
    "\n",
    "# Train Q-Learning\n",
    "np.random.seed(42)\n",
    "Q_qlearning, rewards_qlearning = train_q_learning(\n",
    "    env, n_episodes=500, alpha=0.5, gamma=0.99,\n",
    "    epsilon=0.1, epsilon_decay=1.0, epsilon_min=0.1\n",
    ")\n",
    "\n",
    "# Train SARSA\n",
    "np.random.seed(42)\n",
    "Q_sarsa, rewards_sarsa = train_sarsa(\n",
    "    env, n_episodes=500, alpha=0.5, gamma=0.99,\n",
    "    epsilon=0.1, epsilon_decay=1.0, epsilon_min=0.1\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rewards comparison\n",
    "window = 20\n",
    "if len(rewards_qlearning) >= window:\n",
    "    ma_qlearning = np.convolve(rewards_qlearning, np.ones(window)/window, mode='valid')\n",
    "    ma_sarsa = np.convolve(rewards_sarsa, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax1.plot(range(window-1, len(rewards_qlearning)), ma_qlearning, \n",
    "             label='Q-Learning', linewidth=2, alpha=0.8)\n",
    "    ax1.plot(range(window-1, len(rewards_sarsa)), ma_sarsa,\n",
    "             label='SARSA', linewidth=2, alpha=0.8)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Average Reward')\n",
    "    ax1.set_title('Learning Progress Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance comparison\n",
    "final_ql = np.mean(rewards_qlearning[-100:])\n",
    "final_sarsa = np.mean(rewards_sarsa[-100:])\n",
    "\n",
    "ax2.bar(['Q-Learning', 'SARSA'], [final_ql, final_sarsa], \n",
    "        color=['#ff7f0e', '#2ca02c'], alpha=0.7)\n",
    "ax2.set_ylabel('Average Reward (last 100 episodes)')\n",
    "ax2.set_title('Final Performance')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Performance (last 100 episodes):\")\n",
    "print(f\"Q-Learning: {final_ql:.2f}\")\n",
    "print(f\"SARSA: {final_sarsa:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Q-Learning learns faster but takes more risky path (falls off cliff during training)\")\n",
    "print(f\"- SARSA is more conservative, learns safer path away from cliff\")\n",
    "print(f\"- During evaluation (epsilon=0), Q-Learning achieves optimal policy\")\n",
    "print(f\"- During training (epsilon>0), SARSA gets better average rewards\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented SARSA and compared it with Q-Learning! Here's what you've learned:\n",
    "\n",
    "✅ The difference between on-policy (SARSA) and off-policy (Q-Learning)\n",
    "\n",
    "✅ How to implement the SARSA update rule\n",
    "\n",
    "✅ When SARSA is preferable to Q-Learning\n",
    "\n",
    "✅ How exploration affects learning in both algorithms\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| Algorithm | Type | Update Rule | Behavior |\n",
    "|-----------|------|-------------|----------|\n",
    "| **Q-Learning** | Off-policy | Uses $\\max Q(s',a')$ | Learns optimal policy, risky during training |\n",
    "| **SARSA** | On-policy | Uses $Q(s',a')$ (actual) | Learns safe policy, conservative |\n",
    "\n",
    "**When to use SARSA:**\n",
    "- When exploration is costly or dangerous\n",
    "- When you want the learned policy to account for exploration\n",
    "- In environments with severe penalties for mistakes\n",
    "\n",
    "**When to use Q-Learning:**\n",
    "- When you want to learn the optimal policy\n",
    "- When exploration costs are acceptable\n",
    "- When off-policy learning is advantageous\n",
    "\n",
    "### Next Steps:\n",
    "- Explore Expected SARSA (combines benefits of both)\n",
    "- Learn about n-step methods\n",
    "- Move on to Deep RL with DQN!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
