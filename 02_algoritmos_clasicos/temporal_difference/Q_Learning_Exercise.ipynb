{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm\n",
    "\n",
    "Welcome to the Q-Learning assignment! In this notebook, you'll implement one of the most fundamental algorithms in Reinforcement Learning. By the end of this assignment, you'll be able to:\n",
    "\n",
    "* Understand the Q-Learning algorithm and its core update rule\n",
    "* Implement the epsilon-greedy exploration strategy\n",
    "* Build a complete Q-Learning agent from scratch\n",
    "* Train and evaluate your agent on classic RL environments\n",
    "* Visualize the learning progress\n",
    "\n",
    "Q-Learning is an **off-policy** temporal difference (TD) control algorithm that learns the optimal action-value function Q*(s,a) directly, without requiring a model of the environment.\n",
    "\n",
    "## The Q-Learning Update Rule\n",
    "\n",
    "The core of Q-Learning is its update rule:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $s$ = current state\n",
    "- $a$ = action taken\n",
    "- $r$ = reward received\n",
    "- $s'$ = next state\n",
    "- $\\alpha$ = learning rate (how much to update)\n",
    "- $\\gamma$ = discount factor (how much to value future rewards)\n",
    "- $\\max_{a'} Q(s',a')$ = maximum Q-value for the next state\n",
    "\n",
    "The term in brackets is called the **TD error** (temporal difference error).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*QeoQEqWYYPs1P8yUwyaJVQ.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Q-Table Initialization](#2)\n",
    "    - [Exercise 1 - initialize_q_table](#ex-1)\n",
    "- [3 - Epsilon-Greedy Policy](#3)\n",
    "    - [Exercise 2 - epsilon_greedy_action](#ex-2)\n",
    "- [4 - Q-Learning Update](#4)\n",
    "    - [Exercise 3 - q_learning_update](#ex-3)\n",
    "- [5 - Training Loop](#5)\n",
    "    - [Exercise 4 - train_q_learning](#ex-4)\n",
    "- [6 - Testing the Complete Agent](#6)\n",
    "    - [6.1 - Train on FrozenLake](#6-1)\n",
    "    - [6.2 - Visualize Results](#6-2)\n",
    "    - [6.3 - Evaluate Agent](#6-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from q_learning_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Q-Table Initialization\n",
    "\n",
    "The Q-table stores the expected cumulative reward for each state-action pair. Initially, we don't know these values, so we initialize them.\n",
    "\n",
    "There are several initialization strategies:\n",
    "- **Zeros**: Conservative, slow to explore\n",
    "- **Random small values**: Encourages initial exploration\n",
    "- **Optimistic initialization**: Initialize with high values to encourage exploration\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - initialize_q_table\n",
    "\n",
    "Implement a function that initializes the Q-table. For discrete environments, use a 2D numpy array of shape `(n_states, n_actions)`. For environments with many states, you might want to use a dictionary (defaultdict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_q_table\n",
    "\n",
    "def initialize_q_table(n_states, n_actions, init_value=0.0):\n",
    "    \"\"\"\n",
    "    Initialize Q-table with given value.\n",
    "    \n",
    "    Arguments:\n",
    "    n_states -- number of states in the environment\n",
    "    n_actions -- number of actions in the environment\n",
    "    init_value -- initial value for Q-table entries (default: 0.0)\n",
    "    \n",
    "    Returns:\n",
    "    Q -- numpy array of shape (n_states, n_actions) initialized with init_value\n",
    "    \"\"\"\n",
    "    # (approx. 1 line)\n",
    "    # Q = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "Q = initialize_q_table(16, 4, init_value=0.0)\n",
    "print(\"Q-table shape:\", Q.shape)\n",
    "print(\"Q-table sample:\\n\", Q[:4, :])\n",
    "\n",
    "# Run the grader\n",
    "initialize_q_table_test(initialize_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Q-table shape: (16, 4)\n",
    "Q-table sample:\n",
    " [[0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]\n",
    " [0. 0. 0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Epsilon-Greedy Policy\n",
    "\n",
    "The **exploration-exploitation dilemma** is fundamental in RL:\n",
    "- **Exploitation**: Use current knowledge to maximize reward\n",
    "- **Exploration**: Try new actions to discover better strategies\n",
    "\n",
    "The **ε-greedy policy** balances these:\n",
    "- With probability $\\epsilon$: choose a random action (explore)\n",
    "- With probability $1-\\epsilon$: choose the best known action (exploit)\n",
    "\n",
    "$$\n",
    "a = \\begin{cases}\n",
    "\\arg\\max_{a'} Q(s, a') & \\text{with probability } 1-\\epsilon \\\\\n",
    "\\text{random action} & \\text{with probability } \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Typically, we start with high $\\epsilon$ (e.g., 1.0) and decay it over time.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - epsilon_greedy_action\n",
    "\n",
    "Implement the epsilon-greedy action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: epsilon_greedy_action\n",
    "\n",
    "def epsilon_greedy_action(Q, state, n_actions, epsilon):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-table, numpy array of shape (n_states, n_actions)\n",
    "    state -- current state (integer)\n",
    "    n_actions -- number of possible actions\n",
    "    epsilon -- exploration rate (0 to 1)\n",
    "    \n",
    "    Returns:\n",
    "    action -- selected action (integer)\n",
    "    \"\"\"\n",
    "    # (approx. 4-5 lines)\n",
    "    # With probability epsilon, choose random action\n",
    "    # Otherwise, choose the action with highest Q-value for current state\n",
    "    # Hint: use np.random.random() to generate random number in [0,1)\n",
    "    # Hint: use np.argmax() to find action with highest Q-value\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "np.random.seed(42)\n",
    "Q_test = np.array([[0.1, 0.5, 0.2, 0.3],\n",
    "                   [0.4, 0.1, 0.6, 0.2]])\n",
    "\n",
    "# Test with epsilon=0 (always exploit)\n",
    "action = epsilon_greedy_action(Q_test, state=0, n_actions=4, epsilon=0.0)\n",
    "print(f\"Epsilon=0.0, State=0, Action selected: {action} (should be 1, highest Q-value)\")\n",
    "\n",
    "# Test with epsilon=1 (always explore)\n",
    "actions = [epsilon_greedy_action(Q_test, state=0, n_actions=4, epsilon=1.0) for _ in range(100)]\n",
    "print(f\"Epsilon=1.0: Actions are random: {len(set(actions)) > 1}\")\n",
    "\n",
    "# Run the grader\n",
    "epsilon_greedy_action_test(epsilon_greedy_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Q-Learning Update\n",
    "\n",
    "Now we implement the core Q-Learning update rule. After taking action $a$ in state $s$ and observing reward $r$ and next state $s'$, we update:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right]$$\n",
    "\n",
    "Let's break this down:\n",
    "1. **Current estimate**: $Q(s,a)$\n",
    "2. **TD target**: $r + \\gamma \\max_{a'} Q(s',a')$ (immediate reward + discounted best future value)\n",
    "3. **TD error**: $r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ (difference between target and current)\n",
    "4. **Update**: Move Q(s,a) toward the target by a fraction $\\alpha$\n",
    "\n",
    "**Special case**: If $s'$ is a terminal state (episode ended), there's no future reward, so:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r - Q(s,a)\\right]$$\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - q_learning_update\n",
    "\n",
    "Implement the Q-Learning update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: q_learning_update\n",
    "\n",
    "def q_learning_update(Q, state, action, reward, next_state, done, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Update Q-table using Q-Learning rule.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-table, numpy array of shape (n_states, n_actions)\n",
    "    state -- current state\n",
    "    action -- action taken\n",
    "    reward -- reward received\n",
    "    next_state -- next state after taking action\n",
    "    done -- boolean, True if next_state is terminal\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    Q -- updated Q-table\n",
    "    td_error -- TD error (for tracking learning progress)\n",
    "    \"\"\"\n",
    "    # (approx. 5-7 lines)\n",
    "    # Step 1: Get current Q-value\n",
    "    # Step 2: Calculate TD target\n",
    "    #         If done: target = reward\n",
    "    #         Else: target = reward + gamma * max Q-value of next_state\n",
    "    # Step 3: Calculate TD error = target - current Q-value\n",
    "    # Step 4: Update Q-value: Q[state, action] += alpha * td_error\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "Q_test = np.zeros((4, 2))\n",
    "Q_test[1] = [0.5, 0.3]  # Q-values for next_state\n",
    "\n",
    "# Test non-terminal update\n",
    "Q_updated, td_error = q_learning_update(\n",
    "    Q_test.copy(), state=0, action=0, reward=1.0, \n",
    "    next_state=1, done=False, alpha=0.1, gamma=0.9\n",
    ")\n",
    "print(f\"Non-terminal update:\")\n",
    "print(f\"  Q[0,0] before: 0.0\")\n",
    "print(f\"  Q[0,0] after: {Q_updated[0, 0]:.4f}\")\n",
    "print(f\"  TD error: {td_error:.4f}\")\n",
    "\n",
    "# Test terminal update\n",
    "Q_updated, td_error = q_learning_update(\n",
    "    Q_test.copy(), state=0, action=0, reward=1.0,\n",
    "    next_state=1, done=True, alpha=0.1, gamma=0.9\n",
    ")\n",
    "print(f\"\\nTerminal update:\")\n",
    "print(f\"  Q[0,0] after: {Q_updated[0, 0]:.4f}\")\n",
    "print(f\"  TD error: {td_error:.4f}\")\n",
    "\n",
    "# Run the grader\n",
    "q_learning_update_test(q_learning_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output (approximately):**\n",
    "```\n",
    "Non-terminal update:\n",
    "  Q[0,0] before: 0.0\n",
    "  Q[0,0] after: 0.1450\n",
    "  TD error: 1.4500\n",
    "\n",
    "Terminal update:\n",
    "  Q[0,0] after: 0.1000\n",
    "  TD error: 1.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Training Loop\n",
    "\n",
    "Now let's put it all together! The training loop follows this structure:\n",
    "\n",
    "```\n",
    "For each episode:\n",
    "    Initialize state s\n",
    "    For each step in episode:\n",
    "        Choose action a using ε-greedy policy\n",
    "        Take action a, observe reward r and next state s'\n",
    "        Update Q(s,a) using Q-Learning rule\n",
    "        s ← s'\n",
    "    Decay ε\n",
    "```\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - train_q_learning\n",
    "\n",
    "Implement the complete Q-Learning training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_q_learning\n",
    "\n",
    "def train_q_learning(env, n_episodes=1000, alpha=0.1, gamma=0.99, \n",
    "                     epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                     max_steps=100):\n",
    "    \"\"\"\n",
    "    Train Q-Learning agent.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gymnasium environment\n",
    "    n_episodes -- number of episodes to train\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    epsilon -- initial exploration rate\n",
    "    epsilon_decay -- decay rate for epsilon after each episode\n",
    "    epsilon_min -- minimum value for epsilon\n",
    "    max_steps -- maximum steps per episode\n",
    "    \n",
    "    Returns:\n",
    "    Q -- trained Q-table\n",
    "    rewards_history -- list of total rewards per episode\n",
    "    epsilon_history -- list of epsilon values per episode\n",
    "    \"\"\"\n",
    "    # Initialize Q-table\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = initialize_q_table(n_states, n_actions)\n",
    "    \n",
    "    rewards_history = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    # Training loop (approx. 20-25 lines)\n",
    "    # For each episode:\n",
    "    #   1. Reset environment to get initial state\n",
    "    #   2. Initialize episode_reward = 0\n",
    "    #   3. For each step (up to max_steps):\n",
    "    #      a. Select action using epsilon_greedy_action\n",
    "    #      b. Take action in environment (env.step)\n",
    "    #      c. Update Q-table using q_learning_update\n",
    "    #      d. Add reward to episode_reward\n",
    "    #      e. Update state\n",
    "    #      f. If done, break\n",
    "    #   4. Decay epsilon: epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    #   5. Store episode_reward and epsilon in histories\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, rewards_history, epsilon_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Testing the Complete Agent\n",
    "\n",
    "Now let's test your complete Q-Learning implementation on the FrozenLake environment!\n",
    "\n",
    "**FrozenLake Environment:**\n",
    "- 4x4 grid world\n",
    "- Start at top-left (S)\n",
    "- Goal: reach bottom-right (G)\n",
    "- Holes (H) cause failure\n",
    "- Frozen tiles (F) are safe\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - Train on FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment (is_slippery=False makes it deterministic)\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "print(\"Training Q-Learning agent on FrozenLake...\")\n",
    "print(f\"States: {env.observation_space.n}\")\n",
    "print(f\"Actions: {env.action_space.n}\")\n",
    "print()\n",
    "\n",
    "# Train the agent\n",
    "Q, rewards_history, epsilon_history = train_q_learning(\n",
    "    env,\n",
    "    n_episodes=2000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final epsilon: {epsilon_history[-1]:.4f}\")\n",
    "print(f\"Average reward (last 100 episodes): {np.mean(rewards_history[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(rewards_history, alpha=0.3, label='Episode reward')\n",
    "window = 50\n",
    "if len(rewards_history) >= window:\n",
    "    moving_avg = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(rewards_history)), moving_avg, \n",
    "             label=f'Moving average ({window} episodes)', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Training Progress')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon decay\n",
    "ax2.plot(epsilon_history)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon (ε)')\n",
    "ax2.set_title('Exploration Rate Decay')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize learned Q-values\n",
    "print(\"\\nLearned Q-table (first 8 states):\")\n",
    "print(Q[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-3'></a>\n",
    "### 6.3 - Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, Q, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate trained agent (no exploration).\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Always exploit (epsilon=0)\n",
    "            action = np.argmax(Q[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(total_rewards), np.std(total_rewards)\n",
    "\n",
    "# Evaluate\n",
    "mean_reward, std_reward = evaluate_agent(env, Q, n_episodes=100)\n",
    "print(f\"\\nEvaluation over 100 episodes:\")\n",
    "print(f\"Average reward: {mean_reward:.4f} ± {std_reward:.4f}\")\n",
    "print(f\"Success rate: {mean_reward * 100:.1f}%\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Q-Learning from scratch! Here's what you've learned:\n",
    "\n",
    "✅ How to initialize and maintain a Q-table\n",
    "\n",
    "✅ How to balance exploration and exploitation with ε-greedy\n",
    "\n",
    "✅ How to apply the Q-Learning update rule\n",
    "\n",
    "✅ How to train a complete RL agent\n",
    "\n",
    "✅ How to evaluate and visualize learning progress\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Q-Learning is off-policy**: It learns the optimal policy regardless of the exploration policy used\n",
    "2. **Exploration is crucial**: Without proper exploration (ε-greedy), the agent might not find the optimal policy\n",
    "3. **Hyperparameters matter**: α (learning rate), γ (discount), and ε (exploration) significantly affect learning\n",
    "4. **Convergence takes time**: Q-Learning needs many episodes to converge, especially with high exploration\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try Q-Learning on other environments (CartPole, Taxi, etc.)\n",
    "- Experiment with different hyperparameters\n",
    "- Compare Q-Learning with SARSA (on-policy alternative)\n",
    "- Learn about Deep Q-Networks (DQN) for continuous state spaces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
