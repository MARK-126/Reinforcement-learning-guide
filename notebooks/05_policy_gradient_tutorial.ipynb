{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods: REINFORCE and Actor-Critic\n",
    "\n",
    "Welcome to the comprehensive tutorial on Policy Gradient methods in Deep Reinforcement Learning!\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the theoretical foundations of Policy Gradient Theorem\n",
    "- Implement REINFORCE algorithm with and without baseline\n",
    "- Implement Actor-Critic methods with Generalized Advantage Estimation (GAE)\n",
    "- Compare different policy gradient approaches empirically\n",
    "- Apply these methods to discrete and continuous action spaces\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1- Packages](#1)\n",
    "- [2 - Mathematical Foundations](#2)\n",
    "    - [2.1 - Policy Gradient Theorem](#2-1)\n",
    "    - [2.2 - REINFORCE Algorithm](#2-2)\n",
    "    - [2.3 - Baseline for Variance Reduction](#2-3)\n",
    "    - [2.4 - Actor-Critic Methods](#2-4)\n",
    "    - [2.5 - Generalized Advantage Estimation](#2-5)\n",
    "- [3 - Exercise 1: Implement Policy Network](#ex-1)\n",
    "- [4 - Exercise 2: Implement REINFORCE Loss](#ex-2)\n",
    "- [5 - Exercise 3: Implement Baseline (Value Network)](#ex-3)\n",
    "- [6 - Exercise 4: Implement Actor-Critic](#ex-4)\n",
    "- [7 - Exercise 5: Implement GAE](#ex-5)\n",
    "- [8 - Experimental Comparison](#8)\n",
    "    - [8.1 - REINFORCE vs A2C](#8-1)\n",
    "    - [8.2 - Impact of Baseline](#8-2)\n",
    "    - [8.3 - Continuous Action Spaces](#8-3)\n",
    "- [9 - Conclusions](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gymnasium as gym\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import test utilities\n",
    "from pg_utils import (\n",
    "    test_implement_policy_network,\n",
    "    test_implement_reinforce_loss,\n",
    "    test_implement_baseline,\n",
    "    test_implement_actor_critic,\n",
    "    test_implement_gae\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Mathematical Foundations\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Policy Gradient Theorem\n",
    "\n",
    "The **Policy Gradient Theorem** is the cornerstone of policy-based reinforcement learning:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)]$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ = policy parameters\n",
    "- $\\rho^\\pi(s)$ = state visitation distribution under $\\pi$\n",
    "- $\\pi_\\theta(a|s)$ = policy (probability of action $a$ given state $s$)\n",
    "- $Q^\\pi(s,a)$ = state-action value function\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ = score function (policy gradient)\n",
    "\n",
    "**Key insight:** The gradient is proportional to the log-probability of an action, weighted by how good that action is (Q-value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - REINFORCE Algorithm\n",
    "\n",
    "**REINFORCE** estimates $Q^\\pi(s,a)$ using Monte Carlo returns:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$$\n",
    "\n",
    "where:\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n",
    "\n",
    "**Update rule:**\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "**Advantages:**\n",
    "- Unbiased gradient estimates\n",
    "- Works with episodic tasks\n",
    "- Handles continuous action spaces naturally\n",
    "\n",
    "**Disadvantages:**\n",
    "- HIGH VARIANCE: Must wait until episode end to compute gradients\n",
    "- SLOW CONVERGENCE: Requires many samples\n",
    "- No bootstrapping available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Baseline for Variance Reduction\n",
    "\n",
    "Subtracting a baseline $b(s)$ reduces variance **without introducing bias**:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\sum_{t} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) [G_t - b(s_t)]$$\n",
    "\n",
    "The **optimal baseline** is the state value function:\n",
    "$$b^*(s) = V^\\pi(s) = \\mathbb{E}[G_t | s_t = s]$$\n",
    "\n",
    "This leads to the **Advantage Function**:\n",
    "$$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$$\n",
    "\n",
    "which tells us how much better action $a$ is compared to the average action in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-4'></a>\n",
    "### 2.4 - Actor-Critic Methods\n",
    "\n",
    "**Actor-Critic** combines two networks:\n",
    "\n",
    "1. **Actor** (Policy Network): $\\pi_\\theta(a|s)$\n",
    "   - Updates using policy gradient\n",
    "   - Goal: Learn better actions\n",
    "\n",
    "2. **Critic** (Value Network): $V_\\phi(s)$\n",
    "   - Updates using Temporal Difference (TD) learning\n",
    "   - Goal: Better baseline for variance reduction\n",
    "\n",
    "**Advantages over REINFORCE:**\n",
    "- LOWER VARIANCE: TD bootstrap instead of Monte Carlo\n",
    "- FASTER CONVERGENCE: Critic provides immediate feedback\n",
    "- ONLINE UPDATES: Don't need to wait for episode end\n",
    "\n",
    "**Loss functions:**\n",
    "\n",
    "Actor loss (policy improvement):\n",
    "$$L_{actor} = -\\log \\pi_\\theta(a|s) \\cdot A(s,a)$$\n",
    "\n",
    "Critic loss (value estimation):\n",
    "$$L_{critic} = (r + \\gamma V_\\phi(s') - V_\\phi(s))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-5'></a>\n",
    "### 2.5 - Generalized Advantage Estimation (GAE)\n",
    "\n",
    "GAE provides a flexible interpolation between TD (low variance, high bias) and Monte Carlo (high variance, no bias):\n",
    "\n",
    "$$A_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where the TD error is:\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "**The $\\lambda$ parameter controls bias-variance tradeoff:**\n",
    "- $\\lambda = 0$: Pure TD(0) - low variance, high bias\n",
    "- $\\lambda = 0.95$: Recommended (good balance)\n",
    "- $\\lambda = 1$: Pure Monte Carlo - high variance, no bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "## 3 - Exercise 1: Implement Policy Network\n",
    "\n",
    "**Objective:** Create a neural network that outputs action probabilities (for discrete actions) or action distribution parameters (for continuous actions).\n",
    "\n",
    "Complete the `PolicyNetwork` class below. This network should:\n",
    "- Take state as input\n",
    "- Output logits for discrete actions OR (mean, log_std) for continuous actions\n",
    "- Have a method `get_action()` that samples actions and computes log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: PolicyNetwork\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network for policy π_θ(a|s)\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 hidden_dims: List[int] = [128, 128],\n",
    "                 continuous: bool = False):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Build shared hidden layers\n",
    "        # Then create action head(s) based on continuous flag\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        if continuous:\n",
    "            # For continuous: output mean and log standard deviation\n",
    "            self.mean_layer = nn.Linear(prev_dim, action_dim)\n",
    "            self.log_std_layer = nn.Linear(prev_dim, action_dim)\n",
    "        else:\n",
    "            # For discrete: output logits for each action\n",
    "            self.action_head = nn.Linear(prev_dim, action_dim)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        \"\"\"Forward pass through network\"\"\"\n",
    "        x = self.shared_layers(state)\n",
    "        \n",
    "        if self.continuous:\n",
    "            mean = self.mean_layer(x)\n",
    "            log_std = torch.clamp(self.log_std_layer(x), -20, 2)\n",
    "            return mean, log_std\n",
    "        else:\n",
    "            logits = self.action_head(x)\n",
    "            return logits, None\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        if self.continuous:\n",
    "            mean, log_std = self.forward(state)\n",
    "            std = log_std.exp()\n",
    "            \n",
    "            if deterministic:\n",
    "                return mean, None, None\n",
    "            \n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            entropy = dist.entropy().sum(dim=-1)\n",
    "            return action, log_prob, entropy\n",
    "        else:\n",
    "            logits, _ = self.forward(state)\n",
    "            \n",
    "            if deterministic:\n",
    "                action = logits.argmax(dim=-1)\n",
    "                return action, None, None\n",
    "            \n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            return action, log_prob, entropy\n",
    "\n",
    "print(\"PolicyNetwork defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Exercise 1\n",
    "print(\"Testing Exercise 1: PolicyNetwork\")\n",
    "test_implement_policy_network()\n",
    "print(\"\\nAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember:**\n",
    "- A policy network outputs action probabilities (discrete) or action distribution parameters (continuous)\n",
    "- The score function $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ is what we use to update the policy\n",
    "- For continuous actions, we typically use a Gaussian distribution with learned mean and standard deviation\n",
    "- The log probability is crucial for computing policy gradients\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "## 4 - Exercise 2: Implement REINFORCE Loss\n",
    "\n",
    "**Objective:** Implement the REINFORCE loss function that optimizes the policy to maximize expected returns.\n",
    "\n",
    "The loss function should:\n",
    "- Take log probabilities and returns (or advantages) as input\n",
    "- Compute: $L = -\\frac{1}{N} \\sum_t \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "- Be differentiable with respect to policy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_reinforce_loss\n",
    "\n",
    "def compute_reinforce_loss(log_probs: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute REINFORCE loss (negative expected return weighted by log probabilities)\n",
    "    \n",
    "    Arguments:\n",
    "    log_probs -- log probabilities of taken actions, shape (batch_size,)\n",
    "    returns -- cumulative discounted returns, shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- scalar loss value (we want to minimize this)\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # Compute the negative expected return weighted by log probabilities\n",
    "    # This is: -mean(log_prob * return)\n",
    "    loss = -(log_probs * returns).mean()\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return loss\n",
    "\n",
    "print(\"compute_reinforce_loss defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Exercise 2\n",
    "print(\"Testing Exercise 2: REINFORCE Loss\")\n",
    "test_implement_reinforce_loss()\n",
    "print(\"\\nAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember:**\n",
    "- REINFORCE loss is: $L = -\\frac{1}{N} \\sum \\log \\pi(a|s) \\cdot G$\n",
    "- We negate the return because optimizers minimize loss (we want to maximize returns)\n",
    "- High-variance returns can make training unstable (this is why baselines are important)\n",
    "- The loss is unbiased: $\\mathbb{E}[\\nabla L] = \\nabla J(\\theta)$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "## 5 - Exercise 3: Implement Baseline (Value Network)\n",
    "\n",
    "**Objective:** Implement a value network that estimates $V(s)$, used as a baseline to reduce variance.\n",
    "\n",
    "Requirements:\n",
    "- Input: state (shape: [batch_size, state_dim])\n",
    "- Output: scalar value estimate for each state (shape: [batch_size])\n",
    "- Used to compute advantages: $A = G_t - V(s_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ValueNetwork\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Neural network for value function V(s)\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, hidden_dims: List[int] = [128, 128]):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Build a neural network:\n",
    "        # Input: state_dim\n",
    "        # Hidden layers with ReLU activations\n",
    "        # Output: 1 (scalar value)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returns V(s)\"\"\"\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "print(\"ValueNetwork defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Exercise 3\n",
    "print(\"Testing Exercise 3: Baseline (Value Network)\")\n",
    "test_implement_baseline()\n",
    "print(\"\\nAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember:**\n",
    "- The baseline $b(s) = V(s)$ reduces variance without adding bias\n",
    "- Optimal baseline is the state value function\n",
    "- Advantages $A = G_t - V(s_t)$ tell us how much better/worse an action is vs. average\n",
    "- The value network is trained with MSE loss: $L = (G_t - V(s_t))^2$\n",
    "- Baselines are critical for practical policy gradient learning\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "## 6 - Exercise 4: Implement Actor-Critic\n",
    "\n",
    "**Objective:** Combine the policy network (actor) and value network (critic) into a unified training procedure.\n",
    "\n",
    "The actor-critic algorithm should:\n",
    "1. Collect experience using the current policy\n",
    "2. Compute TD errors: $\\delta = r + \\gamma V(s') - V(s)$\n",
    "3. Update actor using policy gradient weighted by advantages\n",
    "4. Update critic using TD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ActorCriticAgent\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    \"\"\"Advantage Actor-Critic (A2C) agent\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 continuous: bool = False,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 1e-3,\n",
    "                 gamma: float = 0.99,\n",
    "                 entropy_coef: float = 0.01,\n",
    "                 normalize_advantages: bool = True,\n",
    "                 hidden_dims: List[int] = [256, 256]):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.continuous = continuous\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "        self.device = device\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Initialize actor (PolicyNetwork) and critic (ValueNetwork)\n",
    "        # Create Adam optimizers for both\n",
    "        \n",
    "        self.actor = PolicyNetwork(state_dim, action_dim, hidden_dims, continuous).to(device)\n",
    "        self.critic = ValueNetwork(state_dim, hidden_dims).to(device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        self.history = {\n",
    "            'episode_rewards': [],\n",
    "            'actor_losses': [],\n",
    "            'critic_losses': [],\n",
    "        }\n",
    "    \n",
    "    def compute_td_error(self, rewards: List[float], values: torch.Tensor,\n",
    "                        next_values: torch.Tensor, dones: List[bool]) -> torch.Tensor:\n",
    "        \"\"\"Compute TD errors for advantage estimation\"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        # δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "        td_errors = []\n",
    "        for t in range(len(rewards)):\n",
    "            next_val = next_values[t] if (t == len(rewards) - 1) else values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * next_val * (1 - dones[t]) - values[t]\n",
    "            td_errors.append(delta)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        return torch.stack(td_errors)\n",
    "    \n",
    "    def train_step(self, states: List[np.ndarray], actions: List,\n",
    "                  rewards: List[float], next_states: List[np.ndarray],\n",
    "                  dones: List[bool]) -> Dict[str, float]:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        next_states_tensor = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        \n",
    "        # Compute values\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states_tensor)\n",
    "            next_values = self.critic(next_states_tensor)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = self.compute_td_error(rewards, values, next_values, dones)\n",
    "        \n",
    "        if self.normalize_advantages:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        returns = advantages + values\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Get log probabilities and compute actor loss\n",
    "        # Compute critic loss\n",
    "        # Update both networks\n",
    "        \n",
    "        if self.continuous:\n",
    "            actions_tensor = torch.FloatTensor(np.array(actions)).to(self.device)\n",
    "            mean, log_std = self.actor.forward(states_tensor)\n",
    "            std = log_std.exp()\n",
    "            dist = Normal(mean, std)\n",
    "            log_probs = dist.log_prob(actions_tensor).sum(dim=-1)\n",
    "            entropies = dist.entropy().sum(dim=-1)\n",
    "        else:\n",
    "            actions_tensor = torch.LongTensor(actions).to(self.device)\n",
    "            logits, _ = self.actor.forward(states_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            log_probs = dist.log_prob(actions_tensor)\n",
    "            entropies = dist.entropy()\n",
    "        \n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        entropy_loss = -entropies.mean()\n",
    "        total_actor_loss = actor_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        values_new = self.critic(states_tensor)\n",
    "        critic_loss = F.mse_loss(values_new, returns.detach())\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        total_actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item(),\n",
    "        }\n",
    "\n",
    "print(\"ActorCriticAgent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Exercise 4\n",
    "print(\"Testing Exercise 4: Actor-Critic\")\n",
    "test_implement_actor_critic()\n",
    "print(\"\\nAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember:**\n",
    "- Actor-Critic combines policy gradient (actor) with value learning (critic)\n",
    "- TD errors $\\delta_t = r_t + \\gamma V(s') - V(s)$ estimate the advantage\n",
    "- Actor loss: $L_{actor} = -\\log \\pi(a|s) \\cdot \\delta_t$\n",
    "- Critic loss: $L_{critic} = \\delta_t^2$\n",
    "- Actor-Critic converges faster than pure REINFORCE due to lower variance\n",
    "- Both networks benefit from sharing hidden layers (but we use separate for clarity)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-5'></a>\n",
    "## 7 - Exercise 5: Implement GAE\n",
    "\n",
    "**Objective:** Implement Generalized Advantage Estimation (GAE) for more efficient advantage computation.\n",
    "\n",
    "GAE formula:\n",
    "$$A_t^{GAE} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "This interpolates between:\n",
    "- $\\lambda = 0$: Pure 1-step TD (low variance, high bias)\n",
    "- $\\lambda = 1$: Monte Carlo returns (high variance, no bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_gae\n",
    "\n",
    "def compute_gae(rewards: List[float], values: torch.Tensor,\n",
    "                next_values: torch.Tensor, dones: List[bool],\n",
    "                gamma: float = 0.99, gae_lambda: float = 0.95) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation\n",
    "    \n",
    "    Arguments:\n",
    "    rewards -- list of rewards for each timestep\n",
    "    values -- value estimates V(s_t), shape (T,)\n",
    "    next_values -- value estimates V(s_{t+1}), shape (T,)\n",
    "    dones -- whether episode ended at each timestep\n",
    "    gamma -- discount factor\n",
    "    gae_lambda -- GAE parameter (0-1)\n",
    "    \n",
    "    Returns:\n",
    "    advantages -- estimated advantages A(s,a)\n",
    "    returns -- estimated returns G_t\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # Compute TD errors: δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    # Then compute GAE by accumulating: A_t = δ_t + (γλ) * A_{t+1}\n",
    "    \n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_value = next_values[t]\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        # TD error\n",
    "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]\n",
    "        \n",
    "        # GAE recursion\n",
    "        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = advantages + values\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "print(\"compute_gae defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Exercise 5\n",
    "print(\"Testing Exercise 5: GAE\")\n",
    "test_implement_gae()\n",
    "print(\"\\nAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember:**\n",
    "- GAE is computed backwards through the episode: $A_t = \\delta_t + (\\gamma\\lambda) A_{t+1}$\n",
    "- GAE parameter $\\lambda$ controls the bias-variance tradeoff\n",
    "- $\\lambda = 0.95$ is a good default that works well in practice\n",
    "- GAE allows efficient advantage estimation without waiting for episode end\n",
    "- Returns are computed as: $G_t = A_t + V(s_t)$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Experimental Comparison\n",
    "\n",
    "Now let's train agents and compare different policy gradient approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"Environment: CartPole-v1\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Action dimension: {action_dim}\")\n",
    "print(f\"  Action space: Discrete ({action_dim} actions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8-1'></a>\n",
    "### 8.1 - REINFORCE vs A2C Comparison\n",
    "\n",
    "Let's compare the performance of REINFORCE with baseline vs Actor-Critic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_agent(agent, env, n_episodes=300, max_steps=500, agent_name=\"Agent\"):\n",
    "    \"\"\"Train an agent for n_episodes\"\"\"\n",
    "    print(f\"Training {agent_name}...\\n\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Get action from policy\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = agent.actor.get_action(\n",
    "                    torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                )\n",
    "            action = action.item()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(float(done))\n",
    "            episode_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train\n",
    "        metrics = agent.train_step(states, actions, rewards, next_states, dones)\n",
    "        agent.history['episode_rewards'].append(episode_reward)\n",
    "        agent.history['actor_losses'].append(metrics['actor_loss'])\n",
    "        agent.history['critic_losses'].append(metrics['critic_loss'])\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(agent.history['episode_rewards'][-50:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | Avg Reward (50): {avg_reward:.1f}\")\n",
    "    \n",
    "    print(\"Training complete!\\n\")\n",
    "    return agent.history\n",
    "\n",
    "# Train Actor-Critic\n",
    "ac_agent = ActorCriticAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    continuous=False,\n",
    "    actor_lr=3e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    entropy_coef=0.01\n",
    ")\n",
    "\n",
    "history_ac = train_agent(ac_agent, env, n_episodes=300, agent_name=\"Actor-Critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Performance Comparison Table\n",
    "\n",
    "Let's create a comparison table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute statistics\n",
    "ac_last100 = np.mean(history_ac['episode_rewards'][-100:])\n",
    "ac_final_var = np.var(history_ac['episode_rewards'][-100:])\n",
    "ac_final_loss = np.mean(history_ac['actor_losses'][-10:])\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Avg Reward (last 100 episodes)',\n",
    "        'Reward Variance (last 100)',\n",
    "        'Final Actor Loss',\n",
    "        'Final Critic Loss',\n",
    "        'Training Stability'\n",
    "    ],\n",
    "    'Actor-Critic': [\n",
    "        f\"{ac_last100:.1f}\",\n",
    "        f\"{ac_final_var:.1f}\",\n",
    "        f\"{ac_final_loss:.4f}\",\n",
    "        f\"{np.mean(history_ac['critic_losses'][-10:]):.4f}\",\n",
    "        \"Very Good\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE COMPARISON: Actor-Critic Methods\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Summary\n",
    "\n",
    "<table style=\"border: 2px solid black; margin-left: 20px;\">\n",
    "    <tr style=\"background-color: #4CAF50; color: white;\">\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Method</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Variance</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Convergence Speed</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Stability</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Continuous Actions</th>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #f2f2f2;\">\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><strong>REINFORCE</strong></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">High</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Slow</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Good</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Excellent</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #ffffff;\">\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><strong>A2C/Actor-Critic</strong></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Low</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Fast</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Very Good</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Excellent</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #f2f2f2;\">\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><strong>PPO</strong></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Very Low</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Very Fast</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Excellent</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Excellent</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8-2'></a>\n",
    "### 8.2 - Impact of Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 20\n",
    "rewards = history_ac['episode_rewards']\n",
    "ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Rewards\n",
    "ax = axes[0]\n",
    "ax.plot(rewards, alpha=0.3, color='blue')\n",
    "ax.plot(range(window-1, len(rewards)), ma, linewidth=2.5, color='darkblue')\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Episode Reward', fontsize=11)\n",
    "ax.set_title('Actor-Critic: Training Progress', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Actor vs Critic Loss\n",
    "ax = axes[1]\n",
    "actor_losses = history_ac['actor_losses']\n",
    "critic_losses = history_ac['critic_losses']\n",
    "\n",
    "ax.plot(actor_losses, alpha=0.3, label='Actor Loss', color='green')\n",
    "ax.plot(critic_losses, alpha=0.3, label='Critic Loss', color='red')\n",
    "\n",
    "if len(actor_losses) >= window:\n",
    "    actor_ma = np.convolve(actor_losses, np.ones(window)/window, mode='valid')\n",
    "    critic_ma = np.convolve(critic_losses, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(actor_losses)), actor_ma, linewidth=2, color='darkgreen')\n",
    "    ax.plot(range(window-1, len(critic_losses)), critic_ma, linewidth=2, color='darkred')\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Loss Evolution', fontsize=12)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8-3'></a>\n",
    "### 8.3 - Continuous Action Spaces\n",
    "\n",
    "Let's test our implementation on a continuous control task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pendulum environment (continuous actions)\n",
    "env_continuous = gym.make('Pendulum-v1')\n",
    "state_dim_cont = env_continuous.observation_space.shape[0]\n",
    "action_dim_cont = env_continuous.action_space.shape[0]\n",
    "\n",
    "print(f\"Environment: Pendulum-v1\")\n",
    "print(f\"  State dimension: {state_dim_cont} (continuous)\")\n",
    "print(f\"  Action dimension: {action_dim_cont} (continuous)\")\n",
    "print(f\"  Action bounds: [{env_continuous.action_space.low}, {env_continuous.action_space.high}]\")\n",
    "\n",
    "# Create continuous action actor-critic agent\n",
    "ac_continuous = ActorCriticAgent(\n",
    "    state_dim=state_dim_cont,\n",
    "    action_dim=action_dim_cont,\n",
    "    continuous=True,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    entropy_coef=0.001  # Lower for continuous\n",
    ")\n",
    "\n",
    "print(\"\\nTraining continuous control agent...\")\n",
    "history_continuous = train_agent(ac_continuous, env_continuous, \n",
    "                                n_episodes=200, max_steps=200,\n",
    "                                agent_name=\"Continuous AC\")\n",
    "\n",
    "print(f\"Final avg reward (Pendulum): {np.mean(history_continuous['episode_rewards'][-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9 - Conclusions and Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Policy Gradient Theorem** is the theoretical foundation for optimizing policies directly\n",
    "   - Enables handling of continuous action spaces naturally\n",
    "   - Provides unbiased gradient estimates\n",
    "\n",
    "2. **REINFORCE** is the simplest policy gradient algorithm\n",
    "   - Uses full episode returns as targets\n",
    "   - Suffers from high variance but is guaranteed to converge\n",
    "   - Baseline (value function) crucially reduces variance\n",
    "\n",
    "3. **Actor-Critic** methods significantly improve upon REINFORCE\n",
    "   - Actor learns the policy, Critic estimates value function\n",
    "   - TD bootstrap in critic reduces variance\n",
    "   - Enables online learning within episodes\n",
    "\n",
    "4. **GAE** provides elegant bias-variance tradeoff\n",
    "   - Interpolates between TD and Monte Carlo\n",
    "   - Single $\\lambda$ parameter controls the tradeoff\n",
    "   - $\\lambda = 0.95$ is nearly always a good choice\n",
    "\n",
    "5. **Practical considerations**\n",
    "   - Normalize advantages for stable training\n",
    "   - Clip gradients to prevent instability\n",
    "   - Entropy regularization encourages exploration\n",
    "   - GAE is nearly always preferable to raw TD or MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "To extend your knowledge of policy gradient methods:\n",
    "\n",
    "1. **PPO (Proximal Policy Optimization)**\n",
    "   - Adds clipping to limit policy updates\n",
    "   - State-of-the-art performance and stability\n",
    "   - Recommended for practical applications\n",
    "\n",
    "2. **TRPO (Trust Region Policy Optimization)**\n",
    "   - Constrains updates using KL divergence\n",
    "   - Stronger theoretical guarantees\n",
    "   - More complex to implement\n",
    "\n",
    "3. **A3C (Asynchronous Advantage Actor-Critic)**\n",
    "   - Parallel training with multiple workers\n",
    "   - No need for replay buffer\n",
    "   - Excellent for distributed systems\n",
    "\n",
    "4. **SAC (Soft Actor-Critic)**\n",
    "   - Off-policy learning\n",
    "   - Entropy regularization in reward\n",
    "   - Superior exploration properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ac_reward = np.mean(history_ac['episode_rewards'][-100:])\n",
    "ac_reward_std = np.std(history_ac['episode_rewards'][-100:])\n",
    "\n",
    "print(f\"\\nActor-Critic (CartPole):\")\n",
    "print(f\"  Average reward (last 100 episodes): {ac_reward:.2f} ± {ac_reward_std:.2f}\")\n",
    "print(f\"  Final actor loss: {np.mean(history_ac['actor_losses'][-10:]):.4f}\")\n",
    "print(f\"  Final critic loss: {np.mean(history_ac['critic_losses'][-10:]):.4f}\")\n",
    "\n",
    "cont_reward = np.mean(history_continuous['episode_rewards'][-50:])\n",
    "print(f\"\\nContinuous Control (Pendulum):\")\n",
    "print(f\"  Average reward (last 50 episodes): {cont_reward:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Congratulations! You've completed the Policy Gradient tutorial.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
