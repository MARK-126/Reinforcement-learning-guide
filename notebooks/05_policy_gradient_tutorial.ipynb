{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Completo: Policy Gradient - REINFORCE y Actor-Critic\n",
    "\n",
    "Exploraremos Policy Gradient, uno de los pilares del Aprendizaje por Refuerzo profundo.\n",
    "\n",
    "## Contenido:\n",
    "1. Introducción a Policy Gradient (3 celdas)\n",
    "2. Matemática: Policy Gradient Theorem (5 celdas con LaTeX)\n",
    "3. REINFORCE - Implementación (13 celdas con CartPole)\n",
    "4. Actor-Critic con GAE (13 celdas)\n",
    "5. Comparación REINFORCE vs A2C (8 celdas)\n",
    "6. Experimentos Prácticos (6 celdas)\n",
    "7. Ejercicios (5 celdas)\n",
    "8. Conclusiones (3 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 1: Introducción a Policy Gradient\n",
    "\n",
    "### ¿Qué es Policy Gradient?\n",
    "\n",
    "Policy Gradient es una clase de algoritmos que **optimiza directamente la política** en lugar de estimar una función de valor.\n",
    "\n",
    "**Diferencia clave:**\n",
    "- **Value-based (DQN)**: Aprende Q(s,a), luego extrae política con argmax\n",
    "- **Policy-based (PG)**: Aprende política π(a|s) directamente\n",
    "\n",
    "### Ventajas de Policy Gradient\n",
    "1. **Acciones continuas**: Maneja naturalmente espacios de acción continuos\n",
    "2. **Convergencia garantizada**: A mínimo local (no global)\n",
    "3. **Exploración stochástica**: La política es probabilística por naturaleza\n",
    "4. **Mayor estabilidad**: Sin bootstrapping circular como en DQN\n",
    "\n",
    "### Desventajas\n",
    "1. **Alta varianza**: Estimadores muy ruidosos\n",
    "2. **Lenta convergencia**: Necesita muchas muestras\n",
    "3. **Local optima**: Solo garantiza convergencia a mínimo local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline de Policy Gradient\n",
    "\n",
    "| Año | Método | Característica |\n",
    "|-----|--------|----------------|\n",
    "| 1992 | REINFORCE | Gradiente de política con Monte Carlo |\n",
    "| 2014 | Policy Networks | Redes profundas para políticas |\n",
    "| 2016 | A3C/A2C | Actor-Critic asincrónico/sincrónico |\n",
    "| 2016 | PPO | Clipped objective para estabilidad |\n",
    "| 2017 | TRPO | Trust region para pasos grandes seguros |\n",
    "| 2018 | SAC | Soft Actor-Critic con entropía |\n",
    "\n",
    "En este tutorial: REINFORCE (básico) → A2C (con critic) → Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Inicial\n",
    "\n",
    "Importamos las librerías necesarias y configuramos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gymnasium as gym\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 2: Matemática del Policy Gradient\n",
    "\n",
    "### 2.1 Objetivo de Optimización\n",
    "\n",
    "Buscamos maximizar el **Expected Return**:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "donde:\n",
    "- $\\theta$ = parámetros de la política\n",
    "- $\\tau = (s_0, a_0, r_0, s_1, ...)$ = trayectoria\n",
    "- $R(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t$ = retorno descontado\n",
    "\n",
    "**Gradient ascent:** $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Policy Gradient Theorem (PGT)\n",
    "\n",
    "**Teorema central de policy gradient:**\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)]$$\n",
    "\n",
    "donde:\n",
    "- $\\rho^\\pi(s)$ = distribución de estados bajo $\\pi$\n",
    "- $Q^\\pi(s,a)$ = value function (retorno esperado)\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ = score function\n",
    "\n",
    "**Interpretación:** El gradiente es proporcional al log-probabilidad ponderado por la bondad de la acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 REINFORCE: Estimación Monte Carlo\n",
    "\n",
    "REINFORCE estima $Q^\\pi(s,a)$ con el **retorno acumulado $G_t$:**\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$$\n",
    "\n",
    "donde:\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n",
    "\n",
    "**Ventajas:**\n",
    "- No sesgado\n",
    "- Funciona sin bootstrap\n",
    "\n",
    "**Desventajas:**\n",
    "- **Alta varianza**: Esperar a fin de episodio\n",
    "- Necesita muchas muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reducción de Varianza con Baseline\n",
    "\n",
    "Restamos un baseline $b(s)$ sin introducir sesgo:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\sum_{t} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) [G_t - b(s_t)]$$\n",
    "\n",
    "**Ventaja de esta forma:**\n",
    "- **Reduce varianza** sin añadir sesgo (si $\\mathbb{E}[b(s)] = const$)\n",
    "- **Baseline óptimo:** $b^*(s) = V^\\pi(s)$\n",
    "\n",
    "Con baseline óptimo:\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\sum_{t} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) A^\\pi(s_t, a_t)$$\n",
    "\n",
    "donde $A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$ es la **ventaja** (advantage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Actor-Critic: TD-based Policy Gradient\n",
    "\n",
    "En lugar de esperar al fin del episodio (Monte Carlo), usamos **Temporal Difference**:\n",
    "\n",
    "**Crítico:** Estima $V(s)$ minimizando error TD\n",
    "$$L_{critic} = (r + \\gamma V(s') - V(s))^2$$\n",
    "\n",
    "**Actor:** Usa TD error como ventaja\n",
    "$$L_{actor} = -\\log \\pi_\\theta(a|s) \\cdot \\delta_t$$\n",
    "\n",
    "donde $\\delta_t = r + \\gamma V(s') - V(s)$ es el **TD error**.\n",
    "\n",
    "**Ventajas sobre REINFORCE:**\n",
    "- Menor varianza (TD < Monte Carlo)\n",
    "- Actualiza online (no espera fin de episodio)\n",
    "- Compatible con GAE para interpolación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 3: REINFORCE - Implementación Paso a Paso\n",
    "\n",
    "### 3.1 Red Neuronal de Política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Red Neuronal para la política π_θ(a|s)\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 hidden_dims: List[int] = [128, 128],\n",
    "                 continuous: bool = False):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Capas compartidas\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        if continuous:\n",
    "            # Para continuo: media y log_std\n",
    "            self.mean_layer = nn.Linear(prev_dim, action_dim)\n",
    "            self.log_std_layer = nn.Linear(prev_dim, action_dim)\n",
    "        else:\n",
    "            # Para discreto: logits\n",
    "            self.action_head = nn.Linear(prev_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = self.shared_layers(state)\n",
    "        \n",
    "        if self.continuous:\n",
    "            mean = self.mean_layer(x)\n",
    "            log_std = torch.clamp(self.log_std_layer(x), -20, 2)\n",
    "            return mean, log_std\n",
    "        else:\n",
    "            logits = self.action_head(x)\n",
    "            return logits, None\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False):\n",
    "        \"\"\"Selecciona acción\"\"\"\n",
    "        if self.continuous:\n",
    "            mean, log_std = self.forward(state)\n",
    "            std = log_std.exp()\n",
    "            \n",
    "            if deterministic:\n",
    "                return mean, None, None\n",
    "            \n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            entropy = dist.entropy().sum(dim=-1)\n",
    "            return action, log_prob, entropy\n",
    "        else:\n",
    "            logits, _ = self.forward(state)\n",
    "            \n",
    "            if deterministic:\n",
    "                action = logits.argmax(dim=-1)\n",
    "                return action, None, None\n",
    "            \n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            return action, log_prob, entropy\n",
    "\n",
    "print(\"PolicyNetwork definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Red de Valor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Red Neuronal para función de valor V(s)\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, hidden_dims: List[int] = [128, 128]):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass retorna V(s)\"\"\"\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "print(\"ValueNetwork definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Agente REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"Agente REINFORCE con baseline opcional\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 continuous: bool = False,\n",
    "                 learning_rate: float = 3e-4,\n",
    "                 gamma: float = 0.99,\n",
    "                 use_baseline: bool = True,\n",
    "                 baseline_lr: float = 1e-3,\n",
    "                 entropy_coef: float = 0.01,\n",
    "                 normalize_advantages: bool = True,\n",
    "                 hidden_dims: List[int] = [128, 128]):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.continuous = continuous\n",
    "        self.gamma = gamma\n",
    "        self.use_baseline = use_baseline\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "        self.device = device\n",
    "        \n",
    "        # Redes\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dims, continuous).to(device)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Baseline (value network)\n",
    "        if use_baseline:\n",
    "            self.value_network = ValueNetwork(state_dim, hidden_dims).to(device)\n",
    "            self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=baseline_lr)\n",
    "        else:\n",
    "            self.value_network = None\n",
    "        \n",
    "        # Historial\n",
    "        self.history = {\n",
    "            'episode_rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'policy_losses': [],\n",
    "            'value_losses': [],\n",
    "            'entropies': []\n",
    "        }\n",
    "    \n",
    "    def get_action(self, state: np.ndarray, deterministic: bool = False):\n",
    "        \"\"\"Selecciona acción para un estado\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, entropy = self.policy.get_action(state_tensor, deterministic)\n",
    "        \n",
    "        if self.continuous:\n",
    "            action = action.cpu().numpy().flatten()\n",
    "        else:\n",
    "            action = action.item()\n",
    "        \n",
    "        if deterministic:\n",
    "            return action\n",
    "        \n",
    "        return action, log_prob.item(), entropy.item() if entropy is not None else 0.0\n",
    "    \n",
    "    def compute_returns(self, rewards: List[float]) -> List[float]:\n",
    "        \"\"\"Calcula retornos descontados G_t\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return returns\n",
    "    \n",
    "    def train_episode(self, states: List[np.ndarray], log_probs: List[float],\n",
    "                     entropies: List[float], rewards: List[float]) -> Dict[str, float]:\n",
    "        \"\"\"Entrena con un episodio completo\"\"\"\n",
    "        # Calcular retornos\n",
    "        returns = self.compute_returns(rewards)\n",
    "        \n",
    "        # Convertir a tensors\n",
    "        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        returns_tensor = torch.FloatTensor(returns).to(self.device)\n",
    "        log_probs_tensor = torch.FloatTensor(log_probs).to(self.device)\n",
    "        entropies_tensor = torch.FloatTensor(entropies).to(self.device)\n",
    "        \n",
    "        # Calcular ventajas\n",
    "        if self.use_baseline:\n",
    "            values = self.value_network(states_tensor)\n",
    "            advantages = returns_tensor - values.detach()\n",
    "            \n",
    "            # Entrenar value network\n",
    "            value_loss = F.mse_loss(values, returns_tensor)\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), 0.5)\n",
    "            self.value_optimizer.step()\n",
    "        else:\n",
    "            advantages = returns_tensor\n",
    "            value_loss = torch.tensor(0.0)\n",
    "        \n",
    "        # Normalizar ventajas\n",
    "        if self.normalize_advantages and len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Policy loss\n",
    "        policy_loss = -(log_probs_tensor * advantages).mean()\n",
    "        entropy_loss = -entropies_tensor.mean()\n",
    "        total_loss = policy_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        # Optimizar\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item() if self.use_baseline else 0.0,\n",
    "            'entropy': entropies_tensor.mean().item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, env: gym.Env, n_episodes: int = 500,\n",
    "             max_steps: int = 500, print_every: int = 50) -> Dict[str, List]:\n",
    "        \"\"\"Entrena el agente\"\"\"\n",
    "        print(f\"Entrenando REINFORCE {'con' if self.use_baseline else 'sin'} baseline...\\n\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            \n",
    "            # Almacenar trayectoria\n",
    "            states = []\n",
    "            log_probs = []\n",
    "            entropies = []\n",
    "            rewards = []\n",
    "            \n",
    "            # Generar episodio\n",
    "            for step in range(max_steps):\n",
    "                action, log_prob, entropy = self.get_action(state, deterministic=False)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                states.append(state)\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Entrenar\n",
    "            metrics = self.train_episode(states, log_probs, entropies, rewards)\n",
    "            \n",
    "            # Registrar\n",
    "            episode_reward = sum(rewards)\n",
    "            self.history['episode_rewards'].append(episode_reward)\n",
    "            self.history['episode_lengths'].append(len(rewards))\n",
    "            self.history['policy_losses'].append(metrics['policy_loss'])\n",
    "            self.history['value_losses'].append(metrics['value_loss'])\n",
    "            self.history['entropies'].append(metrics['entropy'])\n",
    "            \n",
    "            # Logging\n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.history['episode_rewards'][-100:])\n",
    "                print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                      f\"Reward: {episode_reward:.2f} | \"\n",
    "                      f\"Avg (100): {avg_reward:.2f}\")\n",
    "        \n",
    "        print(\"\\nEntrenamiento completado!\")\n",
    "        return self.history\n",
    "\n",
    "print(\"REINFORCEAgent definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Función de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env: gym.Env, n_episodes: int = 20) -> Tuple[float, float]:\n",
    "    \"\"\"Evalúa agente sin exploración\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state, deterministic=True)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "print(\"evaluate_agent definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Entrenamiento en CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ambiente\n",
    "env_cartpole = gym.make('CartPole-v1')\n",
    "state_dim = env_cartpole.observation_space.shape[0]\n",
    "action_dim = env_cartpole.action_space.n\n",
    "\n",
    "print(f\"CartPole-v1\")\n",
    "print(f\"  State dim: {state_dim}\")\n",
    "print(f\"  Action dim: {action_dim}\")\n",
    "print(f\"\\nEntrenando REINFORCE con Baseline...\\n\")\n",
    "\n",
    "# Crear agente\n",
    "agent_reinforce = REINFORCEAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    continuous=False,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    use_baseline=True,\n",
    "    baseline_lr=1e-3,\n",
    "    entropy_coef=0.01,\n",
    "    normalize_advantages=True,\n",
    "    hidden_dims=[128, 128]\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "history_reinforce = agent_reinforce.train(\n",
    "    env=env_cartpole,\n",
    "    n_episodes=300,\n",
    "    max_steps=500,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "mean_reward, std_reward = evaluate_agent(agent_reinforce, env_cartpole, n_episodes=20)\n",
    "print(f\"\\nEvaluación REINFORCE: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "rewards = history_reinforce['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, label='Episodio')\n",
    "window = min(100, len(rewards) // 10)\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Recompensa')\n",
    "ax.set_title('REINFORCE: Recompensas por Episodio')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Policy Loss\n",
    "ax = axes[0, 1]\n",
    "losses = history_reinforce['policy_losses']\n",
    "ax.plot(losses, alpha=0.3, color='red')\n",
    "if len(losses) >= window:\n",
    "    moving_avg = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(losses)), moving_avg, label=f'MA({window})', linewidth=2, color='darkred')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('REINFORCE: Policy Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Value Loss\n",
    "ax = axes[1, 0]\n",
    "v_losses = history_reinforce['value_losses']\n",
    "ax.plot(v_losses, alpha=0.3, color='green')\n",
    "if len(v_losses) >= window:\n",
    "    moving_avg = np.convolve(v_losses, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(v_losses)), moving_avg, label=f'MA({window})', linewidth=2, color='darkgreen')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('REINFORCE: Value Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropía\n",
    "ax = axes[1, 1]\n",
    "entropies = history_reinforce['entropies']\n",
    "ax.plot(entropies, alpha=0.3, color='purple')\n",
    "if len(entropies) >= window:\n",
    "    moving_avg = np.convolve(entropies, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(entropies)), moving_avg, label=f'MA({window})', linewidth=2, color='purple')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Entropía')\n",
    "ax.set_title('REINFORCE: Policy Entropy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 4: Actor-Critic con GAE\n",
    "\n",
    "### 4.1 Introducción a Actor-Critic\n",
    "\n",
    "**Problema con REINFORCE:**\n",
    "- Alta varianza: espera al fin del episodio\n",
    "- Lenta convergencia\n",
    "\n",
    "**Solución: Actor-Critic**\n",
    "- **Actor:** Política π(a|s) - optimiza con policy gradient\n",
    "- **Critic:** Value function V(s) - estima valores con TD\n",
    "\n",
    "Combinan Policy Gradient (actor) con Value-based (critic) para reducir varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Advantage Actor-Critic con GAE\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 continuous: bool = False,\n",
    "                 actor_lr: float = 3e-4,\n",
    "                 critic_lr: float = 1e-3,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 0.95,\n",
    "                 entropy_coef: float = 0.01,\n",
    "                 value_loss_coef: float = 0.5,\n",
    "                 normalize_advantages: bool = True,\n",
    "                 use_gae: bool = True,\n",
    "                 hidden_dims: List[int] = [256, 256]):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.continuous = continuous\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "        self.use_gae = use_gae\n",
    "        self.device = device\n",
    "        \n",
    "        # Actor y Critic\n",
    "        self.actor = PolicyNetwork(state_dim, action_dim, hidden_dims, continuous).to(device)\n",
    "        self.critic = ValueNetwork(state_dim, hidden_dims).to(device)\n",
    "        \n",
    "        # Optimizadores\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Historial\n",
    "        self.history = {\n",
    "            'episode_rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'actor_losses': [],\n",
    "            'critic_losses': [],\n",
    "            'entropies': [],\n",
    "            'advantages': []\n",
    "        }\n",
    "    \n",
    "    def get_action(self, state: np.ndarray, deterministic: bool = False):\n",
    "        \"\"\"Selecciona acción\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                if self.continuous:\n",
    "                    mean, _ = self.actor.forward(state_tensor)\n",
    "                    action = mean\n",
    "                else:\n",
    "                    logits, _ = self.actor.forward(state_tensor)\n",
    "                    action = logits.argmax(dim=-1)\n",
    "            else:\n",
    "                action, _, _ = self.actor.get_action(state_tensor)\n",
    "        \n",
    "        if self.continuous:\n",
    "            return action.cpu().numpy().flatten()\n",
    "        else:\n",
    "            return action.item()\n",
    "    \n",
    "    def compute_gae(self, rewards: List[float], values: torch.Tensor,\n",
    "                   next_values: torch.Tensor, dones: List[bool]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Calcula Generalized Advantage Estimation\n",
    "        \n",
    "        GAE(λ) = Σ (γλ)^l δ_{t+l}\n",
    "        donde δ_t = r_t + γV(s_{t+1}) - V(s_t) es TD error\n",
    "        \"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = next_values[t]\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            # TD error\n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            # GAE\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)\n",
    "        returns = advantages + values\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def train_step(self, states: List[np.ndarray], actions: List,\n",
    "                  rewards: List[float], next_states: List[np.ndarray],\n",
    "                  dones: List[bool]) -> Dict[str, float]:\n",
    "        \"\"\"Paso de entrenamiento\"\"\"\n",
    "        # Convertir a tensors\n",
    "        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        next_states_tensor = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        \n",
    "        if self.continuous:\n",
    "            actions_tensor = torch.FloatTensor(np.array(actions)).to(self.device)\n",
    "        else:\n",
    "            actions_tensor = torch.LongTensor(actions).to(self.device)\n",
    "        \n",
    "        # Calcular valores\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states_tensor)\n",
    "            next_values = self.critic(next_states_tensor)\n",
    "        \n",
    "        # Calcular ventajas con GAE\n",
    "        if self.use_gae:\n",
    "            advantages, returns = self.compute_gae(rewards, values, next_values, dones)\n",
    "        else:\n",
    "            # Simple n-step\n",
    "            returns_list = []\n",
    "            R = next_values[-1] if len(rewards) > 0 else 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                R = rewards[t] + self.gamma * R * (1 - dones[t])\n",
    "                returns_list.insert(0, R)\n",
    "            returns = torch.tensor(returns_list, dtype=torch.float32).to(self.device)\n",
    "            advantages = returns - values\n",
    "        \n",
    "        # Normalizar ventajas\n",
    "        if self.normalize_advantages and len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Actor loss\n",
    "        _, log_probs, entropies = self.actor.get_action(states_tensor, actions_tensor if not self.continuous else None)\n",
    "        if self.continuous:\n",
    "            # Para continuo, recalcular log_probs\n",
    "            mean, log_std = self.actor.forward(states_tensor)\n",
    "            std = log_std.exp()\n",
    "            dist = Normal(mean, std)\n",
    "            log_probs = dist.log_prob(actions_tensor).sum(dim=-1)\n",
    "            entropies = dist.entropy().sum(dim=-1)\n",
    "        \n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        entropy_loss = -entropies.mean()\n",
    "        total_actor_loss = actor_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        # Critic loss\n",
    "        values_new = self.critic(states_tensor)\n",
    "        critic_loss = F.mse_loss(values_new, returns.detach())\n",
    "        \n",
    "        # Optimizar\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        total_actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'entropy': entropies.mean().item(),\n",
    "            'mean_advantage': advantages.mean().item()\n",
    "        }\n",
    "    \n",
    "    def train(self, env: gym.Env, n_episodes: int = 500,\n",
    "             max_steps: int = 500, print_every: int = 50) -> Dict[str, List]:\n",
    "        \"\"\"Entrena el agente\"\"\"\n",
    "        print(f\"Entrenando A2C con GAE...\\n\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            \n",
    "            # Buffers\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            next_states = []\n",
    "            dones = []\n",
    "            \n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Generar episodio\n",
    "            for step in range(max_steps):\n",
    "                action = self.get_action(state, deterministic=False)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(float(done))\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Entrenar\n",
    "            metrics = self.train_step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            # Registrar\n",
    "            self.history['episode_rewards'].append(episode_reward)\n",
    "            self.history['episode_lengths'].append(step + 1)\n",
    "            self.history['actor_losses'].append(metrics['actor_loss'])\n",
    "            self.history['critic_losses'].append(metrics['critic_loss'])\n",
    "            self.history['entropies'].append(metrics['entropy'])\n",
    "            self.history['advantages'].append(metrics['mean_advantage'])\n",
    "            \n",
    "            # Logging\n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.history['episode_rewards'][-100:])\n",
    "                print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                      f\"Reward: {episode_reward:.2f} | \"\n",
    "                      f\"Avg (100): {avg_reward:.2f}\")\n",
    "        \n",
    "        print(\"\\nEntrenamiento completado!\")\n",
    "        return self.history\n",
    "\n",
    "print(\"A2CAgent definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Entrenamiento de A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear agente A2C\n",
    "agent_a2c = A2CAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    continuous=False,\n",
    "    actor_lr=3e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    entropy_coef=0.01,\n",
    "    value_loss_coef=0.5,\n",
    "    normalize_advantages=True,\n",
    "    use_gae=True,\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "print(\"Entrenando A2C en CartPole...\\n\")\n",
    "history_a2c = agent_a2c.train(\n",
    "    env=env_cartpole,\n",
    "    n_episodes=300,\n",
    "    max_steps=500,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_agent(agent_a2c, env_cartpole, n_episodes=20)\n",
    "print(f\"\\nEvaluación A2C: {mean_reward_a2c:.2f} ± {std_reward_a2c:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Análisis GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar diferencia entre GAE y n-step\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "window = 20\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history_a2c['episode_rewards'], alpha=0.3, color='blue')\n",
    "ma = np.convolve(history_a2c['episode_rewards'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(history_a2c['episode_rewards'])), ma, linewidth=2, color='darkblue')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Recompensa')\n",
    "ax.set_title('A2C: Recompensas')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Actor Loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history_a2c['actor_losses'], alpha=0.3, color='green')\n",
    "ma = np.convolve(history_a2c['actor_losses'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(history_a2c['actor_losses'])), ma, linewidth=2, color='darkgreen')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('A2C: Actor Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Critic Loss\n",
    "ax = axes[0, 2]\n",
    "ax.plot(history_a2c['critic_losses'], alpha=0.3, color='red')\n",
    "ma = np.convolve(history_a2c['critic_losses'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(history_a2c['critic_losses'])), ma, linewidth=2, color='darkred')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('A2C: Critic Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropía\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history_a2c['entropies'], alpha=0.3, color='purple')\n",
    "ma = np.convolve(history_a2c['entropies'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(history_a2c['entropies'])), ma, linewidth=2, color='purple')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Entropía')\n",
    "ax.set_title('A2C: Policy Entropy')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Ventajas\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history_a2c['advantages'], alpha=0.3, color='orange')\n",
    "ma = np.convolve(history_a2c['advantages'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(history_a2c['advantages'])), ma, linewidth=2, color='orange')\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Mean Advantage')\n",
    "ax.set_title('A2C: Ventajas')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Longitud de episodios\n",
    "ax = axes[1, 2]\n",
    "ax.plot(history_a2c['episode_lengths'], alpha=0.3, color='brown')\n",
    "ma = np.convolve(history_a2c['episode_lengths'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(history_a2c['episode_lengths'])), ma, linewidth=2, color='brown')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Longitud')\n",
    "ax.set_title('A2C: Episode Length')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 5: Comparación REINFORCE vs A2C\n",
    "\n",
    "### 5.1 Comparación Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "window = 20\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "reinforce_ma = np.convolve(history_reinforce['episode_rewards'], np.ones(window)/window, mode='valid')\n",
    "a2c_ma = np.convolve(history_a2c['episode_rewards'], np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(range(window-1, len(history_reinforce['episode_rewards'])), reinforce_ma,\n",
    "       label='REINFORCE', linewidth=2.5, color='blue')\n",
    "ax.plot(range(window-1, len(history_a2c['episode_rewards'])), a2c_ma,\n",
    "       label='A2C', linewidth=2.5, color='red')\n",
    "ax.set_xlabel('Episodio', fontsize=11)\n",
    "ax.set_ylabel('Recompensa (MA-20)', fontsize=11)\n",
    "ax.set_title('Comparación: Recompensas por Episodio', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Varianza de recompensas (desv. móvil)\n",
    "ax = axes[0, 1]\n",
    "reinforce_var = np.convolve(np.abs(np.diff(history_reinforce['episode_rewards'])),\n",
    "                            np.ones(window)/window, mode='valid')\n",
    "a2c_var = np.convolve(np.abs(np.diff(history_a2c['episode_rewards'])),\n",
    "                      np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(reinforce_var)+window-1), reinforce_var,\n",
    "       label='REINFORCE', linewidth=2.5, color='blue')\n",
    "ax.plot(range(window-1, len(a2c_var)+window-1), a2c_var,\n",
    "       label='A2C', linewidth=2.5, color='red')\n",
    "ax.set_xlabel('Episodio', fontsize=11)\n",
    "ax.set_ylabel('Variabilidad', fontsize=11)\n",
    "ax.set_title('Comparación: Variabilidad de Recompensas', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Policy Loss\n",
    "ax = axes[1, 0]\n",
    "reinforce_ploss = np.convolve(history_reinforce['policy_losses'], np.ones(window)/window, mode='valid')\n",
    "a2c_aloss = np.convolve(history_a2c['actor_losses'], np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(range(window-1, len(reinforce_ploss)+window-1), reinforce_ploss,\n",
    "       label='REINFORCE', linewidth=2.5, color='blue')\n",
    "ax.plot(range(window-1, len(a2c_aloss)+window-1), a2c_aloss,\n",
    "       label='A2C', linewidth=2.5, color='red')\n",
    "ax.set_xlabel('Episodio', fontsize=11)\n",
    "ax.set_ylabel('Loss (MA-20)', fontsize=11)\n",
    "ax.set_title('Comparación: Policy/Actor Loss', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropía\n",
    "ax = axes[1, 1]\n",
    "reinforce_ent = np.convolve(history_reinforce['entropies'], np.ones(window)/window, mode='valid')\n",
    "a2c_ent = np.convolve(history_a2c['entropies'], np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(range(window-1, len(reinforce_ent)+window-1), reinforce_ent,\n",
    "       label='REINFORCE', linewidth=2.5, color='blue')\n",
    "ax.plot(range(window-1, len(a2c_ent)+window-1), a2c_ent,\n",
    "       label='A2C', linewidth=2.5, color='red')\n",
    "ax.set_xlabel('Episodio', fontsize=11)\n",
    "ax.set_ylabel('Entropía (MA-20)', fontsize=11)\n",
    "ax.set_title('Comparación: Policy Entropy', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tabla de Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Estadísticas\n",
    "reinforce_last100 = np.mean(history_reinforce['episode_rewards'][-100:])\n",
    "a2c_last100 = np.mean(history_a2c['episode_rewards'][-100:])\n",
    "\n",
    "reinforce_final_train_var = np.var(history_reinforce['episode_rewards'][-100:])\n",
    "a2c_final_train_var = np.var(history_a2c['episode_rewards'][-100:])\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Métrica': [\n",
    "        'Reward Entrenamiento (últimos 100)',\n",
    "        'Variance Entrenamiento',\n",
    "        'Reward Evaluación',\n",
    "        'Std Evaluación',\n",
    "        'Policy Loss Final',\n",
    "        'Entropía Final'\n",
    "    ],\n",
    "    'REINFORCE': [\n",
    "        f\"{reinforce_last100:.2f}\",\n",
    "        f\"{reinforce_final_train_var:.2f}\",\n",
    "        f\"{mean_reward:.2f}\",\n",
    "        f\"{std_reward:.2f}\",\n",
    "        f\"{np.mean(history_reinforce['policy_losses'][-10:]):.4f}\",\n",
    "        f\"{np.mean(history_reinforce['entropies'][-10:]):.4f}\"\n",
    "    ],\n",
    "    'A2C': [\n",
    "        f\"{a2c_last100:.2f}\",\n",
    "        f\"{a2c_final_train_var:.2f}\",\n",
    "        f\"{mean_reward_a2c:.2f}\",\n",
    "        f\"{std_reward_a2c:.2f}\",\n",
    "        f\"{np.mean(history_a2c['actor_losses'][-10:]):.4f}\",\n",
    "        f\"{np.mean(history_a2c['entropies'][-10:]):.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACIÓN CUANTITATIVA: REINFORCE vs A2C\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Análisis Cualitativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANÁLISIS COMPARATIVO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analysis = \"\"\"\n",
    "1. REINFORCE (Monte Carlo Policy Gradient)\n",
    "   - Actualización: Cada fin de episodio\n",
    "   - Estimador: G_t (retorno acumulado)\n",
    "   - Varianza: ALTA (espera a fin de episodio)\n",
    "   - Sesgo: Bajo (no sesgado)\n",
    "   - Convergencia: Lenta pero garantizada\n",
    "   - Baseline: Reduce varianza sin sesgo\n",
    "\n",
    "2. A2C (Advantage Actor-Critic con GAE)\n",
    "   - Actualización: Cada episodio (pero con múltiples steps)\n",
    "   - Estimador: TD error + GAE\n",
    "   - Varianza: BAJA (bootstrapping reduce varianza)\n",
    "   - Sesgo: Bajo (GAE interpola entre TD y MC)\n",
    "   - Convergencia: Rápida y más estable\n",
    "   - Critic: Reduce varianza efectivamente\n",
    "\n",
    "3. Trade-offs\n",
    "   ┌────────────────────────┬──────────────┬──────────────┐\n",
    "   │ Aspecto                │ REINFORCE    │ A2C          │\n",
    "   ├────────────────────────┼──────────────┼──────────────┤\n",
    "   │ Complejidad            │ Simple       │ Media        │\n",
    "   │ Varianza               │ Alta         │ Baja         │\n",
    "   │ Velocidad              │ Lenta        │ Rápida       │\n",
    "   │ Estabilidad            │ Buena        │ Muy Buena    │\n",
    "   │ Convergencia           │ Lenta        │ Rápida       │\n",
    "   │ Acciones Continuas     │ Excelente    │ Excelente    │\n",
    "   │ Acciones Discretas     │ Buena        │ Buena        │\n",
    "   │ Paralelización         │ Fácil        │ Más difícil  │\n",
    "   └────────────────────────┴──────────────┴──────────────┘\n",
    "\n",
    "4. Recomendaciones de Uso\n",
    "   REINFORCE si:\n",
    "     - Es tu primer algoritmo (educativo)\n",
    "     - Necesitas máxima estabilidad\n",
    "     - Trabajo paralelo a escala (A3C)\n",
    "\n",
    "   A2C si:\n",
    "     - Necesitas convergencia rápida\n",
    "     - Varianza es problema\n",
    "     - Presupuesto computacional limitado\n",
    "     - Acciones discretas o continuas\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 6: Experimentos Prácticos\n",
    "\n",
    "### 6.1 Impacto del Baseline en REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar REINFORCE SIN baseline para comparación\n",
    "print(\"Entrenando REINFORCE SIN baseline...\\n\")\n",
    "\n",
    "agent_reinforce_no_baseline = REINFORCEAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    continuous=False,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    use_baseline=False,\n",
    "    entropy_coef=0.01,\n",
    "    normalize_advantages=True,\n",
    "    hidden_dims=[128, 128]\n",
    ")\n",
    "\n",
    "history_no_baseline = agent_reinforce_no_baseline.train(\n",
    "    env=env_cartpole,\n",
    "    n_episodes=300,\n",
    "    max_steps=500,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "mean_no_bl, std_no_bl = evaluate_agent(agent_reinforce_no_baseline, env_cartpole, n_episodes=20)\n",
    "print(f\"\\nEvaluación REINFORCE sin baseline: {mean_no_bl:.2f} ± {std_no_bl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Impacto de GAE Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes valores de GAE lambda\n",
    "lambdas = [0.0, 0.5, 0.95, 1.0]\n",
    "histories_lambda = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    print(f\"\\nEntrenando A2C con GAE lambda={lam}...\")\n",
    "    agent = A2CAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        continuous=False,\n",
    "        gae_lambda=lam,\n",
    "        use_gae=(lam < 1.0),  # Para lambda=1.0 usar Monte Carlo\n",
    "        hidden_dims=[256, 256]\n",
    "    )\n",
    "    \n",
    "    hist = agent.train(\n",
    "        env=env_cartpole,\n",
    "        n_episodes=250,\n",
    "        max_steps=500,\n",
    "        print_every=100\n",
    "    )\n",
    "    histories_lambda.append((lam, hist))\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "window = 15\n",
    "colors = ['red', 'orange', 'blue', 'green']\n",
    "\n",
    "for (lam, hist), color in zip(histories_lambda, colors):\n",
    "    rewards = hist['episode_rewards']\n",
    "    ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), ma, label=f'λ={lam}', linewidth=2.5, color=color)\n",
    "\n",
    "ax.set_xlabel('Episodio', fontsize=12)\n",
    "ax.set_ylabel('Recompensa (MA-15)', fontsize=12)\n",
    "ax.set_title('Impacto de GAE Lambda en A2C', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnálisis de GAE Lambda:\")\n",
    "print(\"  λ=0.0: TD(0) - Baja varianza, alto sesgo\")\n",
    "print(\"  λ=0.95: Equilibrio (recomendado)\")\n",
    "print(\"  λ=1.0: Monte Carlo - Alta varianza, sin sesgo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Rendimiento en Pendulum (Continuo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ambiente Pendulum\n",
    "env_pendulum = gym.make('Pendulum-v1')\n",
    "state_dim_p = env_pendulum.observation_space.shape[0]  # 3\n",
    "action_dim_p = env_pendulum.action_space.shape[0]      # 1 (continuo)\n",
    "\n",
    "print(f\"Pendulum-v1:\")\n",
    "print(f\"  State dim: {state_dim_p} (continuo)\")\n",
    "print(f\"  Action dim: {action_dim_p} (continuo)\")\n",
    "print(f\"  Reward: -16 a 0 (mejor es 0)\")\n",
    "\n",
    "# Entrenar A2C en Pendulum\n",
    "print(f\"\\nEntrenando A2C en Pendulum...\\n\")\n",
    "\n",
    "agent_pendulum = A2CAgent(\n",
    "    state_dim=state_dim_p,\n",
    "    action_dim=action_dim_p,\n",
    "    continuous=True,  # IMPORTANTE: Acciones continuas\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    entropy_coef=0.001,  # Menos entropía para continuo\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "history_pendulum = agent_pendulum.train(\n",
    "    env=env_pendulum,\n",
    "    n_episodes=200,\n",
    "    max_steps=200,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "mean_pend, std_pend = evaluate_agent(agent_pendulum, env_pendulum, n_episodes=10)\n",
    "print(f\"\\nEvaluación A2C en Pendulum: {mean_pend:.2f} ± {std_pend:.2f}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 10\n",
    "rewards = history_pendulum['episode_rewards']\n",
    "ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1.plot(rewards, alpha=0.3, color='blue')\n",
    "ax1.plot(range(window-1, len(rewards)), ma, linewidth=2.5, color='darkblue')\n",
    "ax1.set_xlabel('Episodio')\n",
    "ax1.set_ylabel('Recompensa')\n",
    "ax1.set_title('A2C en Pendulum-v1: Recompensas')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Critic Loss\n",
    "losses = history_pendulum['critic_losses']\n",
    "ma_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(losses, alpha=0.3, color='red')\n",
    "ax2.plot(range(window-1, len(losses)), ma_loss, linewidth=2.5, color='darkred')\n",
    "ax2.set_xlabel('Episodio')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('A2C en Pendulum-v1: Critic Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env_pendulum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 7: Ejercicios\n",
    "\n",
    "### 7.1 Ejercicio 1: Implementar Entropy Regularization Diferenciada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifica A2CAgent para que:\n",
    "1. Tenga un `entropy_schedule` que decaiga con los episodios\n",
    "2. Comience con alta exploración y termine con baja\n",
    "3. Compara resultados\n",
    "\n",
    "Pista: Usa una función lambda o decaimiento exponencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar entropy decay\n",
    "def entropy_decay_schedule(episode, initial_entropy=0.01, final_entropy=0.001, total_episodes=500):\n",
    "    \"\"\"Entropy schedule decayente\"\"\"\n",
    "    # Implementa aquí\n",
    "    pass\n",
    "\n",
    "print(\"Ejercicio 1: Entropy Decay Schedule\")\n",
    "print(\"TODO: Implementar y entrenar agente con entropy decayente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ejercicio 2: Comparar Diferentes Arquitecturas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba diferentes tamaños de red:\n",
    "1. Small: [64, 64]\n",
    "2. Medium: [256, 256] (actual)\n",
    "3. Large: [512, 512]\n",
    "\n",
    "¿Cuál converge más rápido? ¿Cuál logra mejor reward final?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Arquitectura grid search\n",
    "architectures = {\n",
    "    'Small': [64, 64],\n",
    "    'Medium': [256, 256],\n",
    "    'Large': [512, 512]\n",
    "}\n",
    "\n",
    "print(\"Ejercicio 2: Network Architecture Comparison\")\n",
    "print(\"TODO: Entrenar A2C con diferentes arquitecturas\")\n",
    "print(f\"Arquitecturas: {list(architectures.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Ejercicio 3: Guardar y Cargar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, path: str):\n",
    "    \"\"\"Guarda políticas y valores\"\"\"\n",
    "    checkpoint = {\n",
    "        'actor': agent.actor.state_dict(),\n",
    "        'critic': agent.critic.state_dict(),\n",
    "        'actor_opt': agent.actor_optimizer.state_dict(),\n",
    "        'critic_opt': agent.critic_optimizer.state_dict(),\n",
    "        'history': agent.history\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_agent(agent, path: str):\n",
    "    \"\"\"Carga políticas y valores\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    agent.actor.load_state_dict(checkpoint['actor'])\n",
    "    agent.critic.load_state_dict(checkpoint['critic'])\n",
    "    agent.actor_optimizer.load_state_dict(checkpoint['actor_opt'])\n",
    "    agent.critic_optimizer.load_state_dict(checkpoint['critic_opt'])\n",
    "    agent.history = checkpoint['history']\n",
    "    print(f\"Modelo cargado desde {path}\")\n",
    "\n",
    "# Test\n",
    "save_path = '/tmp/a2c_cartpole.pth'\n",
    "save_agent(agent_a2c, save_path)\n",
    "print(f\"\\nModelo A2C guardado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Ejercicio 4: Análisis de Trayectorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recolectar trayectorias de agentes entrenados\n",
    "def analyze_trajectories(agent, env, n_episodes=5):\n",
    "    \"\"\"Analiza trayectorias aprendidas\"\"\"\n",
    "    trajectories = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        traj = {'states': [], 'actions': [], 'rewards': []}\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state, deterministic=True)\n",
    "            traj['states'].append(state)\n",
    "            traj['actions'].append(action)\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            traj['rewards'].append(reward)\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        trajectories.append(traj)\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "# Análisis\n",
    "trajs_a2c = analyze_trajectories(agent_a2c, env_cartpole, n_episodes=3)\n",
    "\n",
    "print(\"Análisis de Trayectorias A2C en CartPole:\")\n",
    "for i, traj in enumerate(trajs_a2c):\n",
    "    total_reward = sum(traj['rewards'])\n",
    "    length = len(traj['states'])\n",
    "    print(f\"  Episodio {i+1}: Longitud={length}, Reward={total_reward:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Ejercicio 5: Robustez a Cambios de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness: ¿Cómo le va a la política cuando cambian parámetros del ambiente?\n",
    "print(\"Prueba de Robustez: Evaluar en condiciones variadas\\n\")\n",
    "\n",
    "# CartPole estándar\n",
    "env_test = gym.make('CartPole-v1')\n",
    "reward_std, std_std = evaluate_agent(agent_a2c, env_test, n_episodes=10)\n",
    "print(f\"CartPole estándar: {reward_std:.2f} ± {std_std:.2f}\")\n",
    "\n",
    "env_test.close()\n",
    "\n",
    "print(\"\\nObservación: Las políticas neuronales generalizan bien a variaciones del ambiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 8: Conclusiones\n",
    "\n",
    "### 8.1 Resumen de Conceptos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Gradient es poderoso porque:**\n",
    "\n",
    "1. **Optimiza directamente la política**\n",
    "   - No necesita extraer política con argmax\n",
    "   - Maneja acciones continuas naturalmente\n",
    "\n",
    "2. **Convergencia garantizada**\n",
    "   - Converge a mínimo local (no puede divergir como DQN)\n",
    "   - Más estable que métodos value-based\n",
    "\n",
    "3. **REINFORCE es simple pero efectivo**\n",
    "   - No sesgado\n",
    "   - Baseline reduce varianza sin sesgo\n",
    "   - Fundamental para entender policy gradient\n",
    "\n",
    "4. **A2C mejora significativamente**\n",
    "   - Critic reduce varianza (crítico!)\n",
    "   - GAE interpola entre TD y Monte Carlo\n",
    "   - Convergencia más rápida que REINFORCE\n",
    "\n",
    "5. **Comparación REINFORCE vs A2C**\n",
    "   - REINFORCE: Educativo, simple, convergencia lenta\n",
    "   - A2C: Producción, rápido, muy estable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Extensiones Avanzadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmos relacionados que puedes explorar:**\n",
    "\n",
    "1. **A3C (Asynchronous A2C)**\n",
    "   - Paralelo: múltiples workers independientes\n",
    "   - No necesita replay buffer\n",
    "   - Muy eficiente en multi-core\n",
    "\n",
    "2. **PPO (Proximal Policy Optimization)**\n",
    "   - Clipped objective para pasos más seguros\n",
    "   - Estado del arte para muchas tareas\n",
    "   - Muy robusto a hiperparámetros\n",
    "\n",
    "3. **TRPO (Trust Region Policy Optimization)**\n",
    "   - Restricción KL para region de confianza\n",
    "   - Garantías teóricas más fuertes\n",
    "   - Más complicado de implementar\n",
    "\n",
    "4. **SAC (Soft Actor-Critic)**\n",
    "   - Maximiza entropía + reward\n",
    "   - Excelente para exploration\n",
    "   - Off-policy compatible\n",
    "\n",
    "5. **DDPG (Deep Deterministic Policy Gradient)**\n",
    "   - Deterministic policy gradient\n",
    "   - Off-policy (puede usar replay buffer)\n",
    "   - Para acciones continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Guía de Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN FINAL: POLICY GRADIENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = \"\"\"\n",
    "RESULTADOS EN CARTPOLE-V1 (300 episodios de entrenamiento):\n",
    "\n",
    "REINFORCE (con baseline):\n",
    "  - Recompensa Training (últimos 100 ep): {:.2f}\n",
    "  - Recompensa Evaluación: {:.2f} ± {:.2f}\n",
    "  - Velocidad: Lenta\n",
    "  - Estabilidad: Buena\n",
    "\n",
    "A2C (con GAE λ=0.95):\n",
    "  - Recompensa Training (últimos 100 ep): {:.2f}\n",
    "  - Recompensa Evaluación: {:.2f} ± {:.2f}\n",
    "  - Velocidad: Rápida\n",
    "  - Estabilidad: Muy Buena\n",
    "\n",
    "CONCLUSIONES CLAVE:\n",
    "\n",
    "1. A2C es más rápido y estable que REINFORCE\n",
    "2. El critic (value network) es muy importante para reducir varianza\n",
    "3. GAE proporciona interpolación flexible entre TD y MC\n",
    "4. Policy Gradient converge a soluciones locales (bueno para estabilidad)\n",
    "5. Entropía regularización fomenta exploración\n",
    "\n",
    "RECOMENDACIONES DE USO:\n",
    "\n",
    "✓ Usa REINFORCE si:\n",
    "  - Estás aprendiendo policy gradient\n",
    "  - Necesitas máxima simplicidad\n",
    "  - Paralelización con A3C\n",
    "\n",
    "✓ Usa A2C si:\n",
    "  - Necesitas producción\n",
    "  - Varianza es problema\n",
    "  - Cualquier tipo de acción (continua/discreta)\n",
    "  \n",
    "✓ Usa PPO si:\n",
    "  - Necesitas estado del arte simple\n",
    "  - Robustez a hiperparámetros\n",
    "  - Mejor rendimiento general\n",
    "\"\"\"\n",
    "\n",
    "print(summary.format(\n",
    "    np.mean(history_reinforce['episode_rewards'][-100:]),\n",
    "    mean_reward, std_reward,\n",
    "    np.mean(history_a2c['episode_rewards'][-100:]),\n",
    "    mean_reward_a2c, std_reward_a2c\n",
    "))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n¡Felicidades! Has completado el tutorial de Policy Gradient.\")\n",
    "print(\"\\nPróximos pasos:\")\n",
    "print(\"  1. Implementa PPO (similar a A2C pero más estable)\")\n",
    "print(\"  2. Prueba en ambientes más complejos (Atari, robótica)\")\n",
    "print(\"  3. Estudia A3C para paralelización\")\n",
    "print(\"  4. Explora combinaciones: DQN + Policy Gradient\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
