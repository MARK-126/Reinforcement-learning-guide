{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial Completo: Deep Q-Network (DQN) y Variantes\n",
        "\n",
        "Exploraremos los fundamentos de aprendizaje por refuerzo profundo (Deep RL) con DQN y sus mejoras.\n",
        "\n",
        "## Contenido:\n",
        "1. Introducción a Deep RL (3 celdas)\n",
        "2. Fundamentos de PyTorch (4 celdas)\n",
        "3. Arquitectura DQN con CartPole (12 celdas)\n",
        "4. Double DQN (10 celdas)\n",
        "5. Dueling DQN (10 celdas)\n",
        "6. Comparación de los 3 Algoritmos (8 celdas)\n",
        "7. Ejercicios Prácticos (5 celdas)\n",
        "8. Conclusiones (3 celdas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 1: Introducción a Deep RL\n",
        "\n",
        "### ¿Qué es Deep Reinforcement Learning?\n",
        "\n",
        "Deep RL combina:\n",
        "- **Aprendizaje por Refuerzo (RL)**: El agente aprende mediante interacción con el ambiente\n",
        "- **Redes Neuronales Profundas**: Para aproximar funciones complejas (políticas, Q-values, etc.)\n",
        "\n",
        "### Problema de Q-Learning Clásico\n",
        "- Con espacios de estados grandes/continuos es impracticable usar una tabla Q\n",
        "- Solución: Aproximar Q(s,a) con una red neuronal\n",
        "\n",
        "### Deep Q-Network (DQN)\n",
        "Paper seminal: \"Human-level control through deep reinforcement learning\" (Mnih et al., 2015)\n",
        "\n",
        "**Componentes clave:**\n",
        "1. **Experience Replay**: Almacena transiciones y entrena con mini-batches aleatorios\n",
        "2. **Target Network**: Red separada para calcular targets, actualizada periódicamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ecuación de Bellman para DQN\n",
        "\n",
        "$$Q(s,a) \\approx r + \\gamma \\max_{a'} Q(s', a')$$\n",
        "\n",
        "**Ventajas de redes neuronales profundas:**\n",
        "- Generalizan bien a nuevos estados\n",
        "- Pueden aprender representaciones complejas\n",
        "- Escalables a problemas de alta dimensión (imágenes, etc.)\n",
        "\n",
        "**Desafíos:**\n",
        "- Correlación temporal entre datos\n",
        "- Bootstrapping (el target depende de la misma red)\n",
        "- Sobreestimación de Q-values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Timeline de Mejoras\n",
        "\n",
        "| Año | Algoritmo | Mejora Principal |\n",
        "|-----|-----------|------------------|\n",
        "| 2015 | DQN | Experience Replay + Target Network |\n",
        "| 2015 | Double DQN | Reducir sobreestimación de Q-values |\n",
        "| 2016 | Dueling DQN | Separar Value y Advantage streams |\n",
        "| 2017 | Rainbow | Combinar todos los anteriores + PER + Noisy Networks |\n",
        "\n",
        "**Nuestro enfoque:** Estudiar DQN, Double DQN y Dueling DQN en profundidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 2: Fundamentos de PyTorch para Deep RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerías necesarias\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Verificar GPU disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Dispositivo disponible: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conceptos Básicos de PyTorch para RL\n",
        "\n",
        "1. **Tensores**: Equivalente a ndarrays de NumPy pero en GPU\n",
        "2. **Autograd**: Cálculo automático de gradientes\n",
        "3. **nn.Module**: Clase base para definir redes neuronales\n",
        "4. **Optimizadores**: Adam, SGD, etc. para actualizar parámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo: Red neuronal simple para Q-Network\n",
        "class SimpleQNetwork(nn.Module):\n",
        "    \"\"\"Red simple para aproximar Q-values\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(SimpleQNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"state -> Q-values para cada acción\"\"\"\n",
        "        return self.net(state)\n",
        "\n",
        "# Ejemplo de uso\n",
        "state_dim = 4  # CartPole state dimension\n",
        "action_dim = 2  # CartPole action dimension\n",
        "\n",
        "q_net = SimpleQNetwork(state_dim, action_dim).to(device)\n",
        "print(\"Arquitectura de la red:\")\n",
        "print(q_net)\n",
        "\n",
        "# Forward pass\n",
        "dummy_state = torch.randn(1, state_dim).to(device)\n",
        "q_values = q_net(dummy_state)\n",
        "print(f\"\\nEstado (batch=1): {dummy_state.shape}\")\n",
        "print(f\"Q-values output: {q_values.shape}\")\n",
        "print(f\"Q-values: {q_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo: Entrenamiento básico\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Simular un paso de entrenamiento\n",
        "batch_states = torch.randn(32, state_dim).to(device)  # 32 estados\n",
        "batch_actions = torch.randint(0, action_dim, (32,)).to(device)  # acciones\n",
        "batch_targets = torch.randn(32).to(device)  # targets Q-values\n",
        "\n",
        "# Forward pass\n",
        "q_values = q_net(batch_states)\n",
        "q_values_for_actions = q_values.gather(1, batch_actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "# Calcular pérdida\n",
        "loss = loss_fn(q_values_for_actions, batch_targets)\n",
        "\n",
        "# Backward pass\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "torch.nn.utils.clip_grad_norm_(q_net.parameters(), 1.0)  # Clipping de gradientes\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "print(\"\\nComponentes clave del entrenamiento:\")\n",
        "print(\"1. Forward pass: calcular Q(s,a)\")\n",
        "print(\"2. Loss: MSE entre Q actual y target\")\n",
        "print(\"3. Backward: calcular gradientes\")\n",
        "print(\"4. Optimization step: actualizar parámetros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 3: Arquitectura DQN con CartPole\n",
        "\n",
        "Implementaremos DQN paso a paso, entrenándolo en el ambiente CartPole-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Replay Buffer\n",
        "\n",
        "Almacena transiciones (s, a, r, s', done) para romper correlación temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir Transition\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Experience Replay Buffer\"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Prueba\n",
        "replay_buffer = ReplayBuffer(capacity=1000)\n",
        "for i in range(100):\n",
        "    state = np.random.randn(4)\n",
        "    action = 0\n",
        "    reward = 1.0\n",
        "    next_state = np.random.randn(4)\n",
        "    done = False\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "print(f\"Buffer size: {len(replay_buffer)}\")\n",
        "batch = replay_buffer.sample(10)\n",
        "print(f\"Sample batch size: {len(batch)}\")\n",
        "print(f\"First transition: state shape {batch[0].state.shape}, action: {batch[0].action}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Red DQN Básica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"Red Neuronal para Q-Network\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(DQN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Crear instancia\n",
        "q_network = DQN(state_dim=4, action_dim=2).to(device)\n",
        "target_network = DQN(state_dim=4, action_dim=2).to(device)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "target_network.eval()  # Target network en modo evaluación\n",
        "\n",
        "print(\"Q-Network:\")\n",
        "print(q_network)\n",
        "print(\"\\nTarget Network (copia de Q-Network):\")\n",
        "print(\"Los pesos se sincronizan cada N episodios\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Agente DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"Agente DQN completo\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, \n",
        "                 learning_rate=1e-3,\n",
        "                 gamma=0.99,\n",
        "                 epsilon_start=1.0,\n",
        "                 epsilon_end=0.01,\n",
        "                 epsilon_decay=500,\n",
        "                 buffer_size=10000,\n",
        "                 batch_size=64,\n",
        "                 target_update=10):\n",
        "        \n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.device = device\n",
        "        \n",
        "        # Redes\n",
        "        self.q_network = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_network = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        \n",
        "        # Optimizador y buffer\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "        self.steps = 0\n",
        "    \n",
        "    def get_action(self, state, training=True):\n",
        "        \"\"\"ε-greedy action selection\"\"\"\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax(dim=1).item()\n",
        "    \n",
        "    def update_epsilon(self):\n",
        "        \"\"\"Decay epsilon exponencialmente\"\"\"\n",
        "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
        "                       np.exp(-self.steps / self.epsilon_decay)\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"Un paso de entrenamiento\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        # Muestrear batch\n",
        "        transitions = self.replay_buffer.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        \n",
        "        # Convertir a tensores\n",
        "        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)\n",
        "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
        "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
        "        done_batch = torch.FloatTensor(batch.done).to(self.device)\n",
        "        \n",
        "        # Q-values actuales\n",
        "        current_q = self.q_network(state_batch).gather(1, action_batch)\n",
        "        \n",
        "        # Targets: r + γ * max Q_target(s', a')\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_network(next_state_batch).max(1)[0]\n",
        "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q\n",
        "        \n",
        "        # MSE Loss\n",
        "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
        "        \n",
        "        # Optimización\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def update_target_network(self):\n",
        "        \"\"\"Hard update de target network\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Crear agente\n",
        "dqn_agent = DQNAgent(state_dim=4, action_dim=2)\n",
        "print(\"Agente DQN creado exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Función de Entrenamiento para DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(env, agent, n_episodes=300, max_steps=500):\n",
        "    \"\"\"Entrena agente DQN\"\"\"\n",
        "    rewards_history = []\n",
        "    losses_history = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_losses = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Acción\n",
        "            action = agent.get_action(state, training=True)\n",
        "            \n",
        "            # Ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Almacenar y entrenar\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "            \n",
        "            # Actualizar\n",
        "            agent.steps += 1\n",
        "            agent.update_epsilon()\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Target network update\n",
        "        if episode % agent.target_update == 0:\n",
        "            agent.update_target_network()\n",
        "        \n",
        "        rewards_history.append(episode_reward)\n",
        "        avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
        "        losses_history.append(avg_loss)\n",
        "        \n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(rewards_history[-50:])\n",
        "            print(f\"Episode {episode + 1}/{n_episodes} | Avg Reward: {avg_reward:.1f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "    \n",
        "    return rewards_history, losses_history\n",
        "\n",
        "print(\"Función de entrenamiento definida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Entrenamiento en CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear ambiente CartPole\n",
        "env_cartpole = gym.make('CartPole-v1')\n",
        "state_dim = env_cartpole.observation_space.shape[0]\n",
        "action_dim = env_cartpole.action_space.n\n",
        "\n",
        "print(f\"CartPole-v1:\")\n",
        "print(f\"  State dimension: {state_dim}\")\n",
        "print(f\"  Action dimension: {action_dim}\")\n",
        "print(f\"  Max steps: 500\")\n",
        "print(f\"\\nEntrenando DQN en CartPole... (esto toma ~1-2 minutos)\")\n",
        "\n",
        "# Crear y entrenar agente\n",
        "dqn_agent = DQNAgent(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    learning_rate=1e-3,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=500,\n",
        "    buffer_size=10000,\n",
        "    batch_size=64,\n",
        "    target_update=10\n",
        ")\n",
        "\n",
        "# Entrenar\n",
        "dqn_rewards, dqn_losses = train_dqn(env_cartpole, dqn_agent, n_episodes=200, max_steps=500)\n",
        "\n",
        "print(f\"\\nEntrenamiento completado!\")\n",
        "print(f\"Reward promedio últimos 50 episodios: {np.mean(dqn_rewards[-50:]):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Evaluación y Visualización de DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar resultados\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Recompensas\n",
        "ax1.plot(dqn_rewards, alpha=0.6, label='Reward', color='blue')\n",
        "window = 10\n",
        "if len(dqn_rewards) >= window:\n",
        "    moving_avg = np.convolve(dqn_rewards, np.ones(window)/window, mode='valid')\n",
        "    ax1.plot(range(window-1, len(dqn_rewards)), moving_avg, \n",
        "            label=f'MA({window})', linewidth=2, color='darkblue')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Reward')\n",
        "ax1.set_title('DQN Training Rewards (CartPole)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Losses\n",
        "ax2.plot(dqn_losses, alpha=0.6, label='Loss', color='red')\n",
        "if len(dqn_losses) >= window:\n",
        "    moving_avg = np.convolve(dqn_losses, np.ones(window)/window, mode='valid')\n",
        "    ax2.plot(range(window-1, len(dqn_losses)), moving_avg,\n",
        "            label=f'MA({window})', linewidth=2, color='darkred')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('DQN Training Loss (CartPole)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDQN entrenado exitosamente en CartPole!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar agente entrenado\n",
        "def evaluate_agent(env, agent, n_episodes=10, render=False):\n",
        "    \"\"\"Evalúa el agente sin exploración\"\"\"\n",
        "    eval_rewards = []\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action = agent.get_action(state, training=False)  # Sin exploración\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "        \n",
        "        eval_rewards.append(episode_reward)\n",
        "    \n",
        "    return np.mean(eval_rewards), np.std(eval_rewards)\n",
        "\n",
        "# Evaluar DQN\n",
        "dqn_eval_mean, dqn_eval_std = evaluate_agent(env_cartpole, dqn_agent, n_episodes=20)\n",
        "print(f\"\\nEvaluación DQN (20 episodios):\")\n",
        "print(f\"  Reward promedio: {dqn_eval_mean:.2f}\")\n",
        "print(f\"  Desv. estándar: {dqn_eval_std:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 4: Double DQN\n",
        "\n",
        "### Problema: Sobreestimación de Q-values\n",
        "\n",
        "En DQN estándar:\n",
        "$$Q_{\\text{target}} = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')$$\n",
        "\n",
        "El problema es que si todas las acciones son sobrestimadas, el máximo también lo será.\n",
        "\n",
        "### Solución: Double DQN\n",
        "\n",
        "Usar la red online para **seleccionar** y la red target para **evaluar**:\n",
        "$$Q_{\\text{target}} = r + \\gamma Q_{\\text{target}}(s', \\arg\\max_{a'} Q_{\\text{online}}(s', a'))$$\n",
        "\n",
        "Paper: \"Deep Reinforcement Learning with Double Q-learning\" (van Hasselt et al., 2015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleDQNAgent:\n",
        "    \"\"\"Agente Double DQN - Reduce sobreestimación\"\"\"\n",
        "    def __init__(self, state_dim, action_dim,\n",
        "                 learning_rate=1e-3,\n",
        "                 gamma=0.99,\n",
        "                 epsilon_start=1.0,\n",
        "                 epsilon_end=0.01,\n",
        "                 epsilon_decay=500,\n",
        "                 buffer_size=10000,\n",
        "                 batch_size=64,\n",
        "                 target_update=10):\n",
        "        \n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.device = device\n",
        "        \n",
        "        # Redes (igual que DQN)\n",
        "        self.q_network = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_network = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "        self.steps = 0\n",
        "    \n",
        "    def get_action(self, state, training=True):\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax(dim=1).item()\n",
        "    \n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
        "                       np.exp(-self.steps / self.epsilon_decay)\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"Double DQN training step - Diferencia en el cálculo de targets\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        transitions = self.replay_buffer.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        \n",
        "        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)\n",
        "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
        "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
        "        done_batch = torch.FloatTensor(batch.done).to(self.device)\n",
        "        \n",
        "        current_q = self.q_network(state_batch).gather(1, action_batch)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Double DQN: Diferencia clave aquí\n",
        "            # 1. Seleccionar acciones con online network\n",
        "            next_actions = self.q_network(next_state_batch).argmax(1, keepdim=True)\n",
        "            # 2. Evaluar con target network\n",
        "            next_q = self.target_network(next_state_batch).gather(1, next_actions).squeeze()\n",
        "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q\n",
        "        \n",
        "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "print(\"Clase DoubleDQNAgent definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar Double DQN\n",
        "print(\"Entrenando Double DQN en CartPole...\")\n",
        "\n",
        "ddqn_agent = DoubleDQNAgent(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    learning_rate=1e-3,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=500,\n",
        "    buffer_size=10000,\n",
        "    batch_size=64,\n",
        "    target_update=10\n",
        ")\n",
        "\n",
        "ddqn_rewards, ddqn_losses = train_dqn(env_cartpole, ddqn_agent, n_episodes=200, max_steps=500)\n",
        "\n",
        "print(f\"\\nDouble DQN entrenado!\")\n",
        "print(f\"Reward promedio últimos 50 episodios: {np.mean(ddqn_rewards[-50:]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar DQN vs Double DQN\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "window = 10\n",
        "\n",
        "# DQN\n",
        "dqn_ma = np.convolve(dqn_rewards, np.ones(window)/window, mode='valid')\n",
        "ax1.plot(range(window-1, len(dqn_rewards)), dqn_ma, label='DQN', linewidth=2, color='blue')\n",
        "\n",
        "# Double DQN\n",
        "ddqn_ma = np.convolve(ddqn_rewards, np.ones(window)/window, mode='valid')\n",
        "ax1.plot(range(window-1, len(ddqn_rewards)), ddqn_ma, label='Double DQN', linewidth=2, color='green')\n",
        "\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Average Reward (MA-10)')\n",
        "ax1.set_title('DQN vs Double DQN')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Losses\n",
        "dqn_loss_ma = np.convolve(dqn_losses, np.ones(window)/window, mode='valid')\n",
        "ddqn_loss_ma = np.convolve(ddqn_losses, np.ones(window)/window, mode='valid')\n",
        "\n",
        "ax2.plot(range(window-1, len(dqn_losses)), dqn_loss_ma, label='DQN', linewidth=2, color='blue')\n",
        "ax2.plot(range(window-1, len(ddqn_losses)), ddqn_loss_ma, label='Double DQN', linewidth=2, color='green')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Loss (MA-10)')\n",
        "ax2.set_title('DQN vs Double DQN - Training Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluar\n",
        "ddqn_eval_mean, ddqn_eval_std = evaluate_agent(env_cartpole, ddqn_agent, n_episodes=20)\n",
        "print(f\"\\nEvaluación Double DQN (20 episodios):\")\n",
        "print(f\"  Reward promedio: {ddqn_eval_mean:.2f}\")\n",
        "print(f\"  Desv. estándar: {ddqn_eval_std:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis de diferencias\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ANÁLISIS: DQN vs Double DQN\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nDQN Estándar:\")\n",
        "print(f\"  Target = r + γ * max_a' Q_target(s', a')\")\n",
        "print(f\"  Problema: Sobreestimación si todo está sobrestimado\")\n",
        "\n",
        "print(\"\\nDouble DQN:\")\n",
        "print(f\"  Target = r + γ * Q_target(s', argmax_a' Q_online(s', a'))\")\n",
        "print(f\"  Ventaja: Desacopla selección de evaluación\")\n",
        "\n",
        "print(\"\\nResultados en CartPole-v1:\")\n",
        "print(f\"  DQN:        {dqn_eval_mean:.2f} ± {dqn_eval_std:.2f}\")\n",
        "print(f\"  Double DQN: {ddqn_eval_mean:.2f} ± {ddqn_eval_std:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 5: Dueling DQN\n",
        "\n",
        "### Arquitectura Dueling\n",
        "\n",
        "Separa la estimación de Q-values en dos streams:\n",
        "- **Value stream V(s)**: Valor de estar en el estado s\n",
        "- **Advantage stream A(s,a)**: Ventaja de tomar la acción a respecto al promedio\n",
        "\n",
        "$$Q(s,a) = V(s) + \\left(A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a')\\right)$$\n",
        "\n",
        "### Ventajas\n",
        "1. Aprende qué estados son valiosos sin aprender el efecto de cada acción\n",
        "2. Mejor generalización en espacios de acción grandes\n",
        "3. Útil cuando muchas acciones tienen efecto similar\n",
        "\n",
        "Paper: \"Dueling Network Architectures for Deep Reinforcement Learning\" (Wang et al., 2016)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "    \"\"\"Red Dueling DQN - Separa Value y Advantage\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        \n",
        "        # Feature extraction (compartido)\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Value stream: V(s)\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1)  # Un valor\n",
        "        )\n",
        "        \n",
        "        # Advantage stream: A(s,a)\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, action_dim)  # Ventaja por acción\n",
        "        )\n",
        "        \n",
        "        self.action_dim = action_dim\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Features compartidas\n",
        "        features = self.feature(x)\n",
        "        \n",
        "        # Value stream\n",
        "        value = self.value_stream(features)  # [batch, 1]\n",
        "        \n",
        "        # Advantage stream\n",
        "        advantage = self.advantage_stream(features)  # [batch, action_dim]\n",
        "        \n",
        "        # Aggregation: Q(s,a) = V(s) + (A(s,a) - mean_a(A(s,a)))\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        \n",
        "        return q_values\n",
        "    \n",
        "    def get_value_and_advantage(self, x):\n",
        "        \"\"\"Retorna V(s) y A(s,a) por separado para análisis\"\"\"\n",
        "        features = self.feature(x)\n",
        "        value = self.value_stream(features)\n",
        "        advantage = self.advantage_stream(features)\n",
        "        return value, advantage\n",
        "\n",
        "# Prueba\n",
        "dueling_net = DuelingDQN(state_dim=4, action_dim=2).to(device)\n",
        "print(\"Arquitectura Dueling DQN:\")\n",
        "print(dueling_net)\n",
        "\n",
        "# Forward pass\n",
        "dummy_state = torch.randn(1, 4).to(device)\n",
        "q_vals = dueling_net(dummy_state)\n",
        "v, a = dueling_net.get_value_and_advantage(dummy_state)\n",
        "print(f\"\\nValue V(s): {v.item():.3f}\")\n",
        "print(f\"Advantage A(s,a): {a.cpu().detach().numpy()[0]}\")\n",
        "print(f\"Q-values Q(s,a): {q_vals.cpu().detach().numpy()[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DuelingDQNAgent:\n",
        "    \"\"\"Agente Dueling DQN\"\"\"\n",
        "    def __init__(self, state_dim, action_dim,\n",
        "                 learning_rate=1e-3,\n",
        "                 gamma=0.99,\n",
        "                 epsilon_start=1.0,\n",
        "                 epsilon_end=0.01,\n",
        "                 epsilon_decay=500,\n",
        "                 buffer_size=10000,\n",
        "                 batch_size=64,\n",
        "                 target_update=10):\n",
        "        \n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.device = device\n",
        "        \n",
        "        # Usar arquitectura Dueling\n",
        "        self.q_network = DuelingDQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_network = DuelingDQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "        self.steps = 0\n",
        "    \n",
        "    def get_action(self, state, training=True):\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax(dim=1).item()\n",
        "    \n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
        "                       np.exp(-self.steps / self.epsilon_decay)\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"Training con arquitectura Dueling\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        transitions = self.replay_buffer.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        \n",
        "        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)\n",
        "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
        "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
        "        done_batch = torch.FloatTensor(batch.done).to(self.device)\n",
        "        \n",
        "        # Forward pass - arquitectura Dueling calcula Q automáticamente\n",
        "        current_q = self.q_network(state_batch).gather(1, action_batch)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_network(next_state_batch).max(1)[0]\n",
        "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q\n",
        "        \n",
        "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "    \n",
        "    def analyze_state(self, state):\n",
        "        \"\"\"Analiza V(s) y A(s,a) para un estado\"\"\"\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            value, advantage = self.q_network.get_value_and_advantage(state_tensor)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            \n",
        "            return {\n",
        "                'value': value.cpu().numpy()[0, 0],\n",
        "                'advantage': advantage.cpu().numpy()[0],\n",
        "                'q_values': q_values.cpu().numpy()[0]\n",
        "            }\n",
        "\n",
        "print(\"Clase DuelingDQNAgent definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar Dueling DQN\n",
        "print(\"Entrenando Dueling DQN en CartPole...\")\n",
        "\n",
        "dueling_agent = DuelingDQNAgent(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    learning_rate=1e-3,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=500,\n",
        "    buffer_size=10000,\n",
        "    batch_size=64,\n",
        "    target_update=10\n",
        ")\n",
        "\n",
        "dueling_rewards, dueling_losses = train_dqn(env_cartpole, dueling_agent, n_episodes=200, max_steps=500)\n",
        "\n",
        "print(f\"\\nDueling DQN entrenado!\")\n",
        "print(f\"Reward promedio últimos 50 episodios: {np.mean(dueling_rewards[-50:]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar Value y Advantage de un estado\n",
        "test_state, _ = env_cartpole.reset()\n",
        "analysis = dueling_agent.analyze_state(test_state)\n",
        "\n",
        "print(\"\\nAnálisis de arquitectura Dueling DQN:\")\n",
        "print(f\"Estado inicial: {test_state}\")\n",
        "print(f\"\\nValue V(s): {analysis['value']:.4f}\")\n",
        "print(f\"Advantage A(s,a): {analysis['advantage']}\")\n",
        "print(f\"Q-values Q(s,a): {analysis['q_values']}\")\n",
        "print(f\"\\nMejor acción: {np.argmax(analysis['q_values'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar los 3 algoritmos\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "window = 10\n",
        "algorithms = ['DQN', 'Double DQN', 'Dueling DQN']\n",
        "rewards_list = [dqn_rewards, ddqn_rewards, dueling_rewards]\n",
        "colors = ['blue', 'green', 'red']\n",
        "\n",
        "for i, (ax, algo, rewards, color) in enumerate(zip(axes, algorithms, rewards_list, colors)):\n",
        "    ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "    ax.plot(range(window-1, len(rewards)), ma, linewidth=2, color=color)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Average Reward (MA-10)')\n",
        "    ax.set_title(f'{algo}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim([0, 500])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluar\n",
        "dueling_eval_mean, dueling_eval_std = evaluate_agent(env_cartpole, dueling_agent, n_episodes=20)\n",
        "print(f\"\\nEvaluación Dueling DQN (20 episodios):\")\n",
        "print(f\"  Reward promedio: {dueling_eval_mean:.2f}\")\n",
        "print(f\"  Desv. estándar: {dueling_eval_std:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 6: Comparación de los 3 Algoritmos\n",
        "\n",
        "Análisis detallado de ventajas y desventajas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparación visual lado a lado\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "window = 10\n",
        "\n",
        "# Recompensas\n",
        "dqn_ma = np.convolve(dqn_rewards, np.ones(window)/window, mode='valid')\n",
        "ddqn_ma = np.convolve(ddqn_rewards, np.ones(window)/window, mode='valid')\n",
        "dueling_ma = np.convolve(dueling_rewards, np.ones(window)/window, mode='valid')\n",
        "\n",
        "ax1.plot(range(window-1, len(dqn_rewards)), dqn_ma, label='DQN', linewidth=2.5, color='blue')\n",
        "ax1.plot(range(window-1, len(ddqn_rewards)), ddqn_ma, label='Double DQN', linewidth=2.5, color='green')\n",
        "ax1.plot(range(window-1, len(dueling_rewards)), dueling_ma, label='Dueling DQN', linewidth=2.5, color='red')\n",
        "ax1.set_xlabel('Episode', fontsize=12)\n",
        "ax1.set_ylabel('Average Reward (MA-10)', fontsize=12)\n",
        "ax1.set_title('Comparación de Algoritmos - Recompensas', fontsize=12)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Losses\n",
        "dqn_loss_ma = np.convolve(dqn_losses, np.ones(window)/window, mode='valid')\n",
        "ddqn_loss_ma = np.convolve(ddqn_losses, np.ones(window)/window, mode='valid')\n",
        "dueling_loss_ma = np.convolve(dueling_losses, np.ones(window)/window, mode='valid')\n",
        "\n",
        "ax2.plot(range(window-1, len(dqn_losses)), dqn_loss_ma, label='DQN', linewidth=2.5, color='blue')\n",
        "ax2.plot(range(window-1, len(ddqn_losses)), ddqn_loss_ma, label='Double DQN', linewidth=2.5, color='green')\n",
        "ax2.plot(range(window-1, len(dueling_losses)), dueling_loss_ma, label='Dueling DQN', linewidth=2.5, color='red')\n",
        "ax2.set_xlabel('Episode', fontsize=12)\n",
        "ax2.set_ylabel('Loss (MA-10)', fontsize=12)\n",
        "ax2.set_title('Comparación de Algoritmos - Training Loss', fontsize=12)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tabla de comparación\n",
        "import pandas as pd\n",
        "\n",
        "comparison_data = {\n",
        "    'Algoritmo': ['DQN', 'Double DQN', 'Dueling DQN'],\n",
        "    'Reward Final (Train)': [\n",
        "        f\"{np.mean(dqn_rewards[-50:]):.2f}\",\n",
        "        f\"{np.mean(ddqn_rewards[-50:]):.2f}\",\n",
        "        f\"{np.mean(dueling_rewards[-50:]):.2f}\"\n",
        "    ],\n",
        "    'Reward Eval': [\n",
        "        f\"{dqn_eval_mean:.2f} ± {dqn_eval_std:.2f}\",\n",
        "        f\"{ddqn_eval_mean:.2f} ± {ddqn_eval_std:.2f}\",\n",
        "        f\"{dueling_eval_mean:.2f} ± {dueling_eval_std:.2f}\"\n",
        "    ],\n",
        "    'Complejidad': ['Baja', 'Baja', 'Media'],\n",
        "    'Estabilidad': ['Buena', 'Mejor', 'Muy Buena']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACIÓN CUANTITATIVA DE ALGORITMOS\")\n",
        "print(\"=\"*80)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis Cualitativo\n",
        "\n",
        "| Aspecto | DQN | Double DQN | Dueling DQN |\n",
        "|--------|-----|-----------|-------------|\n",
        "| **Ecuación Update** | r + γ max Q_t(s',a') | r + γ Q_t(s',argmax Q(s',a')) | Arquitectura V+A |\n",
        "| **Problema Principal** | Sobreestimación | Reducida | Separación V/A |\n",
        "| **Convergencia** | Lenta | Más rápida | Mejor generalización |\n",
        "| **Complejidad Código** | Simple | Muy simple | Media |\n",
        "| **Uso de Memoria** | Normal | Normal | Ligeramente mayor |\n",
        "| **Mejoras Compatibles** | PER, Dueling | PER, Dueling | Double, PER |\n",
        "| **Caso de Uso Ideal** | Inicio/Simple | Espacios acción pequeños | Espacios acción grandes |\n",
        "| **Paper** | Mnih et al. (2015) | van Hasselt et al. (2015) | Wang et al. (2016) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumen de claves\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMEN TÉCNICO: DIFERENCIAS CLAVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary = \"\"\"\n",
        "1. DQN (Mnih et al., 2015)\n",
        "   - Usa Experience Replay para decorrelacionar datos\n",
        "   - Target Network separada para estabilidad\n",
        "   - Update: Q ← r + γ max Q_target(s')\n",
        "   - Problema: Max de máximos sobrestima valores\n",
        "\n",
        "2. Double DQN (van Hasselt et al., 2015)\n",
        "   - Pequeño cambio, gran impacto\n",
        "   - Selecciona acción con Q-online, evalúa con Q-target\n",
        "   - Update: Q ← r + γ Q_target(s', argmax_a Q_online(s',a))\n",
        "   - Reduces sesgo de sobreestimación\n",
        "\n",
        "3. Dueling DQN (Wang et al., 2016)\n",
        "   - Arquitectura dual: Value stream + Advantage stream\n",
        "   - V(s): Valor intrínseco del estado\n",
        "   - A(s,a): Ventaja relativa de cada acción\n",
        "   - Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))\n",
        "   - Mejor para espacios de acción grandes\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 7: Ejercicios Prácticos\n",
        "\n",
        "Tareas para consolidar el aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 1: Implementar DQN con Soft Update\n",
        "\n",
        "Modifica el DQNAgent para incluir soft update de la target network.\n",
        "\n",
        "Soft update: θ_target ← τ * θ_online + (1-τ) * θ_target\n",
        "\n",
        "Ventajas: Actualizaciones más suaves, mejor estabilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SoftUpdateDQNAgent(DQNAgent):\n",
        "    \"\"\"DQN con Soft Update (TAU)\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, tau=0.01, **kwargs):\n",
        "        \"\"\"tau: factor de soft update (0.01 típico)\"\"\"\n",
        "        super().__init__(state_dim, action_dim, **kwargs)\n",
        "        self.tau = tau\n",
        "    \n",
        "    def soft_update_target_network(self):\n",
        "        \"\"\"Soft update de target network\"\"\"\n",
        "        for target_param, param in zip(\n",
        "            self.target_network.parameters(),\n",
        "            self.q_network.parameters()\n",
        "        ):\n",
        "            target_param.data.copy_(\n",
        "                self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
        "            )\n",
        "    \n",
        "    # El entrenamiento es igual, solo cambia el update\n",
        "\n",
        "# Prueba\n",
        "print(\"Ejercicio 1: Soft Update implementado\")\n",
        "print(\"TODO: Entrena un agente con soft update (tau=0.01) y compara convergencia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 2: Hiperparámetros Óptimos\n",
        "\n",
        "Experimenta con diferentes valores de:\n",
        "- Learning rate (1e-4, 5e-4, 1e-3, 5e-3)\n",
        "- Gamma (0.95, 0.99, 0.999)\n",
        "- Epsilon decay (100, 500, 1000)\n",
        "- Buffer size (5000, 10000, 50000)\n",
        "\n",
        "¿Cuál combinación da mejor rendimiento?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plantilla para grid search\n",
        "\n",
        "hyperparams = {\n",
        "    'learning_rate': [1e-3],  # Añade más valores\n",
        "    'gamma': [0.99],           # Añade más valores\n",
        "    'epsilon_decay': [500],    # Añade más valores\n",
        "    'batch_size': [64]         # Añade más valores\n",
        "}\n",
        "\n",
        "# TODO: Implementa búsqueda exhaustiva o Bayesiana\n",
        "# Consejos:\n",
        "# 1. Comienza con 2-3 valores por parámetro\n",
        "# 2. Usa reward promedio últimos 50 episodios como métrica\n",
        "# 3. Ejecuta múltiples seeds para varianza\n",
        "\n",
        "print(\"Ejercicio 2: Búsqueda de Hiperparámetros\")\n",
        "print(\"TODO: Implementa grid search sobre los hiperparámetros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 3: Entrenar en LunarLander\n",
        "\n",
        "LunarLander-v2 es más complejo que CartPole.\n",
        "- Estado: 8 dimensiones\n",
        "- Acciones: 4 discretas\n",
        "- Recompensa: -200 a 200+\n",
        "\n",
        "Requisitos:\n",
        "- Network más grande (256 hidden)\n",
        "- Buffer más grande (50000)\n",
        "- Más episodios de entrenamiento (500+)\n",
        "- Ajustar learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plantilla para LunarLander\n",
        "\n",
        "def train_on_lunar_lander():\n",
        "    \"\"\"Entrena los 3 algoritmos en LunarLander-v2\"\"\"\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    \n",
        "    state_dim = env.observation_space.shape[0]  # 8\n",
        "    action_dim = env.action_space.n              # 4\n",
        "    \n",
        "    print(f\"LunarLander-v2: state_dim={state_dim}, action_dim={action_dim}\")\n",
        "    \n",
        "    # Configuración para LunarLander (más exigente)\n",
        "    config = {\n",
        "        'learning_rate': 5e-4,      # Más bajo para estabilidad\n",
        "        'gamma': 0.99,\n",
        "        'epsilon_start': 1.0,\n",
        "        'epsilon_end': 0.01,\n",
        "        'epsilon_decay': 1000,      # Más largo\n",
        "        'buffer_size': 50000,       # Más grande\n",
        "        'batch_size': 128,          # Más grande\n",
        "        'target_update': 5          # Más frecuente\n",
        "    }\n",
        "    \n",
        "    # TODO: Entrena aquí\n",
        "    # Objetivo: Conseguir reward > 200 (considérate experto con >250)\n",
        "    \n",
        "    env.close()\n",
        "    return None\n",
        "\n",
        "print(\"Ejercicio 3: LunarLander-v2\")\n",
        "print(\"TODO: Implementa entrenamiento en ambiente más complejo\")\n",
        "# train_on_lunar_lander()  # Descomenta para ejecutar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 4: Análisis de Value vs Advantage\n",
        "\n",
        "Visualiza cómo evolucionan V(s) y A(s,a) durante el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestrear estados y analizar Value/Advantage\n",
        "values_collected = []\n",
        "advantages_collected = []\n",
        "\n",
        "# Episodios de análisis\n",
        "for ep in range(5):\n",
        "    state, _ = env_cartpole.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        analysis = dueling_agent.analyze_state(state)\n",
        "        values_collected.append(analysis['value'])\n",
        "        advantages_collected.extend(analysis['advantage'])\n",
        "        \n",
        "        action = dueling_agent.get_action(state, training=False)\n",
        "        state, _, terminated, truncated, _ = env_cartpole.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "# Visualizar distribuciones\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(values_collected, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[0].set_xlabel('V(s)')\n",
        "axes[0].set_ylabel('Frecuencia')\n",
        "axes[0].set_title('Distribución de Values V(s)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(advantages_collected, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
        "axes[1].set_xlabel('A(s,a)')\n",
        "axes[1].set_ylabel('Frecuencia')\n",
        "axes[1].set_title('Distribución de Advantages A(s,a)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nEstadísticas Value:\")\n",
        "print(f\"  Media: {np.mean(values_collected):.3f}\")\n",
        "print(f\"  Desv: {np.std(values_collected):.3f}\")\n",
        "print(f\"\\nEstadísticas Advantage:\")\n",
        "print(f\"  Media: {np.mean(advantages_collected):.3f}\")\n",
        "print(f\"  Desv: {np.std(advantages_collected):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 5: Guardar y Cargar Modelos\n",
        "\n",
        "Implementa persistencia de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_agent(agent, path):\n",
        "    \"\"\"Guarda el agente DQN\"\"\"\n",
        "    checkpoint = {\n",
        "        'q_network': agent.q_network.state_dict(),\n",
        "        'target_network': agent.target_network.state_dict(),\n",
        "        'optimizer': agent.optimizer.state_dict(),\n",
        "        'epsilon': agent.epsilon,\n",
        "        'steps': agent.steps\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Modelo guardado en {path}\")\n",
        "\n",
        "def load_agent(agent, path):\n",
        "    \"\"\"Carga el agente DQN\"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    agent.q_network.load_state_dict(checkpoint['q_network'])\n",
        "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
        "    agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    agent.epsilon = checkpoint['epsilon']\n",
        "    agent.steps = checkpoint['steps']\n",
        "    print(f\"Modelo cargado desde {path}\")\n",
        "\n",
        "# Ejemplo\n",
        "save_agent(dqn_agent, '/tmp/dqn_model.pth')\n",
        "load_agent(dqn_agent, '/tmp/dqn_model.pth')\n",
        "\n",
        "print(\"\\nModelos guardados y cargados exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sección 8: Conclusiones\n",
        "\n",
        "### Resumen de lo Aprendido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Deep Q-Network (DQN)** es el fundamento de Deep RL\n",
        "   - Combina Q-Learning con redes neuronales\n",
        "   - Experience Replay rompe correlación temporal\n",
        "   - Target Network proporciona estabilidad\n",
        "\n",
        "2. **Double DQN** mejora DQN reduciendo sobreestimación\n",
        "   - Cambio pequeño: desacopla selección de evaluación\n",
        "   - Mayor estabilidad en entrenamiento\n",
        "   - Mejor rendimiento en muchos ambientes\n",
        "\n",
        "3. **Dueling DQN** utiliza arquitectura dual\n",
        "   - Separa Value (qué tan bueno es estado) de Advantage (qué tan buena es acción)\n",
        "   - Mejor generalización en espacios de acción grandes\n",
        "   - Se combina perfectamente con Double DQN\n",
        "\n",
        "4. **Progresión de mejoras**\n",
        "   - DQN (2015): Fundacional\n",
        "   - Double DQN (2015): Muy poco código extra, gran impacto\n",
        "   - Dueling DQN (2016): Mejor arquitectura\n",
        "   - Rainbow (2017): Combina TODO + PER + Noisy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Próximos Pasos\n",
        "\n",
        "1. **Prioritized Experience Replay (PER)**\n",
        "   - Muestrea transiciones importantes (TD error alto)\n",
        "   - Acelera aprendizaje significativamente\n",
        "\n",
        "2. **Noisy Networks**\n",
        "   - Exploración paramétrica en lugar de ε-greedy\n",
        "   - Mejor exploración coordenada\n",
        "\n",
        "3. **Distributional RL**\n",
        "   - C51: Aprender distribución de Q, no solo esperanza\n",
        "   - QR-DQN: Quantile Regression\n",
        "\n",
        "4. **Policy Gradient Methods**\n",
        "   - Actor-Critic\n",
        "   - PPO (Proximal Policy Optimization)\n",
        "   - TRPO (Trust Region Policy Optimization)\n",
        "\n",
        "5. **Multi-Agent RL**\n",
        "   - QMIX: Mezcla cooperativa de Q-values\n",
        "   - MADDPG: Multi-Agent Deep Deterministic Policy Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumen final\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSIÓN: DEEP REINFORCEMENT LEARNING CON DQN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_stats = f\"\"\"\n",
        "RESULTADOS EN CARTPOLE-V1 (200 episodios de entrenamiento):\n",
        "\n",
        "  DQN:\n",
        "    - Reward Training (MA-50): {np.mean(dqn_rewards[-50:]):.2f}\n",
        "    - Reward Evaluación: {dqn_eval_mean:.2f} ± {dqn_eval_std:.2f}\n",
        "    - Complejidad: Baja\n",
        "\n",
        "  Double DQN:\n",
        "    - Reward Training (MA-50): {np.mean(ddqn_rewards[-50:]):.2f}\n",
        "    - Reward Evaluación: {ddqn_eval_mean:.2f} ± {ddqn_eval_std:.2f}\n",
        "    - Complejidad: Muy baja (1 línea diferencia)\n",
        "\n",
        "  Dueling DQN:\n",
        "    - Reward Training (MA-50): {np.mean(dueling_rewards[-50:]):.2f}\n",
        "    - Reward Evaluación: {dueling_eval_mean:.2f} ± {dueling_eval_std:.2f}\n",
        "    - Complejidad: Media\n",
        "\n",
        "LECCIONES CLAVE:\n",
        "\n",
        "1. DQN es accesible pero fundamental\n",
        "   - Implementación clara de conceptos clave\n",
        "   - Buenos resultados con tuning mínimo\n",
        "\n",
        "2. Mejoras incrementales importan mucho\n",
        "   - Double DQN: cambio mínimo, mejora significativa\n",
        "   - Dueling: arquitectura mejor, más insumos\n",
        "\n",
        "3. Tuning es crítico en Deep RL\n",
        "   - Learning rate, gamma, epsilon decay\n",
        "   - Tamaño buffer y batch size\n",
        "   - Hidden dimensions y target update\n",
        "\n",
        "4. Validación rigurosa necesaria\n",
        "   - Múltiples seeds (mínimo 3)\n",
        "   - Evaluation sin exploración\n",
        "   - Media móvil para noisy rewards\n",
        "\n",
        "RECOMENDACIONES DE USO:\n",
        "\n",
        "  Usa DQN si:\n",
        "    - Es tu primer algoritmo\n",
        "    - Necesitas baseline simple\n",
        "    - Ambiente pequeño, acción discreta\n",
        "\n",
        "  Usa Double DQN si:\n",
        "    - Sospechas sobreestimación\n",
        "    - Entrenamiento inestable\n",
        "    - Poco overhead, mucho beneficio\n",
        "\n",
        "  Usa Dueling DQN si:\n",
        "    - Muchas acciones disponibles\n",
        "    - Espacio de acción discreto grande\n",
        "    - Necesitas mejor generalización\n",
        "\n",
        "  Usa Rainbow si:\n",
        "    - Quieres estado del arte\n",
        "    - Tienes budget computacional\n",
        "    - Performance es crítica\n",
        "\"\"\"\n",
        "\n",
        "print(summary_stats)\n",
        "print(\"=\"*80)\n",
        "print(\"\\n¡Felicidades! Has completado el tutorial de Deep RL/DQN.\")\n",
        "print(\"Ahora eres capaz de:\")\n",
        "print(\"  - Implementar DQN desde cero\")\n",
        "print(\"  - Entender Double DQN y Dueling DQN\")\n",
        "print(\"  - Entrenar en ambientes complejos\")\n",
        "print(\"  - Analizar y debuggear algoritmos\")\n",
        "print(\"\\nPróximos desafíos: Explora Policy Gradient, Actor-Critic y técnicas avanzadas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cerrar ambiente\n",
        "env_cartpole.close()\n",
        "print(\"\\nTutorial completado exitosamente!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
