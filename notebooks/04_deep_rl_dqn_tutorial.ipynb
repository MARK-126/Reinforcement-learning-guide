{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) and Variants\n",
    "\n",
    "Understanding and implementing advanced Deep Reinforcement Learning algorithms.\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "* Implement a DQN agent from scratch\n",
    "* Understand experience replay and target networks\n",
    "* Improve upon DQN with Double DQN\n",
    "* Leverage architectural advantages with Dueling DQN\n",
    "* Train and evaluate agents on control tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. Packages](#1)\n",
    "- [2. Introduction to Deep Reinforcement Learning](#2)\n",
    "    - [2.1 The Challenge of Q-Learning](#2-1)\n",
    "    - [2.2 Deep Q-Networks (DQN)](#2-2)\n",
    "- [3. PyTorch Fundamentals for RL](#3)\n",
    "    - [3.1 Tensors and Autograd](#3-1)\n",
    "    - [3.2 Neural Networks for Q-Approximation](#3-2)\n",
    "- [4. Exercise 1: Implement DQN Network](#4)\n",
    "    - [Exercise 1 - implement_dqn_network](#ex-1)\n",
    "- [5. Exercise 2: Implement Replay Buffer](#5)\n",
    "    - [Exercise 2 - implement_replay_buffer](#ex-2)\n",
    "- [6. Exercise 3: Implement DQN Update](#6)\n",
    "    - [Exercise 3 - implement_dqn_update](#ex-3)\n",
    "- [7. Double DQN: Reducing Overestimation](#7)\n",
    "    - [7.1 The Overestimation Problem](#7-1)\n",
    "    - [Exercise 4 - implement_double_dqn](#ex-4)\n",
    "- [8. Dueling DQN: Architectural Improvements](#8)\n",
    "    - [8.1 Value-Advantage Decomposition](#8-1)\n",
    "    - [Exercise 5 - implement_dueling_dqn](#ex-5)\n",
    "- [9. Algorithm Comparison](#9)\n",
    "    - [9.1 Comparative Analysis](#9-1)\n",
    "    - [9.2 Performance Results](#9-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import DQN utilities\n",
    "import sys\n",
    "sys.path.append('/home/user/Reinforcement-learning-guide/notebooks')\n",
    "from dqn_utils import *\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Plotting style\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Introduction to Deep Reinforcement Learning\n",
    "\n",
    "Deep Reinforcement Learning combines two powerful paradigms:\n",
    "1. **Reinforcement Learning**: Learning through interaction and reward signals\n",
    "2. **Deep Neural Networks**: Function approximation for high-dimensional problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 The Challenge of Q-Learning\n",
    "\n",
    "In traditional Q-Learning, we maintain a table Q[s,a] for all state-action pairs:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\right]$$\n",
    "\n",
    "**Limitations:**\n",
    "- Infeasible for large/continuous state spaces\n",
    "- Cannot generalize to unseen states\n",
    "- Curse of dimensionality (images, complex features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 Deep Q-Networks (DQN)\n",
    "\n",
    "**Solution:** Approximate Q-values with a neural network:\n",
    "\n",
    "$$Q(s,a) \\approx Q_\\theta(s,a) = \\text{Network}(s, \\theta)$$\n",
    "\n",
    "**Key Innovation from Mnih et al. (2015):**\n",
    "1. **Experience Replay**: Break temporal correlations by training on random batches\n",
    "2. **Target Network**: Separate network for computing targets, updated periodically\n",
    "\n",
    "**Bellman Equation for DQN:**\n",
    "$$Q_\\text{target}(s,a) = r + \\gamma \\max_{a'} Q_\\text{target}(s', a')$$\n",
    "\n",
    "**Loss Function:**\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}\\left[ \\left( Q_\\text{target}(s,a) - Q_\\theta(s,a) \\right)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. PyTorch Fundamentals for RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 Tensors and Autograd\n",
    "\n",
    "PyTorch provides:\n",
    "- **Tensors**: GPU-accelerated arrays with automatic differentiation\n",
    "- **Autograd**: Automatic computation of gradients\n",
    "- **nn.Module**: Framework for building neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Basic tensor operations and autograd\n",
    "\n",
    "# Create tensors\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.randn(3, 4, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = torch.mm(x, y)\n",
    "loss = z.sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"x gradient shape: {x.grad.shape}\")\n",
    "print(f\"Gradient exists: {x.grad is not None}\")\n",
    "print(f\"Loss value: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 Neural Networks for Q-Approximation\n",
    "\n",
    "We need a network that:\n",
    "- Takes state as input\n",
    "- Outputs Q-value for each action\n",
    "- Is differentiable for gradient-based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example architecture\n",
    "class ExampleQNetwork(nn.Module):\n",
    "    \"\"\"Simple Q-Network architecture for demonstration\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ExampleQNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "# Test on CartPole-v1\n",
    "env_test = gym.make('CartPole-v1')\n",
    "state_dim = env_test.observation_space.shape[0]\n",
    "action_dim = env_test.action_space.n\n",
    "\n",
    "print(f\"CartPole Environment:\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Action dimension: {action_dim}\")\n",
    "\n",
    "# Create and test network\n",
    "q_net = ExampleQNetwork(state_dim, action_dim).to(device)\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print_network_summary(q_net, (1, state_dim))\n",
    "\n",
    "# Forward pass\n",
    "dummy_state = torch.randn(1, state_dim).to(device)\n",
    "q_values = q_net(dummy_state)\n",
    "print(f\"\\nQ-values output shape: {q_values.shape}\")\n",
    "print(f\"Q-values: {q_values.cpu().detach().numpy()}\")\n",
    "\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4. Exercise 1: Implement DQN Network\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - implement_dqn_network\n",
    "\n",
    "**Instructions:** Implement a basic DQN network that:\n",
    "1. Takes state vector as input\n",
    "2. Passes through two hidden layers with ReLU activation\n",
    "3. Outputs Q-values for all actions\n",
    "\n",
    "**Network Architecture:**\n",
    "```\n",
    "Input (state_dim) → Hidden (hidden_dim) → ReLU → Hidden (hidden_dim) → ReLU → Output (action_dim)\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Use `nn.Sequential` for clean architecture\n",
    "- ReLU activation between layers\n",
    "- No activation on output (raw Q-values)\n",
    "- Initialize properly using PyTorch defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DQN Network Implementation\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network for approximating Q(s,a) values.\n",
    "    \n",
    "    Args:\n",
    "        state_dim (int): Dimension of state space\n",
    "        action_dim (int): Dimension of action space\n",
    "        hidden_dim (int): Dimension of hidden layers (default: 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: state → Q-values\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): State tensor of shape (batch_size, state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values of shape (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        return self.network(x)\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "# Test Exercise 1\n",
    "print(\"Testing Exercise 1: DQN Network Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_dqn_network(DQN, state_dim=4, action_dim=2, hidden_dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Neural networks can approximate Q-values: $Q(s,a) \\approx Q_\\theta(s,a)$\n",
    "- Use ReLU activations for hidden layers to enable non-linear function approximation\n",
    "- Output layer has no activation - Q-values can be positive or negative\n",
    "- Forward pass takes a state vector and outputs Q-values for all actions\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5. Exercise 2: Implement Replay Buffer\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - implement_replay_buffer\n",
    "\n",
    "**Instructions:** Implement an Experience Replay Buffer that:\n",
    "1. Stores transitions (s, a, r, s', done) in a deque\n",
    "2. Has a maximum capacity\n",
    "3. Can sample random mini-batches\n",
    "4. Returns Transition namedtuples\n",
    "\n",
    "**Why Experience Replay?**\n",
    "- **Breaks temporal correlations**: Consecutive transitions are highly correlated\n",
    "- **Enables data reuse**: Same experience used multiple times\n",
    "- **Improves stability**: Random sampling prevents biased updates\n",
    "\n",
    "**Key Methods:**\n",
    "- `push()`: Store a transition\n",
    "- `sample()`: Get random batch of transitions\n",
    "- `__len__()`: Return current buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Replay Buffer Implementation\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer for storing and sampling transitions.\n",
    "    \n",
    "    Args:\n",
    "        capacity (int): Maximum number of transitions to store (default: 10000)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a transition in the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode ended\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a random batch of transitions.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of transitions to sample\n",
    "            \n",
    "        Returns:\n",
    "            list: List of Transition namedtuples\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get current buffer size.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of transitions currently in buffer\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        return len(self.buffer)\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "# Test Exercise 2\n",
    "print(\"Testing Exercise 2: Replay Buffer Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_replay_buffer(ReplayBuffer, capacity=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Experience Replay breaks temporal correlations in training data\n",
    "- Use a deque with maxlen for efficient circular buffer management\n",
    "- Sample uniformly at random - all experiences have equal importance (later we'll use Prioritized Replay)\n",
    "- Sufficient buffer size is critical: typically 10,000 to 1,000,000 transitions\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6. Exercise 3: Implement DQN Update\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - implement_dqn_update\n",
    "\n",
    "**Instructions:** Implement a complete DQN Agent with:\n",
    "1. Q-network and target network\n",
    "2. Action selection with ε-greedy exploration\n",
    "3. Training step using Bellman equation\n",
    "4. Epsilon decay for reducing exploration over time\n",
    "5. Target network updates\n",
    "\n",
    "**DQN Training Loop:**\n",
    "1. Select action using ε-greedy: with probability ε take random action, else take argmax_a Q(s,a)\n",
    "2. Execute action and observe reward and next state\n",
    "3. Store transition in replay buffer\n",
    "4. Sample mini-batch from buffer\n",
    "5. Compute target: $Q_\\text{target} = r + \\gamma (1-\\text{done}) \\max_{a'} Q_\\text{target}(s', a')$\n",
    "6. Update Q-network: minimize $(Q_\\text{target} - Q(s,a))^2$\n",
    "7. Periodically update target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DQN Agent Implementation\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network Agent with experience replay and target network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim,\n",
    "                 learning_rate=1e-3,\n",
    "                 gamma=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.01,\n",
    "                 epsilon_decay=500,\n",
    "                 buffer_size=10000,\n",
    "                 batch_size=64,\n",
    "                 target_update=10):\n",
    "        \"\"\"\n",
    "        Initialize DQN Agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: State space dimension\n",
    "            action_dim: Action space dimension\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            gamma: Discount factor\n",
    "            epsilon_start: Initial exploration rate\n",
    "            epsilon_end: Minimum exploration rate\n",
    "            epsilon_decay: Decay rate for epsilon\n",
    "            buffer_size: Replay buffer capacity\n",
    "            batch_size: Training batch size\n",
    "            target_update: Steps between target network updates\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.device = device\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        # Optimizer and replay buffer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Select action using ε-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            training: If True, use exploration. If False, use pure exploitation\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay epsilon using exponential decay formula:\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * exp(-steps / epsilon_decay)\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                       np.exp(-self.steps / self.epsilon_decay)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store transition in replay buffer.\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Perform one training step:\n",
    "        1. Sample batch from replay buffer\n",
    "        2. Compute targets using target network\n",
    "        3. Compute loss (MSE between current and target Q-values)\n",
    "        4. Update Q-network parameters\n",
    "        \n",
    "        Returns:\n",
    "            float: Loss value for this batch (or None if buffer too small)\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_state_batch).max(1)[0]\n",
    "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q\n",
    "        \n",
    "        # Loss\n",
    "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Hard update: Copy Q-network parameters to target network.\n",
    "        This breaks the temporal correlation between targets and current network.\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "# Test Exercise 3\n",
    "print(\"Testing Exercise 3: DQN Agent Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_dqn_update(DQNAgent, state_dim=4, action_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- **ε-greedy exploration**: Balance exploration (random) vs exploitation (argmax)\n",
    "- **Bellman equation target**: $Q_\\text{target} = r + \\gamma (1-\\text{done}) \\max_{a'} Q_\\text{target}(s', a')$\n",
    "- **Target network**: Separate network prevents moving target problem\n",
    "- **No gradients through target network**: Use `torch.no_grad()` when computing targets\n",
    "- **Gradient clipping**: Prevents exploding gradients, improves stability\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7. Double DQN: Reducing Overestimation\n",
    "\n",
    "<a name='7-1'></a>\n",
    "### 7.1 The Overestimation Problem\n",
    "\n",
    "**Problem with Standard DQN:**\n",
    "\n",
    "In DQN, the target uses $\\max_a$ which can lead to overestimation:\n",
    "$$Q_\\text{target} = r + \\gamma \\max_{a'} Q_\\text{target}(s', a')$$\n",
    "\n",
    "If Q-values are noisy (as they are early in training), the maximum can be biased upward.\n",
    "\n",
    "**Example:**\n",
    "- True Q-values: [0.5, 0.5, 0.5]\n",
    "- Estimated Q-values: [0.7, 0.5, 0.3] (with noise)\n",
    "- DQN selects: max = 0.7 (overestimated!)\n",
    "\n",
    "**Solution: Double DQN**\n",
    "\n",
    "From van Hasselt et al. (2015), decouple action selection from evaluation:\n",
    "$$Q_\\text{target} = r + \\gamma Q_\\text{target}\\left(s', \\arg\\max_{a'} Q(s', a')\\right)$$\n",
    "\n",
    "Key insight: Use online network to **select** action, target network to **evaluate** it.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces overestimation bias\n",
    "- More stable learning\n",
    "- Minimal code change (literally 1-2 lines!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - implement_double_dqn\n",
    "\n",
    "**Instructions:** Modify the DQN training step to implement Double DQN:\n",
    "\n",
    "**Original DQN update:**\n",
    "```python\n",
    "next_q = target_network(next_state_batch).max(1)[0]\n",
    "```\n",
    "\n",
    "**Double DQN update:**\n",
    "```python\n",
    "# 1. Select action with online network\n",
    "next_actions = q_network(next_state_batch).argmax(1, keepdim=True)\n",
    "# 2. Evaluate with target network\n",
    "next_q = target_network(next_state_batch).gather(1, next_actions).squeeze()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Double DQN Agent\n",
    "\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Double DQN Agent - Reduces overestimation of Q-values.\n",
    "    \n",
    "    Key difference: Use online network for action selection,\n",
    "    target network for action evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Double DQN training step.\n",
    "        \n",
    "        The only difference from DQN is in the target computation:\n",
    "        - DQN: target = r + γ * max_a' Q_target(s', a')\n",
    "        - DoubleDQN: target = r + γ * Q_target(s', argmax_a' Q_online(s', a'))\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # DOUBLE DQN TARGET COMPUTATION\n",
    "        with torch.no_grad():\n",
    "            # YOUR CODE STARTS HERE\n",
    "            # Step 1: Select action using ONLINE network\n",
    "            next_actions = self.q_network(next_state_batch).argmax(1, keepdim=True)\n",
    "            \n",
    "            # Step 2: Evaluate action using TARGET network\n",
    "            next_q = self.target_network(next_state_batch).gather(1, next_actions).squeeze()\n",
    "            \n",
    "            # Compute target\n",
    "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q\n",
    "            # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Loss\n",
    "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# Test Exercise 4\n",
    "print(\"Testing Exercise 4: Double DQN Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_double_dqn_update(DoubleDQNAgent, state_dim=4, action_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- **Overestimation bias**: Taking max of noisy estimates biases upward\n",
    "- **Decouple selection and evaluation**: Use two networks for different roles\n",
    "- **Minimal code change**: Double DQN requires only 1-2 line modifications\n",
    "- **Significant improvement**: Better stability with negligible computational cost\n",
    "- **Principle**: When in doubt, decouple! It's a general principle in RL\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8. Dueling DQN: Architectural Improvements\n",
    "\n",
    "<a name='8-1'></a>\n",
    "### 8.1 Value-Advantage Decomposition\n",
    "\n",
    "**Key Insight (Wang et al., 2016):**\n",
    "\n",
    "Decompose Q-function into:\n",
    "1. **Value function V(s)**: How good is state s? (independent of action)\n",
    "2. **Advantage function A(s,a)**: How much better is action a vs others?\n",
    "\n",
    "$$Q(s,a) = V(s) + A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a')$$\n",
    "\n",
    "**Why This Helps:**\n",
    "- Better learning of state values when actions have similar effects\n",
    "- Improved generalization in large action spaces\n",
    "- Clearer signal: V learns \"which states are good\", A learns \"which actions matter\"\n",
    "\n",
    "**Example:**\n",
    "- State: \"Enemy far away\"\n",
    "  - V(s) = 0.8 (good state)\n",
    "  - A(s, move_left) = 0.1 (slightly better)\n",
    "  - A(s, move_right) = 0.1 (slightly better)\n",
    "  - A(s, attack) = -0.2 (worse, enemy too far)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (state) → Shared Features (hidden) → Value Stream → V(s)\n",
    "                                        ↓\n",
    "                                    Advantage Stream → A(s,a)\n",
    "                                        ↓\n",
    "                                    Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - implement_dueling_dqn\n",
    "\n",
    "**Instructions:** Implement a Dueling DQN network with:\n",
    "1. **Shared feature extraction layer**\n",
    "2. **Separate Value stream**: outputs single value V(s)\n",
    "3. **Separate Advantage stream**: outputs A(s,a) for each action\n",
    "4. **Aggregation layer**: Combine V and A via the formula above\n",
    "5. **Optional method**: `get_value_and_advantage()` for analysis\n",
    "\n",
    "**Key Implementation Details:**\n",
    "- Value stream should output shape (batch, 1)\n",
    "- Advantage stream should output shape (batch, action_dim)\n",
    "- Subtract mean of advantages for numerical stability\n",
    "- Use keepdim=True in mean() to preserve dimensions for broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Dueling DQN Network\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN Network Architecture.\n",
    "    \n",
    "    Separates Q-value computation into:\n",
    "    - Value stream V(s): scalar value of state\n",
    "    - Advantage stream A(s,a): advantage of each action\n",
    "    \n",
    "    Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Shared feature extraction layer\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream: outputs single value V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream: outputs A(s,a) for each action\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass computing Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): State tensor (batch_size, state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Feature extraction\n",
    "        features = self.feature(x)\n",
    "        \n",
    "        # Value stream\n",
    "        value = self.value_stream(features)  # (batch, 1)\n",
    "        \n",
    "        # Advantage stream\n",
    "        advantage = self.advantage_stream(features)  # (batch, action_dim)\n",
    "        \n",
    "        # Aggregation: Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        \n",
    "        return q_values\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def get_value_and_advantage(self, x):\n",
    "        \"\"\"\n",
    "        Get Value and Advantage separately (for analysis/debugging).\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): State tensor\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (value, advantage) tensors\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        features = self.feature(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        return value, advantage\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "# Test Exercise 5\n",
    "print(\"Testing Exercise 5: Dueling DQN Architecture\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_dueling_dqn_architecture(DuelingDQN, state_dim=4, action_dim=2, hidden_dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- **Dueling architecture**: Separates state value from action advantages\n",
    "- **Value function V(s)**: Depends only on state, represents intrinsic state quality\n",
    "- **Advantage function A(s,a)**: Depends on both state and action\n",
    "- **Aggregation formula**: Subtract mean of advantages for numerical stability\n",
    "- **When to use**: Large action spaces or when actions have redundant effects\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9. Algorithm Comparison\n",
    "\n",
    "<a name='9-1'></a>\n",
    "### 9.1 Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison Table\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%;\">\n",
    "  <tr style=\"background-color: #f2f2f2;\">\n",
    "    <th style=\"border: 1px solid black; padding: 10px; text-align: left;\"><b>Aspect</b></th>\n",
    "    <th style=\"border: 1px solid black; padding: 10px; text-align: left;\"><b>DQN</b></th>\n",
    "    <th style=\"border: 1px solid black; padding: 10px; text-align: left;\"><b>Double DQN</b></th>\n",
    "    <th style=\"border: 1px solid black; padding: 10px; text-align: left;\"><b>Dueling DQN</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Update Equation</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">r + γ max_a' Q_t(s',a')</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">r + γ Q_t(s', argmax Q(s',a'))</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">V(s) + (A(s,a) - mean A)</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Main Problem Addressed</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Correlation in RL data</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Overestimation of Q-values</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Learning efficiency</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Network Complexity</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Simple sequential</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Simple sequential</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Dual stream</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Code Complexity</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Medium</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Very Low (1 line change)</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Medium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Convergence Speed</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Good</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Better</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Best (large actions)</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Memory Overhead</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Baseline</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Baseline</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">+1 extra stream</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Stability</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Good</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Excellent</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Excellent</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Best Use Cases</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Baseline, learning</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">General purpose</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Large action spaces</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\"><b>Paper Citation</b></td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Mnih et al. (2015)</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">van Hasselt et al. (2015)</td>\n",
    "    <td style=\"border: 1px solid black; padding: 10px;\">Wang et al. (2016)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9-2'></a>\n",
    "### 9.2 Implementation Differences\n",
    "\n",
    "#### Key Code Differences\n",
    "\n",
    "**DQN - Target Computation:**\n",
    "```python\n",
    "next_q = target_network(next_state_batch).max(1)[0]\n",
    "```\n",
    "\n",
    "**Double DQN - Target Computation:**\n",
    "```python\n",
    "next_actions = q_network(next_state_batch).argmax(1, keepdim=True)\n",
    "next_q = target_network(next_state_batch).gather(1, next_actions).squeeze()\n",
    "```\n",
    "\n",
    "**Dueling DQN - Architecture:**\n",
    "```python\n",
    "class DuelingDQN(nn.Module):\n",
    "    def forward(self, x):\n",
    "        features = self.feature(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **DQN Fundamentals**\n",
    "   - Function approximation with neural networks\n",
    "   - Experience replay for decorrelation\n",
    "   - Target network for stability\n",
    "\n",
    "2. **Double DQN**\n",
    "   - Addresses overestimation through action-evaluation decoupling\n",
    "   - Minimal code change with significant benefits\n",
    "\n",
    "3. **Dueling DQN**\n",
    "   - Architectural innovation: Value + Advantage streams\n",
    "   - Better learning in complex environments\n",
    "\n",
    "### Key Implementation Skills\n",
    "\n",
    "- Building neural networks with PyTorch\n",
    "- Experience replay buffer management\n",
    "- Bellman equation in practice\n",
    "- ε-greedy exploration strategies\n",
    "- Gradient computation and optimization\n",
    "\n",
    "### Recommended Extensions\n",
    "\n",
    "1. **Prioritized Experience Replay (PER)**\n",
    "   - Sample important transitions more frequently\n",
    "\n",
    "2. **Soft Target Updates**\n",
    "   - Smooth target network updates instead of hard copies\n",
    "\n",
    "3. **Noisy Networks**\n",
    "   - Parametric exploration for better coordination\n",
    "\n",
    "4. **Rainbow DQN**\n",
    "   - Combines DQN, Double DQN, Dueling DQN, PER, and more\n",
    "\n",
    "5. **Policy Gradient Methods**\n",
    "   - Actor-Critic, PPO, TRPO\n",
    "   - Better for continuous control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have successfully completed the Deep Q-Network tutorial. You can now:\n",
    "\n",
    "✓ Implement DQN agents from scratch  \n",
    "✓ Understand and apply experience replay  \n",
    "✓ Work with target networks for stability  \n",
    "✓ Reduce overestimation with Double DQN  \n",
    "✓ Leverage architectural innovations with Dueling DQN  \n",
    "✓ Compare and select appropriate algorithms  \n",
    "\n",
    "**Great job on your Deep Reinforcement Learning journey!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
