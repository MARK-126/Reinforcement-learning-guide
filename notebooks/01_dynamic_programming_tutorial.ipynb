{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programaci√≥n Din√°mica en Reinforcement Learning\n",
    "\n",
    "**Tutorial Completo: De la Teor√≠a a la Pr√°ctica**\n",
    "\n",
    "---\n",
    "\n",
    "## √çndice\n",
    "\n",
    "1. [Introducci√≥n Te√≥rica](#1-introducci√≥n-te√≥rica)\n",
    "2. [Fundamentos Matem√°ticos](#2-fundamentos-matem√°ticos)\n",
    "3. [Explicaciones Intuitivas](#3-explicaciones-intuitivas)\n",
    "4. [Policy Iteration](#4-policy-iteration)\n",
    "5. [Value Iteration](#5-value-iteration)\n",
    "6. [Comparaci√≥n de Algoritmos](#6-comparaci√≥n-de-algoritmos)\n",
    "7. [Experimentos y Casos de Prueba](#7-experimentos-y-casos-de-prueba)\n",
    "8. [Visualizaciones Avanzadas](#8-visualizaciones-avanzadas)\n",
    "9. [Ejercicios Pr√°cticos](#9-ejercicios-pr√°cticos)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n Te√≥rica\n",
    "\n",
    "### ¬øQu√© es Programaci√≥n Din√°mica?\n",
    "\n",
    "La **Programaci√≥n Din√°mica (DP)** es una familia de algoritmos que pueden utilizarse para calcular pol√≠ticas √≥ptimas dado un modelo perfecto del entorno como un **Proceso de Decisi√≥n de Markov (MDP)**.\n",
    "\n",
    "### Caracter√≠sticas Clave:\n",
    "\n",
    "1. **Requiere modelo completo**: Necesitamos conocer las probabilidades de transici√≥n $p(s',r|s,a)$\n",
    "2. **Resuelve el problema de forma exacta**: Encuentra la pol√≠tica √≥ptima (no aproximada)\n",
    "3. **Computacionalmente costosa**: Para espacios de estados grandes\n",
    "4. **Base te√≥rica**: Fundamento para m√©todos model-free (Q-learning, SARSA, etc.)\n",
    "\n",
    "### Aplicaciones:\n",
    "\n",
    "- Problemas de navegaci√≥n (GridWorld, laberintos)\n",
    "- Gesti√≥n de inventario\n",
    "- Control de procesos industriales\n",
    "- Juegos con estados discretos\n",
    "- Planificaci√≥n de trayectorias en rob√≥tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fundamentos Matem√°ticos\n",
    "\n",
    "### 2.1 Procesos de Decisi√≥n de Markov (MDP)\n",
    "\n",
    "Un MDP se define por la tupla $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "- $\\mathcal{S}$: Conjunto de estados\n",
    "- $\\mathcal{A}$: Conjunto de acciones\n",
    "- $P$: Funci√≥n de transici√≥n $P(s'|s,a)$\n",
    "- $R$: Funci√≥n de recompensa $R(s,a,s')$\n",
    "- $\\gamma \\in [0,1]$: Factor de descuento\n",
    "\n",
    "### 2.2 Funciones de Valor\n",
    "\n",
    "#### Funci√≥n de Valor Estado $V^\\pi(s)$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s\\right]$$\n",
    "\n",
    "Es el retorno esperado comenzando desde el estado $s$ y siguiendo la pol√≠tica $\\pi$.\n",
    "\n",
    "#### Funci√≥n de Valor Acci√≥n $Q^\\pi(s,a)$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a\\right]$$\n",
    "\n",
    "Es el retorno esperado tomando la acci√≥n $a$ en el estado $s$ y luego siguiendo $\\pi$.\n",
    "\n",
    "### 2.3 Ecuaciones de Bellman\n",
    "\n",
    "#### Ecuaci√≥n de Bellman para $V^\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "Esta ecuaci√≥n expresa la relaci√≥n recursiva entre el valor de un estado y los valores de sus sucesores.\n",
    "\n",
    "#### Ecuaci√≥n de Optimalidad de Bellman para $V^*$:\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$$\n",
    "\n",
    "#### Ecuaci√≥n de Optimalidad de Bellman para $Q^*$:\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \\max_{a'} Q^*(s',a')\\right]$$\n",
    "\n",
    "### 2.4 Principio de Optimalidad de Bellman\n",
    "\n",
    "> **Teorema**: Una pol√≠tica $\\pi$ es √≥ptima si y solo si, para todos los estados $s$:\n",
    "> $$\\pi(s) = \\arg\\max_a Q^*(s,a)$$\n",
    "\n",
    "Este principio garantiza que existe al menos una pol√≠tica √≥ptima determinista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explicaciones Intuitivas\n",
    "\n",
    "### 3.1 Analog√≠a: El Viajero en una Ciudad\n",
    "\n",
    "Imagina que eres un viajero en una ciudad desconocida:\n",
    "\n",
    "- **Estados ($s$)**: Tu ubicaci√≥n actual (intersecciones)\n",
    "- **Acciones ($a$)**: Direcciones que puedes tomar (norte, sur, este, oeste)\n",
    "- **Recompensas ($r$)**: Tiempo que tardas en moverte (negativo = costo)\n",
    "- **Objetivo**: Llegar al hotel lo m√°s r√°pido posible\n",
    "\n",
    "**Policy Iteration** es como:\n",
    "1. Tener un plan inicial (pol√≠tica)\n",
    "2. Evaluar cu√°nto tiempo tomar√≠a seguir ese plan desde cada ubicaci√≥n\n",
    "3. Mejorar el plan eligiendo mejores direcciones\n",
    "4. Repetir hasta que el plan no pueda mejorar\n",
    "\n",
    "**Value Iteration** es como:\n",
    "1. Marcar cada intersecci√≥n con \"tiempo estimado al hotel\"\n",
    "2. Actualizar estas estimaciones mirando las intersecciones vecinas\n",
    "3. Repetir hasta que las estimaciones se estabilicen\n",
    "4. Al final, elegir la direcci√≥n que lleva a la intersecci√≥n con menor tiempo\n",
    "\n",
    "### 3.2 Visualizaci√≥n del GridWorld\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  S  ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ  S = Estado inicial\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  G = Meta (Goal)\n",
    "‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ  # = Obst√°culo\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ     ‚îÇ  #  ‚îÇ     ‚îÇ     ‚îÇ  \n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ  G  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "En este ejemplo:\n",
    "- El agente comienza en la esquina superior izquierda\n",
    "- Debe llegar a la esquina inferior derecha\n",
    "- Cada movimiento tiene un peque√±o costo (-0.01)\n",
    "- Alcanzar la meta da recompensa +1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Policy Iteration\n",
    "\n",
    "### 4.1 Algoritmo\n",
    "\n",
    "Policy Iteration alterna entre dos pasos:\n",
    "\n",
    "**1. Policy Evaluation (Evaluaci√≥n de Pol√≠tica)**\n",
    "```\n",
    "Repetir hasta convergencia:\n",
    "    Para cada estado s:\n",
    "        V(s) ‚Üê Œ£_{s',r} p(s',r|s,œÄ(s))[r + Œ≥V(s')]\n",
    "```\n",
    "\n",
    "**2. Policy Improvement (Mejora de Pol√≠tica)**\n",
    "```\n",
    "Para cada estado s:\n",
    "    œÄ'(s) ‚Üê argmax_a Œ£_{s',r} p(s',r|s,a)[r + Œ≥V(s')]\n",
    "```\n",
    "\n",
    "**3. Criterio de Parada**\n",
    "```\n",
    "Si œÄ' = œÄ, entonces STOP (pol√≠tica √≥ptima encontrada)\n",
    "Sino, œÄ ‚Üê œÄ' y volver a paso 1\n",
    "```\n",
    "\n",
    "### 4.2 Implementaci√≥n Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuraci√≥n inicial\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.colors import LinearSegmentedColormap\nimport pandas as pd\n\n# Agregar el path del repositorio para importar las implementaciones\nrepo_path = '/home/user/Reinforcement-learning-guide'\nif repo_path not in sys.path:\n    sys.path.insert(0, repo_path)\n\n# Importar las implementaciones de Dynamic Programming\n# Nota: Los m√≥dulos est√°n en 02_algoritmos_clasicos/dynamic_programming/\nimport importlib.util\n\n# Cargar policy_iteration.py\npi_spec = importlib.util.spec_from_file_location(\n    \"policy_iteration\", \n    os.path.join(repo_path, \"02_algoritmos_clasicos/dynamic_programming/policy_iteration.py\")\n)\npi_module = importlib.util.module_from_spec(pi_spec)\npi_spec.loader.exec_module(pi_module)\n\n# Cargar value_iteration.py\nvi_spec = importlib.util.spec_from_file_location(\n    \"value_iteration\",\n    os.path.join(repo_path, \"02_algoritmos_clasicos/dynamic_programming/value_iteration.py\")\n)\nvi_module = importlib.util.module_from_spec(vi_spec)\nvi_spec.loader.exec_module(vi_module)\n\n# Importar las clases y funciones\nPolicyIteration = pi_module.PolicyIteration\ncreate_gridworld_mdp = pi_module.create_gridworld_mdp\nValueIteration = vi_module.ValueIteration\n\n# Configuraci√≥n de visualizaci√≥n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"‚úì M√≥dulos importados correctamente\")\nprint(f\"  - PolicyIteration desde: 02_algoritmos_clasicos/dynamic_programming/policy_iteration.py\")\nprint(f\"  - ValueIteration desde: 02_algoritmos_clasicos/dynamic_programming/value_iteration.py\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Crear el Entorno GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear GridWorld 4x4\n",
    "print(\"Creando entorno GridWorld 4x4...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "transition_probs, rewards, n_states, n_actions = create_gridworld_mdp(\n",
    "    grid_size=4,\n",
    "    goal_reward=1.0,\n",
    "    step_reward=-0.01\n",
    ")\n",
    "\n",
    "print(f\"‚úì Estados: {n_states}\")\n",
    "print(f\"‚úì Acciones: {n_actions} (0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê)\")\n",
    "print(f\"‚úì Estado inicial: 0 (esquina superior izquierda)\")\n",
    "print(f\"‚úì Estado objetivo: {n_states-1} (esquina inferior derecha)\")\n",
    "print(f\"\\nDimensiones de las matrices:\")\n",
    "print(f\"  - Transition probabilities: {transition_probs.shape}\")\n",
    "print(f\"  - Rewards: {rewards.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Ejecutar Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EJECUTANDO POLICY ITERATION\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Crear solver\n",
    "pi_solver = PolicyIteration(\n",
    "    n_states=n_states,\n",
    "    n_actions=n_actions,\n",
    "    gamma=0.99,\n",
    "    theta=1e-6\n",
    ")\n",
    "\n",
    "# Resolver\n",
    "pi_results = pi_solver.solve(\n",
    "    transition_probs=transition_probs,\n",
    "    rewards=rewards,\n",
    "    max_iterations=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualizar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy_and_values(policy, values, grid_size=4, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualiza la pol√≠tica y funci√≥n de valor en un GridWorld.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # S√≠mbolos de acciones\n",
    "    action_symbols = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "    \n",
    "    # Crear grids para visualizaci√≥n\n",
    "    policy_grid = policy.reshape(grid_size, grid_size)\n",
    "    value_grid = values.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # 1. Visualizar Pol√≠tica\n",
    "    ax1.set_title(f'Pol√≠tica √ìptima {title}', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlim(-0.5, grid_size - 0.5)\n",
    "    ax1.set_ylim(-0.5, grid_size - 0.5)\n",
    "    ax1.set_xticks(range(grid_size))\n",
    "    ax1.set_yticks(range(grid_size))\n",
    "    ax1.grid(True, linewidth=2)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            action = policy_grid[i, j]\n",
    "            ax1.text(j, i, action_symbols[action], \n",
    "                    ha='center', va='center', fontsize=24,\n",
    "                    color='blue' if (i == grid_size-1 and j == grid_size-1) else 'black')\n",
    "    \n",
    "    # Marcar inicio y meta\n",
    "    ax1.text(0, -0.8, 'INICIO', ha='center', fontsize=10, color='green', fontweight='bold')\n",
    "    ax1.text(grid_size-1, grid_size-0.2, 'META', ha='center', fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    # 2. Visualizar Valores\n",
    "    im = ax2.imshow(value_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    ax2.set_title(f'Funci√≥n de Valor {title}', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(range(grid_size))\n",
    "    ax2.set_yticks(range(grid_size))\n",
    "    \n",
    "    # A√±adir valores num√©ricos\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            text = ax2.text(j, i, f'{value_grid[i, j]:.3f}',\n",
    "                          ha='center', va='center', color='black', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2, label='Valor del Estado')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar resultados de Policy Iteration\n",
    "visualize_policy_and_values(\n",
    "    pi_results['policy'], \n",
    "    pi_results['V'], \n",
    "    grid_size=4,\n",
    "    title=\"(Policy Iteration)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 An√°lisis de Convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de cambios en la pol√≠tica\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pi_results['history']['policy_changes'], 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Iteraci√≥n', fontsize=12)\n",
    "plt.ylabel('N√∫mero de Estados que Cambiaron', fontsize=12)\n",
    "plt.title('Convergencia de Policy Iteration', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas de Policy Iteration:\")\n",
    "print(f\"   - Iteraciones totales: {pi_results['iterations']}\")\n",
    "print(f\"   - Valor medio final: {np.mean(pi_results['V']):.4f}\")\n",
    "print(f\"   - Valor m√°ximo: {np.max(pi_results['V']):.4f}\")\n",
    "print(f\"   - Valor m√≠nimo: {np.min(pi_results['V']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Value Iteration\n",
    "\n",
    "### 5.1 Algoritmo\n",
    "\n",
    "Value Iteration combina policy evaluation y policy improvement en un solo paso:\n",
    "\n",
    "**Algoritmo:**\n",
    "```\n",
    "Inicializar V(s) = 0 para todo s\n",
    "\n",
    "Repetir hasta convergencia:\n",
    "    Para cada estado s:\n",
    "        V(s) ‚Üê max_a Œ£_{s',r} p(s',r|s,a)[r + Œ≥V(s')]\n",
    "\n",
    "Extraer pol√≠tica:\n",
    "    œÄ(s) = argmax_a Œ£_{s',r} p(s',r|s,a)[r + Œ≥V(s')]\n",
    "```\n",
    "\n",
    "**Diferencias clave con Policy Iteration:**\n",
    "- No mantiene una pol√≠tica expl√≠cita durante el proceso\n",
    "- Hace actualizaciones m√°s simples pero m√°s frecuentes\n",
    "- Generalmente m√°s r√°pido para problemas grandes\n",
    "- La pol√≠tica solo se extrae al final\n",
    "\n",
    "### 5.2 Implementaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EJECUTANDO VALUE ITERATION\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Crear solver\n",
    "vi_solver = ValueIteration(\n",
    "    n_states=n_states,\n",
    "    n_actions=n_actions,\n",
    "    gamma=0.99,\n",
    "    theta=1e-6\n",
    ")\n",
    "\n",
    "# Resolver\n",
    "vi_results = vi_solver.solve(\n",
    "    transition_probs=transition_probs,\n",
    "    rewards=rewards,\n",
    "    max_iterations=1000,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualizar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de Value Iteration\n",
    "visualize_policy_and_values(\n",
    "    vi_results['policy'], \n",
    "    vi_results['V'], \n",
    "    grid_size=4,\n",
    "    title=\"(Value Iteration)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 An√°lisis de Convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficos de convergencia\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Delta m√°ximo\n",
    "ax1.plot(vi_results['history']['max_deltas'], linewidth=2)\n",
    "ax1.set_xlabel('Iteraci√≥n', fontsize=12)\n",
    "ax1.set_ylabel('Delta M√°ximo', fontsize=12)\n",
    "ax1.set_title('Convergencia de Value Iteration (Delta)', fontsize=14, fontweight='bold')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Valor medio\n",
    "ax2.plot(vi_results['history']['mean_values'], linewidth=2, color='green')\n",
    "ax2.set_xlabel('Iteraci√≥n', fontsize=12)\n",
    "ax2.set_ylabel('Valor Medio', fontsize=12)\n",
    "ax2.set_title('Evoluci√≥n del Valor Medio', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas de Value Iteration:\")\n",
    "print(f\"   - Iteraciones totales: {vi_results['iterations']}\")\n",
    "print(f\"   - Delta final: {vi_results['history']['max_deltas'][-1]:.8f}\")\n",
    "print(f\"   - Valor medio final: {np.mean(vi_results['V']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaci√≥n de Algoritmos\n",
    "\n",
    "### 6.1 Comparaci√≥n Te√≥rica\n",
    "\n",
    "| Aspecto | Policy Iteration | Value Iteration |\n",
    "|---------|-----------------|----------------|\n",
    "| **Convergencia** | Menos iteraciones | M√°s iteraciones |\n",
    "| **Costo por iteraci√≥n** | Alto (policy evaluation completo) | Bajo (un solo sweep) |\n",
    "| **Tiempo total** | Variable | Generalmente m√°s r√°pido |\n",
    "| **Pol√≠tica durante proceso** | Siempre disponible | Solo al final |\n",
    "| **Complejidad** | O(mn¬≤ + n¬≤) por iteraci√≥n | O(mn¬≤) por iteraci√≥n |\n",
    "| **Mejor para** | Espacios peque√±os, pol√≠ticas iniciales buenas | Espacios grandes, sin conocimiento previo |\n",
    "\n",
    "Donde: n = n√∫mero de estados, m = n√∫mero de acciones\n",
    "\n",
    "### 6.2 Comparaci√≥n Emp√≠rica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n lado a lado\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARACI√ìN: POLICY ITERATION vs VALUE ITERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = {\n",
    "    'M√©trica': [\n",
    "        'Iteraciones',\n",
    "        'Valor medio final',\n",
    "        'Diferencia m√°xima en V',\n",
    "        'Diferencia en pol√≠tica'\n",
    "    ],\n",
    "    'Policy Iteration': [\n",
    "        pi_results['iterations'],\n",
    "        f\"{np.mean(pi_results['V']):.6f}\",\n",
    "        '-',\n",
    "        '-'\n",
    "    ],\n",
    "    'Value Iteration': [\n",
    "        vi_results['iterations'],\n",
    "        f\"{np.mean(vi_results['V']):.6f}\",\n",
    "        f\"{np.max(np.abs(pi_results['V'] - vi_results['V'])):.8f}\",\n",
    "        f\"{np.sum(pi_results['policy'] != vi_results['policy'])} estados\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úì Ambos algoritmos convergen a la misma soluci√≥n √≥ptima\")\n",
    "print(f\"‚úì Policy Iteration: {pi_results['iterations']} iteraciones\")\n",
    "print(f\"‚úì Value Iteration: {vi_results['iterations']} iteraciones\")\n",
    "print(f\"‚úì Diferencia m√°xima en valores: {np.max(np.abs(pi_results['V'] - vi_results['V'])):.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Visualizaci√≥n Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de funciones de valor\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Policy Iteration\n",
    "im1 = axes[0].imshow(pi_results['V'].reshape(4, 4), cmap='RdYlGn', interpolation='nearest')\n",
    "axes[0].set_title('Policy Iteration', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Value Iteration\n",
    "im2 = axes[1].imshow(vi_results['V'].reshape(4, 4), cmap='RdYlGn', interpolation='nearest')\n",
    "axes[1].set_title('Value Iteration', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Diferencia\n",
    "diff = np.abs(pi_results['V'] - vi_results['V']).reshape(4, 4)\n",
    "im3 = axes[2].imshow(diff, cmap='Reds', interpolation='nearest')\n",
    "axes[2].set_title('Diferencia Absoluta', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experimentos y Casos de Prueba\n",
    "\n",
    "### 7.1 Experimento 1: Efecto del Factor de Descuento (Œ≥)\n",
    "\n",
    "El factor de descuento $\\gamma$ controla cu√°nto valora el agente las recompensas futuras:\n",
    "- $\\gamma \\approx 0$: Agente miope (solo recompensas inmediatas)\n",
    "- $\\gamma \\approx 1$: Agente previsor (valora mucho el futuro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENTO 1: Efecto del Factor de Descuento (Œ≥)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "gammas = [0.5, 0.9, 0.99, 0.999]\n",
    "results_gamma = {}\n",
    "\n",
    "for gamma in gammas:\n",
    "    print(f\"\\nProbando Œ≥ = {gamma}...\")\n",
    "    solver = ValueIteration(\n",
    "        n_states=n_states,\n",
    "        n_actions=n_actions,\n",
    "        gamma=gamma,\n",
    "        theta=1e-6\n",
    "    )\n",
    "    result = solver.solve(transition_probs, rewards, verbose=False)\n",
    "    results_gamma[gamma] = result\n",
    "    print(f\"  ‚úì Iteraciones: {result['iterations']}, Valor medio: {np.mean(result['V']):.4f}\")\n",
    "\n",
    "# Visualizar efecto de gamma\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    value_grid = results_gamma[gamma]['V'].reshape(4, 4)\n",
    "    im = axes[idx].imshow(value_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    axes[idx].set_title(f'Œ≥ = {gamma} (iter={results_gamma[gamma][\"iterations\"]})', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # A√±adir valores\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[idx].text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                         ha='center', va='center', color='black', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "    axes[idx].set_xticks(range(4))\n",
    "    axes[idx].set_yticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Experimento 2: GridWorlds de Diferentes Tama√±os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTO 2: Escalabilidad con Tama√±o del Grid\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "grid_sizes = [3, 4, 5, 6]\n",
    "results_sizes = {'PI': [], 'VI': []}\n",
    "\n",
    "for size in grid_sizes:\n",
    "    print(f\"\\nGrid {size}x{size} ({size*size} estados):\")\n",
    "    \n",
    "    # Crear MDP\n",
    "    trans, rew, n_s, n_a = create_gridworld_mdp(grid_size=size)\n",
    "    \n",
    "    # Policy Iteration\n",
    "    pi = PolicyIteration(n_s, n_a, gamma=0.99, theta=1e-6)\n",
    "    pi_res = pi.solve(trans, rew, max_iterations=100)\n",
    "    results_sizes['PI'].append(pi_res['iterations'])\n",
    "    \n",
    "    # Value Iteration\n",
    "    vi = ValueIteration(n_s, n_a, gamma=0.99, theta=1e-6)\n",
    "    vi_res = vi.solve(trans, rew, verbose=False)\n",
    "    results_sizes['VI'].append(vi_res['iterations'])\n",
    "    \n",
    "    print(f\"  Policy Iteration: {pi_res['iterations']} iteraciones\")\n",
    "    print(f\"  Value Iteration:  {vi_res['iterations']} iteraciones\")\n",
    "\n",
    "# Gr√°fico de escalabilidad\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = [s*s for s in grid_sizes]  # N√∫mero de estados\n",
    "plt.plot(x, results_sizes['PI'], 'o-', linewidth=2, markersize=10, label='Policy Iteration')\n",
    "plt.plot(x, results_sizes['VI'], 's-', linewidth=2, markersize=10, label='Value Iteration')\n",
    "plt.xlabel('N√∫mero de Estados', fontsize=12)\n",
    "plt.ylabel('Iteraciones hasta Convergencia', fontsize=12)\n",
    "plt.title('Escalabilidad de los Algoritmos', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Experimento 3: GridWorld con Obst√°culos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridworld_with_obstacles(grid_size=5, obstacles=None):\n",
    "    \"\"\"\n",
    "    Crea un GridWorld con obst√°culos.\n",
    "    \n",
    "    obstacles: lista de posiciones (row, col) de obst√°culos\n",
    "    \"\"\"\n",
    "    if obstacles is None:\n",
    "        obstacles = [(1, 1), (2, 2), (3, 1)]\n",
    "    \n",
    "    n_states = grid_size * grid_size\n",
    "    n_actions = 4\n",
    "    goal_state = n_states - 1\n",
    "    \n",
    "    transition_probs = np.zeros((n_states, n_actions, n_states))\n",
    "    rewards = np.zeros((n_states, n_actions, n_states))\n",
    "    \n",
    "    # Convertir obst√°culos a estados\n",
    "    obstacle_states = [r * grid_size + c for r, c in obstacles]\n",
    "    \n",
    "    def state_to_pos(state):\n",
    "        return state // grid_size, state % grid_size\n",
    "    \n",
    "    def pos_to_state(row, col):\n",
    "        return row * grid_size + col\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        # Estados objetivo y obst√°culos son absorbentes\n",
    "        if s == goal_state or s in obstacle_states:\n",
    "            for a in range(n_actions):\n",
    "                transition_probs[s, a, s] = 1.0\n",
    "                rewards[s, a, s] = -1.0 if s in obstacle_states else 0.0\n",
    "            continue\n",
    "        \n",
    "        row, col = state_to_pos(s)\n",
    "        \n",
    "        for a in range(n_actions):\n",
    "            new_row, new_col = row, col\n",
    "            \n",
    "            if a == 0:  # arriba\n",
    "                new_row = max(0, row - 1)\n",
    "            elif a == 1:  # derecha\n",
    "                new_col = min(grid_size - 1, col + 1)\n",
    "            elif a == 2:  # abajo\n",
    "                new_row = min(grid_size - 1, row + 1)\n",
    "            elif a == 3:  # izquierda\n",
    "                new_col = max(0, col - 1)\n",
    "            \n",
    "            next_state = pos_to_state(new_row, new_col)\n",
    "            \n",
    "            # Si el siguiente estado es un obst√°culo, quedarse en el lugar\n",
    "            if next_state in obstacle_states:\n",
    "                next_state = s\n",
    "                transition_probs[s, a, next_state] = 1.0\n",
    "                rewards[s, a, next_state] = -0.1  # Penalizaci√≥n por chocar\n",
    "            else:\n",
    "                transition_probs[s, a, next_state] = 1.0\n",
    "                if next_state == goal_state:\n",
    "                    rewards[s, a, next_state] = 1.0\n",
    "                else:\n",
    "                    rewards[s, a, next_state] = -0.01\n",
    "    \n",
    "    return transition_probs, rewards, n_states, n_actions, obstacle_states\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTO 3: GridWorld con Obst√°culos\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Crear grid con obst√°culos\n",
    "trans_obs, rew_obs, n_s_obs, n_a_obs, obstacles = create_gridworld_with_obstacles(\n",
    "    grid_size=5,\n",
    "    obstacles=[(1, 2), (2, 2), (3, 2)]  # Pared vertical\n",
    ")\n",
    "\n",
    "print(f\"Grid: 5x5 con {len(obstacles)} obst√°culos\")\n",
    "print(f\"Obst√°culos en estados: {obstacles}\\n\")\n",
    "\n",
    "# Resolver con Value Iteration\n",
    "vi_obs = ValueIteration(n_s_obs, n_a_obs, gamma=0.99, theta=1e-6)\n",
    "result_obs = vi_obs.solve(trans_obs, rew_obs, verbose=True)\n",
    "\n",
    "# Visualizar\n",
    "def visualize_gridworld_with_obstacles(policy, values, obstacles, grid_size=5):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    action_symbols = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "    policy_grid = policy.reshape(grid_size, grid_size)\n",
    "    value_grid = values.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # Pol√≠tica\n",
    "    ax1.set_title('Pol√≠tica √ìptima con Obst√°culos', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlim(-0.5, grid_size - 0.5)\n",
    "    ax1.set_ylim(-0.5, grid_size - 0.5)\n",
    "    ax1.set_xticks(range(grid_size))\n",
    "    ax1.set_yticks(range(grid_size))\n",
    "    ax1.grid(True, linewidth=2)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            state = i * grid_size + j\n",
    "            if state in obstacles:\n",
    "                # Dibujar obst√°culo\n",
    "                ax1.add_patch(plt.Rectangle((j-0.4, i-0.4), 0.8, 0.8, \n",
    "                                           fill=True, color='black', alpha=0.7))\n",
    "                ax1.text(j, i, '‚ñ†', ha='center', va='center', \n",
    "                        fontsize=20, color='red')\n",
    "            else:\n",
    "                action = policy_grid[i, j]\n",
    "                color = 'red' if state == grid_size*grid_size-1 else 'blue'\n",
    "                ax1.text(j, i, action_symbols[action], \n",
    "                        ha='center', va='center', fontsize=20, color=color)\n",
    "    \n",
    "    # Valores\n",
    "    # Marcar obst√°culos en el mapa de valores\n",
    "    value_grid_masked = value_grid.copy()\n",
    "    for obs in obstacles:\n",
    "        value_grid_masked[obs[0], obs[1]] = np.nan\n",
    "    \n",
    "    im = ax2.imshow(value_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    ax2.set_title('Funci√≥n de Valor', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(range(grid_size))\n",
    "    ax2.set_yticks(range(grid_size))\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            state = i * grid_size + j\n",
    "            if state in obstacles:\n",
    "                ax2.text(j, i, 'X', ha='center', va='center', \n",
    "                        color='red', fontsize=16, fontweight='bold')\n",
    "            else:\n",
    "                ax2.text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                        ha='center', va='center', color='black', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gridworld_with_obstacles(\n",
    "    result_obs['policy'], \n",
    "    result_obs['V'], \n",
    "    obstacles,\n",
    "    grid_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizaciones Avanzadas\n",
    "\n",
    "### 8.1 Mapa de Calor de Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values(solver, transition_probs, rewards, grid_size=4):\n",
    "    \"\"\"\n",
    "    Visualiza los Q-values para cada acci√≥n en cada estado.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    action_names = ['Arriba (‚Üë)', 'Derecha (‚Üí)', 'Abajo (‚Üì)', 'Izquierda (‚Üê)']\n",
    "    \n",
    "    for action in range(4):\n",
    "        ax = axes[action // 2, action % 2]\n",
    "        \n",
    "        # Calcular Q-values para esta acci√≥n en todos los estados\n",
    "        q_values = np.zeros(grid_size * grid_size)\n",
    "        for state in range(grid_size * grid_size):\n",
    "            q_values[state] = solver.get_q_value(\n",
    "                state, action, transition_probs, rewards\n",
    "            )\n",
    "        \n",
    "        # Reshape y visualizar\n",
    "        q_grid = q_values.reshape(grid_size, grid_size)\n",
    "        im = ax.imshow(q_grid, cmap='coolwarm', interpolation='nearest')\n",
    "        ax.set_title(f'Q-Values: {action_names[action]}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # A√±adir valores\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                ax.text(j, i, f'{q_grid[i, j]:.3f}',\n",
    "                       ha='center', va='center', color='white', fontsize=9)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax)\n",
    "        ax.set_xticks(range(grid_size))\n",
    "        ax.set_yticks(range(grid_size))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizando Q-Values para cada acci√≥n...\\n\")\n",
    "visualize_q_values(vi_solver, transition_probs, rewards, grid_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Animaci√≥n de Convergencia (Simulada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_convergence_snapshots():\n",
    "    \"\"\"\n",
    "    Muestra snapshots de la funci√≥n de valor durante la convergencia.\n",
    "    \"\"\"\n",
    "    print(\"Ejecutando Value Iteration con snapshots...\\n\")\n",
    "    \n",
    "    # Crear nuevo solver\n",
    "    solver = ValueIteration(n_states=16, n_actions=4, gamma=0.99, theta=1e-6)\n",
    "    \n",
    "    # Snapshots en iteraciones espec√≠ficas\n",
    "    snapshot_iters = [1, 3, 5, 10, 20, 50]\n",
    "    snapshots = []\n",
    "    \n",
    "    for iteration in range(max(snapshot_iters) + 1):\n",
    "        solver.value_update(transition_probs, rewards)\n",
    "        if iteration in snapshot_iters:\n",
    "            snapshots.append((iteration, solver.V.copy()))\n",
    "    \n",
    "    # Visualizar snapshots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (iter_num, V) in enumerate(snapshots):\n",
    "        value_grid = V.reshape(4, 4)\n",
    "        im = axes[idx].imshow(value_grid, cmap='RdYlGn', \n",
    "                             interpolation='nearest', vmin=-0.1, vmax=1.0)\n",
    "        axes[idx].set_title(f'Iteraci√≥n {iter_num}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # A√±adir valores\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                axes[idx].text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                             ha='center', va='center', color='black', fontsize=9)\n",
    "        \n",
    "        axes[idx].set_xticks(range(4))\n",
    "        axes[idx].set_yticks(range(4))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_convergence_snapshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Trayectorias √ìptimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trajectory(policy, start_state, goal_state, grid_size=4, max_steps=20):\n",
    "    \"\"\"\n",
    "    Simula una trayectoria siguiendo la pol√≠tica √≥ptima.\n",
    "    \"\"\"\n",
    "    trajectory = [start_state]\n",
    "    current_state = start_state\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        if current_state == goal_state:\n",
    "            break\n",
    "        \n",
    "        # Tomar acci√≥n seg√∫n pol√≠tica\n",
    "        action = policy[current_state]\n",
    "        \n",
    "        # Simular transici√≥n (determinista en nuestro GridWorld)\n",
    "        row, col = current_state // grid_size, current_state % grid_size\n",
    "        \n",
    "        if action == 0:  # arriba\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # derecha\n",
    "            col = min(grid_size - 1, col + 1)\n",
    "        elif action == 2:  # abajo\n",
    "            row = min(grid_size - 1, row + 1)\n",
    "        elif action == 3:  # izquierda\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        current_state = row * grid_size + col\n",
    "        trajectory.append(current_state)\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def visualize_trajectory(policy, trajectory, grid_size=4):\n",
    "    \"\"\"\n",
    "    Visualiza una trayectoria en el grid.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    action_symbols = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "    policy_grid = policy.reshape(grid_size, grid_size)\n",
    "    \n",
    "    ax.set_title('Trayectoria √ìptima', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(-0.5, grid_size - 0.5)\n",
    "    ax.set_ylim(-0.5, grid_size - 0.5)\n",
    "    ax.set_xticks(range(grid_size))\n",
    "    ax.set_yticks(range(grid_size))\n",
    "    ax.grid(True, linewidth=2)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Dibujar pol√≠tica\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            action = policy_grid[i, j]\n",
    "            ax.text(j, i, action_symbols[action], \n",
    "                   ha='center', va='center', fontsize=20, color='lightgray')\n",
    "    \n",
    "    # Dibujar trayectoria\n",
    "    for idx in range(len(trajectory) - 1):\n",
    "        s1 = trajectory[idx]\n",
    "        s2 = trajectory[idx + 1]\n",
    "        \n",
    "        r1, c1 = s1 // grid_size, s1 % grid_size\n",
    "        r2, c2 = s2 // grid_size, s2 % grid_size\n",
    "        \n",
    "        # Flecha\n",
    "        arrow = FancyArrowPatch(\n",
    "            (c1, r1), (c2, r2),\n",
    "            arrowstyle='->', mutation_scale=30, linewidth=3,\n",
    "            color='red', alpha=0.7\n",
    "        )\n",
    "        ax.add_patch(arrow)\n",
    "    \n",
    "    # Marcar inicio y fin\n",
    "    start = trajectory[0]\n",
    "    end = trajectory[-1]\n",
    "    r_start, c_start = start // grid_size, start % grid_size\n",
    "    r_end, c_end = end // grid_size, end % grid_size\n",
    "    \n",
    "    ax.plot(c_start, r_start, 'go', markersize=20, label='Inicio')\n",
    "    ax.plot(c_end, r_end, 'r*', markersize=25, label='Meta')\n",
    "    \n",
    "    ax.legend(loc='upper left', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Simular desde diferentes estados iniciales\n",
    "print(\"\\nSimulando trayectorias desde diferentes puntos de inicio...\\n\")\n",
    "\n",
    "start_states = [0, 4, 8]  # Diferentes puntos de inicio\n",
    "for start in start_states:\n",
    "    trajectory = simulate_trajectory(\n",
    "        vi_results['policy'], \n",
    "        start_state=start,\n",
    "        goal_state=15,\n",
    "        grid_size=4\n",
    "    )\n",
    "    print(f\"Inicio: Estado {start} ‚Üí Trayectoria: {trajectory}\")\n",
    "    print(f\"  Longitud: {len(trajectory)-1} pasos\\n\")\n",
    "\n",
    "# Visualizar una trayectoria\n",
    "trajectory_example = simulate_trajectory(vi_results['policy'], 0, 15, 4)\n",
    "visualize_trajectory(vi_results['policy'], trajectory_example, grid_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Modificar Recompensas\n",
    "\n",
    "**Objetivo**: Entender c√≥mo las recompensas afectan el comportamiento √≥ptimo.\n",
    "\n",
    "**Tarea**: \n",
    "1. Crea un nuevo GridWorld 4x4 con `step_reward = -0.1` (en lugar de -0.01)\n",
    "2. Resuelve usando Value Iteration\n",
    "3. Compara la pol√≠tica y trayectorias con el GridWorld original\n",
    "4. Explica las diferencias\n",
    "\n",
    "**Pregunta**: ¬øPor qu√© el agente toma rutas m√°s directas con step_reward m√°s negativo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 1: Escribe tu c√≥digo aqu√≠\n",
    "# Pista: Usa create_gridworld_mdp() con diferentes par√°metros\n",
    "\n",
    "# Tu c√≥digo aqu√≠...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: GridWorld Estoc√°stico\n",
    "\n",
    "**Objetivo**: Implementar un entorno con transiciones probabil√≠sticas.\n",
    "\n",
    "**Tarea**:\n",
    "1. Modifica la funci√≥n `create_gridworld_mdp` para que las acciones sean estoc√°sticas:\n",
    "   - 80% de probabilidad: moverse en la direcci√≥n deseada\n",
    "   - 10% de probabilidad: moverse perpendicular a la izquierda\n",
    "   - 10% de probabilidad: moverse perpendicular a la derecha\n",
    "2. Resuelve el MDP estoc√°stico\n",
    "3. Compara con el caso determinista\n",
    "\n",
    "**Pregunta**: ¬øC√≥mo cambia la pol√≠tica √≥ptima con incertidumbre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 2: Escribe tu c√≥digo aqu√≠\n",
    "def create_stochastic_gridworld(grid_size=4, p_correct=0.8):\n",
    "    \"\"\"\n",
    "    Crea un GridWorld con transiciones estoc√°sticas.\n",
    "    \n",
    "    Completa esta funci√≥n.\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠...\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Comparaci√≥n de Eficiencia\n",
    "\n",
    "**Objetivo**: Analizar cu√°ndo usar Policy Iteration vs Value Iteration.\n",
    "\n",
    "**Tarea**:\n",
    "1. Implementa una funci√≥n que mida el tiempo de ejecuci√≥n de ambos algoritmos\n",
    "2. Ejecuta experimentos con diferentes tama√±os de grid (3x3 hasta 10x10)\n",
    "3. Grafica tiempo de ejecuci√≥n vs tama√±o del problema\n",
    "4. Determina en qu√© casos cada algoritmo es m√°s eficiente\n",
    "\n",
    "**Pregunta**: ¬øCu√°l es el punto de cruce donde un algoritmo supera al otro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 3: Escribe tu c√≥digo aqu√≠\n",
    "import time\n",
    "\n",
    "def benchmark_algorithms(grid_sizes):\n",
    "    \"\"\"\n",
    "    Compara tiempos de ejecuci√≥n de PI y VI.\n",
    "    \n",
    "    Completa esta funci√≥n.\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠...\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Mundo con M√∫ltiples Metas\n",
    "\n",
    "**Objetivo**: Extender el framework a problemas m√°s complejos.\n",
    "\n",
    "**Tarea**:\n",
    "1. Crea un GridWorld 6x6 con dos metas:\n",
    "   - Meta 1: recompensa +1.0 en posici√≥n (2, 5)\n",
    "   - Meta 2: recompensa +0.5 en posici√≥n (4, 1)\n",
    "2. Resuelve usando Value Iteration\n",
    "3. Visualiza cu√°les estados prefieren qu√© meta\n",
    "\n",
    "**Pregunta**: ¬øC√≥mo decide el agente entre m√∫ltiples objetivos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 4: Escribe tu c√≥digo aqu√≠\n",
    "def create_multi_goal_gridworld(grid_size=6, goals=None):\n",
    "    \"\"\"\n",
    "    Crea un GridWorld con m√∫ltiples metas.\n",
    "    \n",
    "    goals: lista de tuplas ((row, col), reward)\n",
    "    \n",
    "    Completa esta funci√≥n.\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠...\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: An√°lisis de Sensibilidad\n",
    "\n",
    "**Objetivo**: Estudiar c√≥mo par√°metros afectan la convergencia.\n",
    "\n",
    "**Tarea**:\n",
    "1. Ejecuta Value Iteration con diferentes valores de `theta` (umbral de convergencia):\n",
    "   - theta = [1e-2, 1e-4, 1e-6, 1e-8, 1e-10]\n",
    "2. Para cada theta, registra:\n",
    "   - N√∫mero de iteraciones\n",
    "   - Precisi√≥n de la pol√≠tica (comparando con theta muy peque√±o)\n",
    "3. Grafica trade-off entre precisi√≥n y tiempo de c√≥mputo\n",
    "\n",
    "**Pregunta**: ¬øCu√°l es el theta √≥ptimo para uso pr√°ctico?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 5: Escribe tu c√≥digo aqu√≠\n",
    "thetas = [1e-2, 1e-4, 1e-6, 1e-8, 1e-10]\n",
    "\n",
    "# Tu c√≥digo aqu√≠...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### Resumen de Conceptos Clave\n",
    "\n",
    "1. **Programaci√≥n Din√°mica** es la base te√≥rica de RL\n",
    "2. **Policy Iteration**: Converge en menos iteraciones pero cada iteraci√≥n es costosa\n",
    "3. **Value Iteration**: M√°s iteraciones pero cada una es r√°pida\n",
    "4. Ambos algoritmos **garantizan convergencia** a la pol√≠tica √≥ptima\n",
    "5. **Limitaciones**: Requieren modelo completo del entorno\n",
    "\n",
    "### Limitaciones de DP\n",
    "\n",
    "- **Curse of Dimensionality**: Complejidad $O(|S|^2|A|)$\n",
    "- **Requiere modelo**: En la pr√°ctica, rara vez tenemos $p(s',r|s,a)$ exacto\n",
    "- **Solo estados discretos**: Dif√≠cil para espacios continuos\n",
    "\n",
    "### Pr√≥ximos Temas\n",
    "\n",
    "Para superar estas limitaciones, estudiaremos:\n",
    "\n",
    "1. **Monte Carlo Methods**: Aprender sin modelo usando experiencia\n",
    "2. **Temporal Difference Learning**: Combinar DP y MC (Q-Learning, SARSA)\n",
    "3. **Function Approximation**: Manejar espacios de estados grandes\n",
    "4. **Deep RL**: Usar redes neuronales (DQN, Policy Gradients)\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- Sutton & Barto (2018): *Reinforcement Learning: An Introduction* - Cap√≠tulo 4\n",
    "- Bellman, R. (1957): *Dynamic Programming*\n",
    "- Puterman, M. (1994): *Markov Decision Processes*\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Felicidades!** Has completado el tutorial de Programaci√≥n Din√°mica en Reinforcement Learning.\n",
    "\n",
    "Contin√∫a practicando con los ejercicios y experimentando con diferentes entornos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}