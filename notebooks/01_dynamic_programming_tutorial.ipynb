{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming in Reinforcement Learning\n",
    "\n",
    "Welcome to this comprehensive tutorial on Dynamic Programming (DP) methods in Reinforcement Learning! By the end of this assignment, you'll be able to:\n",
    "\n",
    "- Implement Policy Evaluation to compute state values under a given policy\n",
    "- Implement Policy Improvement to derive better policies from value functions\n",
    "- Apply Policy Iteration to find optimal policies\n",
    "- Implement Value Iteration for efficient policy optimization\n",
    "- Compare and contrast different DP algorithms\n",
    "- Understand the trade-offs between computational complexity and convergence speed\n",
    "\n",
    "Dynamic Programming provides exact solutions for Markov Decision Processes (MDPs) when we have complete knowledge of the environment dynamics. Think of it as finding the best route in a city where you know all the roads, traffic patterns, and destinations!\n",
    "\n",
    "<img src=\"https://via.placeholder.com/650x300.png?text=GridWorld+Environment\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u> <b>Figure 1</b> </u>: <b>GridWorld Navigation Problem</b><br> The agent must navigate from the start position to the goal, finding the optimal path that maximizes total reward. </center></caption>\n",
    "\n",
    "**Notation**: As usual, $\\frac{\\partial J}{\\partial a} = $ `da` for any variable `a`.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Theoretical Background](#2)\n",
    "    - [2.1 - What is Dynamic Programming?](#2-1)\n",
    "    - [2.2 - Markov Decision Processes (MDPs)](#2-2)\n",
    "    - [2.3 - Bellman Equations](#2-3)\n",
    "- [3 - Policy Iteration](#3)\n",
    "    - [3.1 - Policy Evaluation](#3-1)\n",
    "        - [Exercise 1 - policy_evaluation](#ex-1)\n",
    "    - [3.2 - Policy Improvement](#3-2)\n",
    "        - [Exercise 2 - policy_improvement](#ex-2)\n",
    "    - [3.3 - Running Policy Iteration](#3-3)\n",
    "- [4 - Value Iteration](#4)\n",
    "    - [4.1 - Value Iteration Step](#4-1)\n",
    "        - [Exercise 3 - value_iteration_step](#ex-3)\n",
    "    - [4.2 - Extract Policy from Values](#4-2)\n",
    "        - [Exercise 4 - extract_policy](#ex-4)\n",
    "    - [4.3 - Running Value Iteration](#4-3)\n",
    "- [5 - Algorithm Comparison](#5)\n",
    "    - [Exercise 5 - compare_algorithms](#ex-5)\n",
    "- [6 - Experiments and Analysis](#6)\n",
    "    - [6.1 - Effect of Discount Factor](#6-1)\n",
    "    - [6.2 - Scalability Analysis](#6-2)\n",
    "    - [6.3 - GridWorld with Obstacles](#6-3)\n",
    "- [7 - Advanced Visualizations](#7)\n",
    "    - [7.1 - Q-Value Heatmaps](#7-1)\n",
    "    - [7.2 - Convergence Animation](#7-2)\n",
    "    - [7.3 - Optimal Trajectories](#7-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Add repository path for imports\n",
    "repo_path = '/home/user/Reinforcement-learning-guide'\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "\n",
    "# Add notebooks directory for dp_utils\n",
    "notebooks_path = os.path.join(repo_path, 'notebooks')\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.insert(0, notebooks_path)\n",
    "\n",
    "# Import utility functions and tests\n",
    "from dp_utils import (\n",
    "    PolicyIteration, ValueIteration, create_gridworld_mdp,\n",
    "    visualize_policy_and_values, visualize_trajectory, simulate_trajectory,\n",
    "    policy_evaluation_test, policy_improvement_test, value_iteration_step_test,\n",
    "    extract_policy_test, compare_algorithms_test, gridworld_environment_test\n",
    ")\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'RdYlGn'\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Packages loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Theoretical Background\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - What is Dynamic Programming?\n",
    "\n",
    "**Dynamic Programming (DP)** is a family of algorithms that can compute optimal policies given a perfect model of the environment as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "1. **Requires complete model**: We need to know the transition probabilities $p(s',r|s,a)$\n",
    "2. **Provides exact solutions**: Finds the optimal policy (not an approximation)\n",
    "3. **Computationally expensive**: For large state spaces\n",
    "4. **Theoretical foundation**: Basis for model-free methods (Q-Learning, SARSA, etc.)\n",
    "\n",
    "**Real-World Applications:**\n",
    "\n",
    "- Robot path planning and navigation\n",
    "- Inventory management and supply chain optimization  \n",
    "- Industrial process control\n",
    "- Game playing with discrete states\n",
    "- Resource allocation problems\n",
    "\n",
    "<a name='2-2'></a>\n",
    "### 2.2 - Markov Decision Processes (MDPs)\n",
    "\n",
    "An MDP is defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "- $\\mathcal{S}$: Set of states\n",
    "- $\\mathcal{A}$: Set of actions\n",
    "- $P$: Transition function $P(s'|s,a)$\n",
    "- $R$: Reward function $R(s,a,s')$\n",
    "- $\\gamma \\in [0,1]$: Discount factor\n",
    "\n",
    "**Value Functions:**\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ is:\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s\\right]$$\n",
    "\n",
    "The **action-value function** $Q^\\pi(s,a)$ is:\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a\\right]$$\n",
    "\n",
    "<a name='2-3'></a>\n",
    "### 2.3 - Bellman Equations\n",
    "\n",
    "**Bellman Expectation Equation** for $V^\\pi$:\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "**Bellman Optimality Equation** for $V^*$:\n",
    "$$V^*(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$$\n",
    "\n",
    "**Bellman Optimality Equation** for $Q^*$:\n",
    "$$Q^*(s,a) = \\sum_{s',r} p(s',r|s,a)\\left[r + \\gamma \\max_{a'} Q^*(s',a')\\right]$$\n",
    "\n",
    "These equations express the recursive relationship between the value of a state and the values of its successor states - the foundation of Dynamic Programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Policy Iteration\n",
    "\n",
    "Policy Iteration is a two-step iterative algorithm:\n",
    "\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$\n",
    "2. **Policy Improvement**: Improve $\\pi$ using the computed values\n",
    "\n",
    "This process continues until the policy no longer changes, guaranteeing convergence to the optimal policy $\\pi^*$.\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Policy Evaluation\n",
    "\n",
    "Policy Evaluation computes the state-value function $V^\\pi$ for a given policy $\\pi$. We iteratively apply the Bellman expectation equation:\n",
    "\n",
    "$$V_{k+1}(s) = \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V_k(s')]$$\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "Initialize V(s) = 0 for all s\n",
    "Repeat until convergence:\n",
    "    For each state s:\n",
    "        V(s) ‚Üê Œ£_{s',r} p(s',r|s,œÄ(s))[r + Œ≥V(s')]\n",
    "```\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - policy_evaluation\n",
    "\n",
    "Implement the policy evaluation algorithm. For each state, compute the expected value by summing over all possible next states, weighted by their transition probabilities.\n",
    "\n",
    "**Instructions:**\n",
    "- Loop through all states\n",
    "- For each state, get the action from the policy\n",
    "- Sum over all next states: reward + discounted next state value\n",
    "- Multiply by transition probability\n",
    "- Check convergence using max absolute change (delta < theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: policy_evaluation\n",
    "\n",
    "def policy_evaluation(policy, transition_probs, rewards, gamma=0.99, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a policy by computing the state-value function.\n",
    "    \n",
    "    Arguments:\n",
    "    policy -- numpy array of shape (n_states,) containing action for each state\n",
    "    transition_probs -- numpy array of shape (n_states, n_actions, n_states) \n",
    "                        containing transition probabilities\n",
    "    rewards -- numpy array of shape (n_states, n_actions, n_states) containing rewards\n",
    "    gamma -- discount factor, scalar (default: 0.99)\n",
    "    theta -- convergence threshold, scalar (default: 1e-6)\n",
    "    max_iterations -- maximum number of iterations (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "    V -- numpy array of shape (n_states,) containing state values\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition_probs.shape[0]\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        # Loop through all states\n",
    "        for s in range(n_states):\n",
    "            # (approx. 5 lines)\n",
    "            # Get action from policy: action = ...\n",
    "            # Compute value: sum over next states\n",
    "            # value = 0\n",
    "            # for s_next in range(n_states):\n",
    "            #     value += transition_probs[s, action, s_next] * (rewards[s, action, s_next] + gamma * V[s_next])\n",
    "            # YOUR CODE STARTS HERE\n",
    "            \n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "            \n",
    "            V_new[s] = value\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # Check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test policy_evaluation\n",
    "print(\"Testing policy_evaluation...\\n\")\n",
    "\n",
    "# Create simple GridWorld\n",
    "transition_probs, rewards, n_states, n_actions = create_gridworld_mdp(grid_size=4)\n",
    "\n",
    "# Create random policy\n",
    "test_policy = np.random.randint(0, n_actions, size=n_states)\n",
    "\n",
    "# Evaluate policy\n",
    "V = policy_evaluation(test_policy, transition_probs, rewards, gamma=0.99)\n",
    "\n",
    "print(f\"State values (first 5): {V[:5]}\")\n",
    "print(f\"Value shape: {V.shape}\")\n",
    "print(f\"Max value: {np.max(V):.4f}\")\n",
    "print(f\"Min value: {np.min(V):.4f}\")\n",
    "\n",
    "# Run automated test\n",
    "policy_evaluation_test(policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Testing policy_evaluation...\n",
    "\n",
    "State values (first 5): [...]\n",
    "Value shape: (16,)\n",
    "Max value: ...\n",
    "Min value: ...\n",
    "‚úì All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Policy Improvement\n",
    "\n",
    "Given a value function $V^\\pi$, we can improve the policy by acting greedily:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "This greedy policy $\\pi'$ is guaranteed to be at least as good as $\\pi$, and strictly better unless $\\pi$ is already optimal.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - policy_improvement\n",
    "\n",
    "Implement policy improvement. For each state, try all possible actions and select the one with the highest expected value.\n",
    "\n",
    "**Instructions:**\n",
    "- For each state, initialize action_values array\n",
    "- For each action, compute expected value (similar to policy evaluation)\n",
    "- Select action with maximum value using np.argmax()\n",
    "- Return the improved policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: policy_improvement\n",
    "\n",
    "def policy_improvement(V, transition_probs, rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Improve a policy given state values by acting greedily.\n",
    "    \n",
    "    Arguments:\n",
    "    V -- numpy array of shape (n_states,) containing state values\n",
    "    transition_probs -- numpy array of shape (n_states, n_actions, n_states)\n",
    "    rewards -- numpy array of shape (n_states, n_actions, n_states)\n",
    "    gamma -- discount factor, scalar (default: 0.99)\n",
    "    \n",
    "    Returns:\n",
    "    policy -- numpy array of shape (n_states,) containing improved policy\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition_probs.shape[0]\n",
    "    n_actions = transition_probs.shape[1]\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        # (approx. 6 lines)\n",
    "        # Initialize action_values array\n",
    "        # action_values = np.zeros(n_actions)\n",
    "        # For each action:\n",
    "        #     for a in range(n_actions):\n",
    "        #         for s_next in range(n_states):\n",
    "        #             action_values[a] += ...\n",
    "        # Select best action: policy[s] = np.argmax(action_values)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test policy_improvement\n",
    "print(\"Testing policy_improvement...\\n\")\n",
    "\n",
    "# Use values from previous evaluation\n",
    "improved_policy = policy_improvement(V, transition_probs, rewards, gamma=0.99)\n",
    "\n",
    "print(f\"Improved policy (first 5 states): {improved_policy[:5]}\")\n",
    "print(f\"Policy shape: {improved_policy.shape}\")\n",
    "print(f\"Unique actions used: {np.unique(improved_policy)}\")\n",
    "\n",
    "# Run automated test\n",
    "policy_improvement_test(policy_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Testing policy_improvement...\n",
    "\n",
    "Improved policy (first 5 states): [...]\n",
    "Policy shape: (16,)\n",
    "Unique actions used: [...]\n",
    "‚úì All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Policy Evaluation computes state values for a given policy using the Bellman expectation equation\n",
    "- Policy Improvement creates a better policy by acting greedily with respect to the current value function\n",
    "- These two steps alternate in Policy Iteration until convergence\n",
    "- Convergence is guaranteed and the result is the optimal policy\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Running Policy Iteration\n",
    "\n",
    "Now let's put it all together and run the complete Policy Iteration algorithm on a GridWorld environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridWorld environment\n",
    "print(\"Creating GridWorld 4x4 environment...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "transition_probs, rewards, n_states, n_actions = create_gridworld_mdp(\n",
    "    grid_size=4,\n",
    "    goal_reward=1.0,\n",
    "    step_reward=-0.01\n",
    ")\n",
    "\n",
    "print(f\"‚úì States: {n_states}\")\n",
    "print(f\"‚úì Actions: {n_actions} (0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê)\")\n",
    "print(f\"‚úì Start: State 0 (top-left)\")\n",
    "print(f\"‚úì Goal: State {n_states-1} (bottom-right)\")\n",
    "\n",
    "# Test environment\n",
    "gridworld_environment_test(transition_probs, rewards, n_states, n_actions, expected_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Policy Iteration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING POLICY ITERATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "pi_solver = PolicyIteration(\n",
    "    n_states=n_states,\n",
    "    n_actions=n_actions,\n",
    "    gamma=0.99,\n",
    "    theta=1e-6\n",
    ")\n",
    "\n",
    "pi_results = pi_solver.solve(\n",
    "    transition_probs=transition_probs,\n",
    "    rewards=rewards,\n",
    "    max_iterations=100\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Converged in {pi_results['iterations']} iterations\")\n",
    "print(f\"‚úì Average state value: {np.mean(pi_results['V']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_policy_and_values(\n",
    "    pi_results['policy'],\n",
    "    pi_results['V'],\n",
    "    grid_size=4,\n",
    "    title=\"(Policy Iteration)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Value Iteration\n",
    "\n",
    "Value Iteration combines policy evaluation and improvement into a single update step. Instead of fully evaluating a policy, it performs one sweep of policy evaluation followed by policy improvement.\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "Initialize V(s) = 0 for all s\n",
    "Repeat until convergence:\n",
    "    For each state s:\n",
    "        V(s) ‚Üê max_a Œ£_{s',r} p(s',r|s,a)[r + Œ≥V(s')]\n",
    "```\n",
    "\n",
    "**Key Differences from Policy Iteration:**\n",
    "- Does not maintain explicit policy during iteration\n",
    "- Makes simpler but more frequent updates  \n",
    "- Generally faster for large problems\n",
    "- Policy is extracted only at the end\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Value Iteration Step\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - value_iteration_step\n",
    "\n",
    "Implement one step of value iteration. For each state, compute the maximum value over all actions.\n",
    "\n",
    "**Instructions:**\n",
    "- For each state, compute value for each action\n",
    "- Take the maximum over actions\n",
    "- This is similar to policy_improvement but we update V instead of extracting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: value_iteration_step\n",
    "\n",
    "def value_iteration_step(V, transition_probs, rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Perform one step of value iteration.\n",
    "    \n",
    "    Arguments:\n",
    "    V -- numpy array of shape (n_states,) containing current state values\n",
    "    transition_probs -- numpy array of shape (n_states, n_actions, n_states)\n",
    "    rewards -- numpy array of shape (n_states, n_actions, n_states)\n",
    "    gamma -- discount factor, scalar (default: 0.99)\n",
    "    \n",
    "    Returns:\n",
    "    V_new -- numpy array of shape (n_states,) containing updated state values\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition_probs.shape[0]\n",
    "    n_actions = transition_probs.shape[1]\n",
    "    V_new = np.zeros(n_states)\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        # (approx. 6 lines)\n",
    "        # Initialize action_values\n",
    "        # action_values = np.zeros(n_actions)\n",
    "        # For each action, compute expected value\n",
    "        # for a in range(n_actions):\n",
    "        #     for s_next in range(n_states):\n",
    "        #         action_values[a] += ...\n",
    "        # Take maximum: V_new[s] = np.max(action_values)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test value_iteration_step\n",
    "print(\"Testing value_iteration_step...\\n\")\n",
    "\n",
    "# Initialize V\n",
    "V_test = np.zeros(n_states)\n",
    "\n",
    "# Perform one step\n",
    "V_new = value_iteration_step(V_test, transition_probs, rewards, gamma=0.99)\n",
    "\n",
    "print(f\"Updated values (first 5): {V_new[:5]}\")\n",
    "print(f\"Max change: {np.max(np.abs(V_new - V_test)):.6f}\")\n",
    "\n",
    "# Run automated test\n",
    "value_iteration_step_test(value_iteration_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Testing value_iteration_step...\n",
    "\n",
    "Updated values (first 5): [...]\n",
    "Max change: ...\n",
    "‚úì All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Extract Policy from Values\n",
    "\n",
    "After value iteration converges, we need to extract the optimal policy from the value function.\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - extract_policy\n",
    "\n",
    "Extract the greedy policy from a value function. This is identical to policy improvement!\n",
    "\n",
    "**Instructions:**\n",
    "- For each state, compute value of each action\n",
    "- Select action with maximum value\n",
    "- This should look very similar to policy_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: extract_policy\n",
    "\n",
    "def extract_policy(V, transition_probs, rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Extract greedy policy from value function.\n",
    "    \n",
    "    Arguments:\n",
    "    V -- numpy array of shape (n_states,) containing state values\n",
    "    transition_probs -- numpy array of shape (n_states, n_actions, n_states)\n",
    "    rewards -- numpy array of shape (n_states, n_actions, n_states)\n",
    "    gamma -- discount factor, scalar (default: 0.99)\n",
    "    \n",
    "    Returns:\n",
    "    policy -- numpy array of shape (n_states,) containing optimal policy\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = transition_probs.shape[0]\n",
    "    n_actions = transition_probs.shape[1]\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        # (approx. 5 lines)\n",
    "        # This is identical to policy_improvement\n",
    "        # action_values = np.zeros(n_actions)\n",
    "        # for a in range(n_actions):\n",
    "        #     ... compute action values ...\n",
    "        # policy[s] = np.argmax(action_values)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extract_policy\n",
    "print(\"Testing extract_policy...\\n\")\n",
    "\n",
    "# Extract policy from test values\n",
    "extracted_policy = extract_policy(V_new, transition_probs, rewards, gamma=0.99)\n",
    "\n",
    "print(f\"Extracted policy (first 5): {extracted_policy[:5]}\")\n",
    "print(f\"Policy shape: {extracted_policy.shape}\")\n",
    "\n",
    "# Run automated test\n",
    "extract_policy_test(extract_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Testing extract_policy...\n",
    "\n",
    "Extracted policy (first 5): [...]\n",
    "Policy shape: (16,)\n",
    "‚úì All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Value Iteration combines evaluation and improvement in one step\n",
    "- It uses the Bellman optimality equation directly\n",
    "- The policy is extracted only at the end using greedy action selection\n",
    "- Generally more efficient than Policy Iteration for large state spaces\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Running Value Iteration\n",
    "\n",
    "Now let's run the complete Value Iteration algorithm and compare it with Policy Iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Value Iteration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING VALUE ITERATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "vi_solver = ValueIteration(\n",
    "    n_states=n_states,\n",
    "    n_actions=n_actions,\n",
    "    gamma=0.99,\n",
    "    theta=1e-6\n",
    ")\n",
    "\n",
    "vi_results = vi_solver.solve(\n",
    "    transition_probs=transition_probs,\n",
    "    rewards=rewards,\n",
    "    max_iterations=1000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Converged in {vi_results['iterations']} iterations\")\n",
    "print(f\"‚úì Average state value: {np.mean(vi_results['V']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_policy_and_values(\n",
    "    vi_results['policy'],\n",
    "    vi_results['V'],\n",
    "    grid_size=4,\n",
    "    title=\"(Value Iteration)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Algorithm Comparison\n",
    "\n",
    "Now that we've implemented both algorithms, let's compare them!\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - compare_algorithms\n",
    "\n",
    "Compare the results from Policy Iteration and Value Iteration. They should converge to the same optimal solution!\n",
    "\n",
    "**Instructions:**\n",
    "- Compute the maximum absolute difference in value functions\n",
    "- Count how many states have different actions in the policies\n",
    "- Print a summary comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compare_algorithms\n",
    "\n",
    "def compare_algorithms(pi_results, vi_results):\n",
    "    \"\"\"\n",
    "    Compare results from Policy Iteration and Value Iteration.\n",
    "    \n",
    "    Arguments:\n",
    "    pi_results -- dictionary containing Policy Iteration results\n",
    "    vi_results -- dictionary containing Value Iteration results\n",
    "    \n",
    "    Returns:\n",
    "    comparison -- dictionary with comparison metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    comparison = {}\n",
    "    \n",
    "    # (approx. 4 lines)\n",
    "    # Compute maximum value difference\n",
    "    # comparison['max_value_diff'] = np.max(np.abs(...)) \n",
    "    # Count policy differences\n",
    "    # comparison['policy_diff_count'] = np.sum(...)\n",
    "    # Add iteration counts\n",
    "    # comparison['pi_iterations'] = ...\n",
    "    # comparison['vi_iterations'] = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare algorithms\n",
    "print(\"=\"*70)\n",
    "print(\"ALGORITHM COMPARISON\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "comparison = compare_algorithms(pi_results, vi_results)\n",
    "\n",
    "print(f\"Policy Iteration:  {comparison['pi_iterations']} iterations\")\n",
    "print(f\"Value Iteration:   {comparison['vi_iterations']} iterations\")\n",
    "print(f\"\\nMax value difference: {comparison['max_value_diff']:.8f}\")\n",
    "print(f\"States with different actions: {comparison['policy_diff_count']}/{n_states}\")\n",
    "\n",
    "# Run automated test\n",
    "compare_algorithms_test(pi_results, vi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "ALGORITHM COMPARISON\n",
    "\n",
    "Policy Iteration:  ... iterations\n",
    "Value Iteration:   ... iterations\n",
    "\n",
    "Max value difference: 0.00000...\n",
    "States with different actions: 0/16 (or very few)\n",
    "‚úì All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a detailed comparison table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Iterations to Converge',\n",
    "        'Average State Value',\n",
    "        'Max State Value',\n",
    "        'Min State Value'\n",
    "    ],\n",
    "    'Policy Iteration': [\n",
    "        pi_results['iterations'],\n",
    "        f\"{np.mean(pi_results['V']):.6f}\",\n",
    "        f\"{np.max(pi_results['V']):.6f}\",\n",
    "        f\"{np.min(pi_results['V']):.6f}\"\n",
    "    ],\n",
    "    'Value Iteration': [\n",
    "        vi_results['iterations'],\n",
    "        f\"{np.mean(vi_results['V']):.6f}\",\n",
    "        f\"{np.max(vi_results['V']):.6f}\",\n",
    "        f\"{np.min(vi_results['V']):.6f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Both Policy Iteration and Value Iteration converge to the same optimal solution\n",
    "- Policy Iteration typically requires fewer iterations but each iteration is more expensive\n",
    "- Value Iteration requires more iterations but each iteration is faster\n",
    "- For small problems, the difference may be minimal\n",
    "- For large problems, Value Iteration is often preferred\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Experiments and Analysis\n",
    "\n",
    "<a name='6-1'></a>\n",
    "### 6.1 - Effect of Discount Factor\n",
    "\n",
    "The discount factor $\\gamma$ controls how much the agent values future rewards. Let's see how it affects the optimal policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT: Effect of Discount Factor (Œ≥)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "gammas = [0.5, 0.9, 0.99, 0.999]\n",
    "results_gamma = {}\n",
    "\n",
    "for gamma in gammas:\n",
    "    print(f\"Testing Œ≥ = {gamma}...\")\n",
    "    solver = ValueIteration(\n",
    "        n_states=n_states,\n",
    "        n_actions=n_actions,\n",
    "        gamma=gamma,\n",
    "        theta=1e-6\n",
    "    )\n",
    "    result = solver.solve(transition_probs, rewards, verbose=False)\n",
    "    results_gamma[gamma] = result\n",
    "    print(f\"  Iterations: {result['iterations']}, Avg Value: {np.mean(result['V']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of gamma\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    value_grid = results_gamma[gamma]['V'].reshape(4, 4)\n",
    "    im = axes[idx].imshow(value_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    axes[idx].set_title(f'Œ≥ = {gamma} ({results_gamma[gamma][\"iterations\"]} iterations)',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[idx].text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                         ha='center', va='center', color='black', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "    axes[idx].set_xticks(range(4))\n",
    "    axes[idx].set_yticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Scalability Analysis\n",
    "\n",
    "How do the algorithms scale with problem size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT: Scalability with Grid Size\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "grid_sizes = [3, 4, 5, 6]\n",
    "results_sizes = {'PI': [], 'VI': []}\n",
    "\n",
    "for size in grid_sizes:\n",
    "    print(f\"Grid {size}x{size} ({size*size} states):\")\n",
    "    \n",
    "    trans, rew, n_s, n_a = create_gridworld_mdp(grid_size=size)\n",
    "    \n",
    "    # Policy Iteration\n",
    "    pi = PolicyIteration(n_s, n_a, gamma=0.99, theta=1e-6)\n",
    "    pi_res = pi.solve(trans, rew, max_iterations=100)\n",
    "    results_sizes['PI'].append(pi_res['iterations'])\n",
    "    \n",
    "    # Value Iteration\n",
    "    vi = ValueIteration(n_s, n_a, gamma=0.99, theta=1e-6)\n",
    "    vi_res = vi.solve(trans, rew, verbose=False)\n",
    "    results_sizes['VI'].append(vi_res['iterations'])\n",
    "    \n",
    "    print(f\"  Policy Iteration: {pi_res['iterations']} iterations\")\n",
    "    print(f\"  Value Iteration:  {vi_res['iterations']} iterations\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scalability\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = [s*s for s in grid_sizes]\n",
    "plt.plot(x, results_sizes['PI'], 'o-', linewidth=2, markersize=10, label='Policy Iteration')\n",
    "plt.plot(x, results_sizes['VI'], 's-', linewidth=2, markersize=10, label='Value Iteration')\n",
    "plt.xlabel('Number of States', fontsize=12)\n",
    "plt.ylabel('Iterations to Convergence', fontsize=12)\n",
    "plt.title('Algorithm Scalability', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-3'></a>\n",
    "### 6.3 - GridWorld with Obstacles\n",
    "\n",
    "Let's test our algorithms on a more challenging environment with obstacles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridworld_with_obstacles(grid_size=5, obstacles=None):\n",
    "    \"\"\"\n",
    "    Creates a GridWorld with obstacles.\n",
    "    \"\"\"\n",
    "    if obstacles is None:\n",
    "        obstacles = [(1, 2), (2, 2), (3, 2)]\n",
    "    \n",
    "    n_states = grid_size * grid_size\n",
    "    n_actions = 4\n",
    "    goal_state = n_states - 1\n",
    "    \n",
    "    transition_probs = np.zeros((n_states, n_actions, n_states))\n",
    "    rewards = np.zeros((n_states, n_actions, n_states))\n",
    "    \n",
    "    obstacle_states = [r * grid_size + c for r, c in obstacles]\n",
    "    \n",
    "    def state_to_pos(state):\n",
    "        return state // grid_size, state % grid_size\n",
    "    \n",
    "    def pos_to_state(row, col):\n",
    "        return row * grid_size + col\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        if s == goal_state or s in obstacle_states:\n",
    "            for a in range(n_actions):\n",
    "                transition_probs[s, a, s] = 1.0\n",
    "                rewards[s, a, s] = -1.0 if s in obstacle_states else 0.0\n",
    "            continue\n",
    "        \n",
    "        row, col = state_to_pos(s)\n",
    "        \n",
    "        for a in range(n_actions):\n",
    "            new_row, new_col = row, col\n",
    "            \n",
    "            if a == 0: new_row = max(0, row - 1)\n",
    "            elif a == 1: new_col = min(grid_size - 1, col + 1)\n",
    "            elif a == 2: new_row = min(grid_size - 1, row + 1)\n",
    "            elif a == 3: new_col = max(0, col - 1)\n",
    "            \n",
    "            next_state = pos_to_state(new_row, new_col)\n",
    "            \n",
    "            if next_state in obstacle_states:\n",
    "                next_state = s\n",
    "                transition_probs[s, a, next_state] = 1.0\n",
    "                rewards[s, a, next_state] = -0.1\n",
    "            else:\n",
    "                transition_probs[s, a, next_state] = 1.0\n",
    "                rewards[s, a, next_state] = 1.0 if next_state == goal_state else -0.01\n",
    "    \n",
    "    return transition_probs, rewards, n_states, n_actions, obstacle_states\n",
    "\n",
    "# Create and solve obstacle GridWorld\n",
    "print(\"=\"*70)\n",
    "print(\"GridWorld with Obstacles (5x5)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "trans_obs, rew_obs, n_s_obs, n_a_obs, obstacles = create_gridworld_with_obstacles(\n",
    "    grid_size=5,\n",
    "    obstacles=[(1, 2), (2, 2), (3, 2)]\n",
    ")\n",
    "\n",
    "print(f\"Obstacles at states: {obstacles}\\n\")\n",
    "\n",
    "vi_obs = ValueIteration(n_s_obs, n_a_obs, gamma=0.99, theta=1e-6)\n",
    "result_obs = vi_obs.solve(trans_obs, rew_obs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Advanced Visualizations\n",
    "\n",
    "<a name='7-1'></a>\n",
    "### 7.1 - Q-Value Heatmaps\n",
    "\n",
    "Let's visualize the Q-values for each action to understand how the agent evaluates different choices in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values(solver, transition_probs, rewards, grid_size=4):\n",
    "    \"\"\"\n",
    "    Visualizes Q-values for each action in each state.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    action_names = ['Up (‚Üë)', 'Right (‚Üí)', 'Down (‚Üì)', 'Left (‚Üê)']\n",
    "    \n",
    "    for action in range(4):\n",
    "        ax = axes[action // 2, action % 2]\n",
    "        \n",
    "        q_values = np.zeros(grid_size * grid_size)\n",
    "        for state in range(grid_size * grid_size):\n",
    "            q_values[state] = solver.get_q_value(state, action, transition_probs, rewards)\n",
    "        \n",
    "        q_grid = q_values.reshape(grid_size, grid_size)\n",
    "        im = ax.imshow(q_grid, cmap='coolwarm', interpolation='nearest')\n",
    "        ax.set_title(f'Q-Values: {action_names[action]}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                ax.text(j, i, f'{q_grid[i, j]:.3f}',\n",
    "                       ha='center', va='center', color='white', fontsize=9)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax)\n",
    "        ax.set_xticks(range(grid_size))\n",
    "        ax.set_yticks(range(grid_size))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing Q-Values for each action...\\n\")\n",
    "visualize_q_values(vi_solver, transition_probs, rewards, grid_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-2'></a>\n",
    "### 7.2 - Convergence Animation\n",
    "\n",
    "Let's see how the value function evolves during the learning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_convergence_snapshots():\n",
    "    \"\"\"\n",
    "    Shows snapshots of value function at different iterations.\n",
    "    \"\"\"\n",
    "    print(\"Running Value Iteration with snapshots...\\n\")\n",
    "    \n",
    "    solver = ValueIteration(n_states=16, n_actions=4, gamma=0.99, theta=1e-6)\n",
    "    snapshot_iters = [1, 3, 5, 10, 20, 50]\n",
    "    snapshots = []\n",
    "    \n",
    "    for iteration in range(max(snapshot_iters) + 1):\n",
    "        solver.value_update(transition_probs, rewards)\n",
    "        if iteration in snapshot_iters:\n",
    "            snapshots.append((iteration, solver.V.copy()))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (iter_num, V) in enumerate(snapshots):\n",
    "        value_grid = V.reshape(4, 4)\n",
    "        im = axes[idx].imshow(value_grid, cmap='RdYlGn',\n",
    "                             interpolation='nearest', vmin=-0.1, vmax=1.0)\n",
    "        axes[idx].set_title(f'Iteration {iter_num}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                axes[idx].text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                             ha='center', va='center', color='black', fontsize=9)\n",
    "        \n",
    "        axes[idx].set_xticks(range(4))\n",
    "        axes[idx].set_yticks(range(4))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_convergence_snapshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-3'></a>\n",
    "### 7.3 - Optimal Trajectories\n",
    "\n",
    "Finally, let's visualize the optimal path the agent takes from different starting positions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate trajectories from different start states\n",
    "print(\"Simulating optimal trajectories...\\n\")\n",
    "\n",
    "start_states = [0, 4, 8]\n",
    "for start in start_states:\n",
    "    trajectory = simulate_trajectory(\n",
    "        vi_results['policy'],\n",
    "        start_state=start,\n",
    "        goal_state=15,\n",
    "        grid_size=4\n",
    "    )\n",
    "    print(f\"Start State {start}: {trajectory} ({len(trajectory)-1} steps)\")\n",
    "\n",
    "# Visualize one trajectory\n",
    "print(\"\\nVisualizing trajectory from state 0...\")\n",
    "trajectory_example = simulate_trajectory(vi_results['policy'], 0, 15, 4)\n",
    "visualize_trajectory(vi_results['policy'], trajectory_example, grid_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully completed the Dynamic Programming tutorial! Here's what you've accomplished:\n",
    "\n",
    "‚úÖ Implemented Policy Evaluation to compute state values  \n",
    "‚úÖ Implemented Policy Improvement for greedy policy updates  \n",
    "‚úÖ Built a complete Policy Iteration algorithm  \n",
    "‚úÖ Implemented Value Iteration for efficient optimization  \n",
    "‚úÖ Compared and analyzed different DP algorithms  \n",
    "‚úÖ Experimented with various parameters and environments  \n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**Key Takeaways**:\n",
    "- Dynamic Programming requires complete knowledge of environment dynamics\n",
    "- Policy Iteration alternates between evaluation and improvement steps\n",
    "- Value Iteration combines both steps using the Bellman optimality equation\n",
    "- Both algorithms guarantee convergence to the optimal policy\n",
    "- The discount factor Œ≥ controls the agent's preference for immediate vs future rewards\n",
    "- DP forms the theoretical foundation for model-free RL methods\n",
    "</font>\n",
    "\n",
    "**Next Steps**:\n",
    "\n",
    "Dynamic Programming is powerful but limited to environments where we know all the dynamics. In the next tutorials, you'll learn:\n",
    "\n",
    "1. **Monte Carlo Methods** - Learning from experience without a model\n",
    "2. **Temporal Difference Learning** - Combining DP and MC (TD, Q-Learning, SARSA)\n",
    "3. **Function Approximation** - Handling large/continuous state spaces\n",
    "4. **Deep Reinforcement Learning** - Using neural networks (DQN, Policy Gradients)\n",
    "\n",
    "**References**:\n",
    "\n",
    "- Sutton & Barto (2018): *Reinforcement Learning: An Introduction* - Chapter 4\n",
    "- Bellman, R. (1957): *Dynamic Programming*\n",
    "- Puterman, M. (1994): *Markov Decision Processes*\n",
    "\n",
    "Great work! Keep exploring and experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
