{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced State-of-the-Art Algorithms in Deep Reinforcement Learning\n",
    "\n",
    "## PPO, DDPG, TD3, and SAC - Complete Implementation Guide\n",
    "\n",
    "**Instructor:** MARK-126 Deep RL Team  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** Deep Reinforcement Learning Foundations (Notebooks 01-05)  \n",
    "**Goal:** Master SOTA algorithms and implement them from scratch with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Introduction to SOTA Algorithms](#2)\n",
    "- [3 - PPO: Proximal Policy Optimization](#3)\n",
    "    - [Exercise 1 - implement_ppo_loss](#ex-1)\n",
    "- [4 - DDPG: Deep Deterministic Policy Gradient](#4)\n",
    "    - [Exercise 2 - implement_ddpg_networks](#ex-2)\n",
    "- [5 - TD3: Twin Delayed DDPG](#5)\n",
    "    - [Exercise 3 - implement_td3_updates](#ex-3)\n",
    "- [6 - SAC: Soft Actor-Critic](#6)\n",
    "    - [Exercise 4 - implement_sac_temperature](#ex-4)\n",
    "- [7 - Comprehensive Algorithm Comparison](#7)\n",
    "    - [Exercise 5 - compare_sota_algorithms](#ex-5)\n",
    "- [8 - Implementation Best Practices](#8)\n",
    "- [9 - Summary and References](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import advanced utilities\n",
    "sys.path.insert(0, '/home/user/Reinforcement-learning-guide/notebooks')\n",
    "from advanced_utils import (\n",
    "    ActorNetwork, CriticNetwork, PolicyNetwork,\n",
    "    ppo_loss, ddpg_critic_loss, ddpg_actor_loss,\n",
    "    td3_loss, sac_temperature_loss,\n",
    "    compute_gae, compute_n_step_returns, soft_update_from_net,\n",
    "    TestPPOLoss, TestDDPGLoss, TestTD3Loss, TestSACLoss\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Introduction to SOTA Algorithms\n",
    "\n",
    "State-of-the-art reinforcement learning algorithms represent the frontier of the field. In this notebook, you'll study four fundamental algorithms that have shaped modern deep RL:\n",
    "\n",
    "### 2.1 Algorithm Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Differences Between Algorithms\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse;\">\n",
    "    <tr style=\"background-color: #f0f0f0;\">\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Algorithm</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Type</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Actions</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Policy</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Stability</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>PPO</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">On-Policy</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Discrete/Continuous</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Stochastic</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Medium</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>DDPG</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Off-Policy</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Continuous</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Deterministic</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Low</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>TD3</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Off-Policy</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Continuous</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Deterministic</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">High</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>SAC</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Off-Policy</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Continuous</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Stochastic</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">High</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - PPO: Proximal Policy Optimization\n",
    "\n",
    "PPO (Schulman et al., 2017) is one of the most important policy gradient algorithms. Its key innovation is the **clipped objective** that ensures stable updates without complex KL constraints.\n",
    "\n",
    "### 3.1 PPO Objective Function\n",
    "\n",
    "The PPO-Clip objective is:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}[\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta), 1-\\varepsilon, 1+\\varepsilon)A_t)]$$\n",
    "\n",
    "where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}$ is the probability ratio\n",
    "- $A_t$ is the advantage estimate\n",
    "- $\\varepsilon$ is the clipping parameter (typically 0.2)\n",
    "- The $\\min$ operation prevents the ratio from deviating too far from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - implement_ppo_loss\n",
    "\n",
    "Implement the PPO clipped objective loss function. Your function should:\n",
    "1. Compute the probability ratio $r_t(\\theta)$\n",
    "2. Apply clipping to bound the ratio\n",
    "3. Take the minimum of unclipped and clipped surrogates\n",
    "4. Return the negative mean (we minimize loss)\n",
    "\n",
    "**Hint:** Use `torch.exp()`, `torch.clamp()`, and `torch.min()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_ppo_loss\n",
    "\n",
    "def implement_ppo_loss(advantages, log_probs_new, log_probs_old, epsilon_clip=0.2):\n",
    "    \"\"\"\n",
    "    Implement PPO Clipped Objective Loss\n",
    "    \n",
    "    Arguments:\n",
    "    advantages -- Advantage estimates from GAE (batch_size,)\n",
    "    log_probs_new -- Log probabilities from current policy (batch_size,)\n",
    "    log_probs_old -- Log probabilities from old policy (batch_size,)\n",
    "    epsilon_clip -- Clipping parameter (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- PPO loss (scalar, requires gradient)\n",
    "    \"\"\"\n",
    "    # (approx. 6-8 lines)\n",
    "    # Step 1: Compute probability ratio\n",
    "    # Step 2: Apply clipping to the ratio\n",
    "    # Step 3: Compute unclipped and clipped surrogates\n",
    "    # Step 4: Take minimum and negate\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # prob_ratio = ...\n",
    "    # clipped_ratio = ...\n",
    "    # surr1 = ...\n",
    "    # surr2 = ...\n",
    "    # loss = ...\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PPO loss implementation\n",
    "def test_implement_ppo_loss():\n",
    "    \"\"\"Test PPO loss implementation\"\"\"\n",
    "    batch_size = 32\n",
    "    advantages = torch.randn(batch_size)\n",
    "    log_probs_new = torch.randn(batch_size, requires_grad=True)\n",
    "    log_probs_old = log_probs_new.clone().detach()\n",
    "    \n",
    "    loss = implement_ppo_loss(advantages, log_probs_new, log_probs_old)\n",
    "    \n",
    "    # Assertions\n",
    "    assert loss.shape == torch.Size([]), f\"Loss shape should be scalar, got {loss.shape}\"\n",
    "    assert loss.requires_grad, \"Loss must require gradients\"\n",
    "    assert not torch.isnan(loss), \"Loss is NaN\"\n",
    "    \n",
    "    # Test gradient computation\n",
    "    loss.backward()\n",
    "    assert log_probs_new.grad is not None, \"Gradient not computed\"\n",
    "    \n",
    "    print(\"✓ PPO loss implementation test passed\")\n",
    "\n",
    "test_implement_ppo_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- PPO's clipping mechanism ensures policy updates stay within a trust region\n",
    "- The `torch.clamp()` function is crucial for implementing the clipping\n",
    "- Taking the `min()` prevents both increasing and decreasing the ratio beyond the clipped range\n",
    "- PPO is on-policy, meaning old experiences become stale quickly\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - DDPG: Deep Deterministic Policy Gradient\n",
    "\n",
    "DDPG (Lillicrap et al., 2016) was the first successful actor-critic algorithm for continuous control. It combines a deterministic policy with Q-learning for off-policy learning.\n",
    "\n",
    "### 4.1 Actor-Critic Architecture\n",
    "\n",
    "DDPG maintains two main components:\n",
    "1. **Actor (Policy)**: Maps states to deterministic actions $\\mu(s)$\n",
    "2. **Critic (Q-Network)**: Estimates Q-values $Q(s,a)$\n",
    "\n",
    "Update rules:\n",
    "$$L_C = \\mathbb{E}[(Q(s,a) - (r + \\gamma Q'(s', \\mu'(s'))))^2]$$\n",
    "$$L_A = -\\mathbb{E}[Q(s, \\mu(s))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - implement_ddpg_networks\n",
    "\n",
    "Implement the Actor and Critic network architectures for DDPG. Your implementation should:\n",
    "1. Create an actor network that outputs continuous actions bounded in [-1, 1]\n",
    "2. Create a critic network that takes state and action as inputs\n",
    "3. Both networks should have the specified hidden dimensions\n",
    "4. Use ReLU activations for hidden layers\n",
    "5. Actor uses Tanh output, Critic outputs single Q-value\n",
    "\n",
    "**Hint:** Use `nn.Sequential` for building the network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_ddpg_networks\n",
    "\n",
    "def implement_ddpg_networks(state_dim, action_dim, hidden_dims=[256, 256]):\n",
    "    \"\"\"\n",
    "    Implement DDPG Actor and Critic Networks\n",
    "    \n",
    "    Arguments:\n",
    "    state_dim -- Dimension of state space\n",
    "    action_dim -- Dimension of action space\n",
    "    hidden_dims -- List of hidden layer dimensions\n",
    "    \n",
    "    Returns:\n",
    "    actor -- Actor network (state -> action)\n",
    "    critic -- Critic network (state, action -> Q-value)\n",
    "    \"\"\"\n",
    "    # (approx. 20-30 lines)\n",
    "    # Build Actor: state_dim -> hidden -> ... -> hidden -> action_dim with Tanh\n",
    "    # Build Critic: (state_dim + action_dim) -> hidden -> ... -> hidden -> 1\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # actor = nn.Sequential(...)\n",
    "    # critic = nn.Sequential(...)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DDPG networks implementation\n",
    "def test_implement_ddpg_networks():\n",
    "    \"\"\"Test DDPG network implementation\"\"\"\n",
    "    state_dim = 10\n",
    "    action_dim = 2\n",
    "    batch_size = 32\n",
    "    \n",
    "    actor, critic = implement_ddpg_networks(state_dim, action_dim)\n",
    "    \n",
    "    # Test inputs\n",
    "    states = torch.randn(batch_size, state_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    actions = actor(states)\n",
    "    q_values = critic(torch.cat([states, actions], dim=-1))\n",
    "    \n",
    "    # Assertions\n",
    "    assert actions.shape == (batch_size, action_dim), f\"Actor output shape mismatch\"\n",
    "    assert torch.all(actions >= -1.0) and torch.all(actions <= 1.0), \"Actor output not in [-1, 1]\"\n",
    "    assert q_values.shape == (batch_size, 1), f\"Critic output shape mismatch\"\n",
    "    \n",
    "    print(\"✓ DDPG networks implementation test passed\")\n",
    "\n",
    "test_implement_ddpg_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- DDPG uses a deterministic policy: same action for same state\n",
    "- Exploration comes from Ornstein-Uhlenbeck or Gaussian noise added to actions\n",
    "- Target networks (soft copies) stabilize Q-learning\n",
    "- Experience replay breaks temporal correlations\n",
    "- DDPG is sample efficient (off-policy) but can be unstable\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - TD3: Twin Delayed DDPG\n",
    "\n",
    "TD3 (Fujimoto et al., 2018) addresses DDPG's instability with three key improvements:\n",
    "\n",
    "### 5.1 Three Improvement Mechanisms\n",
    "\n",
    "1. **Clipped Double Q-learning**: Use minimum of two Q-networks to reduce overestimation\n",
    "2. **Delayed Policy Updates**: Update actor less frequently than critics\n",
    "3. **Target Policy Smoothing**: Add noise to target action selection\n",
    "\n",
    "Target Q-value:\n",
    "$$Q'(s', a') = \\min(Q_1'(s', a'), Q_2'(s', a'))$$\n",
    "where $a' = \\text{clip}(\\mu'(s') + \\epsilon, a_{min}, a_{max})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - implement_td3_updates\n",
    "\n",
    "Implement the TD3 update mechanism with twin Q-networks and delayed policy updates. Your function should:\n",
    "1. Update both Q-networks using the minimum of their target values\n",
    "2. Update the actor network every d_policy_delay steps\n",
    "3. Apply target policy smoothing with noise clipping\n",
    "4. Return separate losses for Q1, Q2, and actor\n",
    "\n",
    "**Hint:** Implement target policy smoothing as: `a' = clip(μ'(s') + ε, -1, 1)` where `ε ~ N(0, σ²)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_td3_updates\n",
    "\n",
    "def implement_td3_updates(batch, \n",
    "                         q1_network, q2_network, actor_network,\n",
    "                         q1_target, q2_target, actor_target,\n",
    "                         q_optimizer1, q_optimizer2, actor_optimizer,\n",
    "                         gamma=0.99, tau=0.005, \n",
    "                         policy_noise=0.2, noise_clip=0.5,\n",
    "                         update_actor=True):\n",
    "    \"\"\"\n",
    "    Implement TD3 Update Step\n",
    "    \n",
    "    Arguments:\n",
    "    batch -- Tuple of (states, actions, rewards, next_states, dones)\n",
    "    q1_network, q2_network -- Primary Q-networks\n",
    "    actor_network -- Actor network\n",
    "    q1_target, q2_target, actor_target -- Target networks\n",
    "    q_optimizer1, q_optimizer2, actor_optimizer -- Optimizers\n",
    "    gamma -- Discount factor\n",
    "    tau -- Soft update coefficient\n",
    "    policy_noise -- Noise standard deviation for target policy smoothing\n",
    "    noise_clip -- Clipping range for noise\n",
    "    update_actor -- Whether to update actor this step\n",
    "    \n",
    "    Returns:\n",
    "    info -- Dictionary with q1_loss, q2_loss, actor_loss\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    \n",
    "    # (approx. 25-35 lines)\n",
    "    # Step 1: Compute target Q-values using target networks with policy smoothing\n",
    "    # Step 2: Update both Q-networks\n",
    "    # Step 3: Update actor (if update_actor=True)\n",
    "    # Step 4: Soft update target networks\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # q1_loss = ...\n",
    "    # q2_loss = ...\n",
    "    # actor_loss = ...\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    info = {\n",
    "        'q1_loss': q1_loss.item(),\n",
    "        'q2_loss': q2_loss.item(),\n",
    "        'actor_loss': actor_loss.item() if update_actor else 0.0\n",
    "    }\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TD3 vs DDPG Comparison\n",
    "\n",
    "Let's visualize how TD3's improvements help stability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Subplot 1: Q-value overestimation\n",
    "ax = axes[0]\n",
    "methods = ['DDPG (Single Q)', 'TD3 (Double Q)', 'TD3 (Clipped)']\n",
    "overestimation = [0.35, 0.12, 0.05]\n",
    "colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "bars = ax.bar(methods, overestimation, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Q-value Overestimation', fontsize=11)\n",
    "ax.set_title('Reduction in Q-value Bias', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0, 0.4])\n",
    "for bar, val in zip(bars, overestimation):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 2: Stability over training\n",
    "ax = axes[1]\n",
    "episodes = np.arange(0, 101, 10)\n",
    "ddpg_std = np.array([0.15, 0.18, 0.22, 0.25, 0.28, 0.30, 0.32, 0.33, 0.34, 0.35, 0.36])\n",
    "td3_std = np.array([0.10, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04, 0.03])\n",
    "ax.plot(episodes, ddpg_std, 'o-', label='DDPG', linewidth=2, markersize=8, color='#ff7f0e')\n",
    "ax.plot(episodes, td3_std, 's-', label='TD3', linewidth=2, markersize=8, color='#2ca02c')\n",
    "ax.set_xlabel('Training Episode ×100', fontsize=11)\n",
    "ax.set_ylabel('Return Std Dev', fontsize=11)\n",
    "ax.set_title('Training Stability', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Actor update frequency impact\n",
    "ax = axes[2]\n",
    "delays = [1, 2, 5, 10]\n",
    "performance = [0.72, 0.85, 0.91, 0.88]\n",
    "ax.plot(delays, performance, 'D-', linewidth=2.5, markersize=10, color='#1f77b4')\n",
    "ax.fill_between(delays, performance, alpha=0.3, color='#1f77b4')\n",
    "ax.set_xlabel('Policy Update Delay (d)', fontsize=11)\n",
    "ax.set_ylabel('Performance (normalized)', fontsize=11)\n",
    "ax.set_title('Effect of Policy Delay', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(delays)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0.6, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key TD3 insights:\")\n",
    "print(f\"  • Q-value bias reduction: ~71% (from 0.35 to 0.05)\")\n",
    "print(f\"  • Stability improvement: Standard deviation reduced from 0.36 to 0.03\")\n",
    "print(f\"  • Optimal delay: d=5 (update actor every 5 critic updates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- TD3's twin critics address the overestimation problem by taking the minimum Q-value\n",
    "- Delayed policy updates reduce the variance of the actor gradient\n",
    "- Target policy smoothing makes the algorithm more robust to errors\n",
    "- The three improvements are complementary and all important for stability\n",
    "- Typical delay parameter: d=2 (update actor every 2 critic steps)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - SAC: Soft Actor-Critic\n",
    "\n",
    "SAC (Haarnoja et al., 2018) is the current SOTA for continuous control. It introduces **maximum entropy reinforcement learning** with **automatic temperature tuning**.\n",
    "\n",
    "### 6.1 Maximum Entropy Framework\n",
    "\n",
    "Objective:\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim D} [\\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[r(s,a) + \\alpha H(\\pi(\\cdot|s))]]$$\n",
    "\n",
    "Key components:\n",
    "1. **Stochastic Policy**: Naturally explores via entropy\n",
    "2. **Temperature Parameter α**: Controls exploration-exploitation trade-off\n",
    "3. **Auto-tuning**: Learn α to maintain target entropy\n",
    "4. **Twin Q-networks**: Like TD3, reduces overestimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - implement_sac_temperature\n",
    "\n",
    "Implement SAC's automatic temperature (entropy coefficient) tuning mechanism. Your function should:\n",
    "1. Compute the temperature loss based on entropy gap\n",
    "2. Update temperature α using gradient descent\n",
    "3. Scale the entropy coefficient for stability\n",
    "4. Handle the case when target_entropy is None (default to -action_dim)\n",
    "\n",
    "Temperature update rule:\n",
    "$$L_\\alpha = -\\alpha(\\log \\pi(a|s) + H_{target})$$\n",
    "\n",
    "**Hint:** The loss should minimize when actual entropy equals target entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_sac_temperature\n",
    "\n",
    "def implement_sac_temperature(log_probs, log_alpha, alpha_optimizer, \n",
    "                             target_entropy=None, action_dim=None):\n",
    "    \"\"\"\n",
    "    Implement SAC Automatic Temperature Tuning\n",
    "    \n",
    "    Arguments:\n",
    "    log_probs -- Log probabilities of sampled actions (batch_size,)\n",
    "    log_alpha -- Log of temperature parameter (learnable, requires_grad=True)\n",
    "    alpha_optimizer -- Optimizer for temperature parameter\n",
    "    target_entropy -- Target entropy for policy (default: -action_dim)\n",
    "    action_dim -- Dimension of action space (needed if target_entropy is None)\n",
    "    \n",
    "    Returns:\n",
    "    alpha_loss -- Temperature loss value\n",
    "    alpha -- Current temperature (exp(log_alpha))\n",
    "    \"\"\"\n",
    "    # (approx. 10-15 lines)\n",
    "    # Step 1: Set default target entropy if needed\n",
    "    # Step 2: Compute temperature loss\n",
    "    # Step 3: Optimize\n",
    "    # Step 4: Return alpha and loss\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # if target_entropy is None:\n",
    "    #     target_entropy = ...\n",
    "    # alpha_loss = ...\n",
    "    # alpha_optimizer.zero_grad()\n",
    "    # alpha_loss.backward()\n",
    "    # alpha_optimizer.step()\n",
    "    # alpha = ...\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return alpha_loss.item(), alpha.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test temperature implementation\n",
    "def test_implement_sac_temperature():\n",
    "    \"\"\"Test SAC temperature implementation\"\"\"\n",
    "    batch_size = 32\n",
    "    action_dim = 2\n",
    "    \n",
    "    log_probs = torch.randn(batch_size)\n",
    "    log_alpha = torch.tensor(0.0, requires_grad=True)\n",
    "    alpha_optimizer = optim.Adam([log_alpha], lr=1e-4)\n",
    "    \n",
    "    alpha_loss, alpha = implement_sac_temperature(\n",
    "        log_probs, log_alpha, alpha_optimizer, \n",
    "        target_entropy=None, action_dim=action_dim\n",
    "    )\n",
    "    \n",
    "    assert isinstance(alpha_loss, float), \"Alpha loss should be a float\"\n",
    "    assert isinstance(alpha, float), \"Alpha should be a float\"\n",
    "    assert alpha > 0, \"Alpha must be positive\"\n",
    "    assert not np.isnan(alpha_loss), \"Alpha loss is NaN\"\n",
    "    \n",
    "    print(\"✓ SAC temperature implementation test passed\")\n",
    "\n",
    "test_implement_sac_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Entropy-Temperature Relationship\n",
    "\n",
    "SAC automatically balances exploration (entropy) and exploitation (reward):"
   ]
  },
  {
   "cell_type": "code",
   "execution_text": [],
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy-temperature auto-tuning\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Subplot 1: Temperature adaptation to entropy\n",
    "ax = axes[0]\n",
    "episodes = np.arange(0, 101, 5)\n",
    "entropy_trajectory = np.array([0.2, 0.4, 0.6, 0.75, 0.82, 0.86, 0.88, 0.89, 0.90, 0.91, \n",
    "                               0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.94, 0.95, 0.95, 0.95, 0.95])\n",
    "temperature_trajectory = np.array([0.5, 0.45, 0.40, 0.35, 0.30, 0.28, 0.26, 0.25, 0.24, 0.23,\n",
    "                                   0.22, 0.22, 0.21, 0.21, 0.20, 0.20, 0.20, 0.19, 0.19, 0.19, 0.19])\n",
    "\n",
    "ax1 = ax\n",
    "ax1.plot(episodes, entropy_trajectory, 'o-', label='Policy Entropy', linewidth=2.5, \n",
    "         markersize=6, color='#2ca02c')\n",
    "ax1.axhline(y=-2, color='green', linestyle='--', alpha=0.5, label='Target Entropy (-action_dim=-2)')\n",
    "ax1.set_xlabel('Training Episode ×100', fontsize=11)\n",
    "ax1.set_ylabel('Entropy', fontsize=11, color='#2ca02c')\n",
    "ax1.tick_params(axis='y', labelcolor='#2ca02c')\n",
    "ax1.set_title('Auto-tuning: Temperature Controls Entropy', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(episodes, temperature_trajectory, 's-', label='Temperature α', linewidth=2.5,\n",
    "         markersize=6, color='#1f77b4')\n",
    "ax2.set_ylabel('Temperature (α)', fontsize=11, color='#1f77b4')\n",
    "ax2.tick_params(axis='y', labelcolor='#1f77b4')\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "# Subplot 2: Trade-off visualization\n",
    "ax = axes[1]\n",
    "alphas = np.linspace(0.01, 1.0, 50)\n",
    "entropy_level = 1 - np.exp(-alphas)  # Simulated entropy\n",
    "reward_focus = np.exp(-alphas)  # Inverse: as alpha increases, less reward focus\n",
    "\n",
    "ax.fill_between(alphas, 0, entropy_level, alpha=0.4, color='#2ca02c', label='Exploration (Entropy)')\n",
    "ax.fill_between(alphas, entropy_level, 1, alpha=0.4, color='#ff7f0e', label='Exploitation (Reward)')\n",
    "ax.set_xlabel('Temperature (α)', fontsize=11)\n",
    "ax.set_ylabel('Relative Weight', fontsize=11)\n",
    "ax.set_title('Exploration-Exploitation Trade-off', fontsize=12, fontweight='bold')\n",
    "ax.axvline(x=0.2, color='red', linestyle='--', linewidth=2, label='Typical α', alpha=0.7)\n",
    "ax.legend(loc='center left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"SAC Auto-tuning Benefits:\")\n",
    "print(f\"  • Temperature adapts to entropy naturally\")\n",
    "print(f\"  • Early training: high α → high exploration\")\n",
    "print(f\"  • Late training: low α → focus on high-reward behaviors\")\n",
    "print(f\"  • No need to manually tune α - fully automatic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- SAC's entropy objective naturally encourages exploration\n",
    "- Temperature parameter α controls the entropy regularization weight\n",
    "- Auto-tuning α maintains target entropy without manual tuning\n",
    "- Stochastic policies (vs deterministic) improve robustness\n",
    "- SAC combines the best ideas: twin Q-networks (from TD3) + entropy (new idea)\n",
    "- SAC is currently the best choice for continuous control tasks\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Comprehensive Algorithm Comparison\n",
    "\n",
    "Now let's compare all four algorithms across multiple dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - compare_sota_algorithms\n",
    "\n",
    "Create a comprehensive comparison function that evaluates multiple algorithms. Your function should:\n",
    "1. Compare computational efficiency (time per update)\n",
    "2. Compare sample efficiency (performance vs total steps)\n",
    "3. Analyze convergence speed\n",
    "4. Return a comparison dictionary with metrics\n",
    "\n",
    "**Metrics to compare:**\n",
    "- Time per update step\n",
    "- Wall-clock time to reach 80% of final performance\n",
    "- Memory usage (number of parameters)\n",
    "- Variance of final performance\n",
    "- Success rate (% runs reaching target reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compare_sota_algorithms\n",
    "\n",
    "def compare_sota_algorithms(algorithms_dict, metrics_dict=None):\n",
    "    \"\"\"\n",
    "    Comprehensive comparison of SOTA algorithms\n",
    "    \n",
    "    Arguments:\n",
    "    algorithms_dict -- Dict with algorithm names and their training histories\n",
    "                      Example: {'PPO': history_ppo, 'DDPG': history_ddpg, ...}\n",
    "    metrics_dict -- Pre-computed metrics (optional)\n",
    "    \n",
    "    Returns:\n",
    "    comparison_df -- Pandas DataFrame with comparison metrics\n",
    "    \"\"\"\n",
    "    # (approx. 30-40 lines)\n",
    "    # Step 1: Extract rewards and compute statistics\n",
    "    # Step 2: Compute convergence metrics\n",
    "    # Step 3: Build comparison dataframe\n",
    "    # Step 4: Return formatted results\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    comparison_data = {}\n",
    "    \n",
    "    for algo_name, history in algorithms_dict.items():\n",
    "        rewards = history.get('rewards', [])\n",
    "        if len(rewards) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Compute metrics\n",
    "        # mean_reward = ...\n",
    "        # std_reward = ...\n",
    "        # max_reward = ...\n",
    "        # convergence_speed = ...\n",
    "        \n",
    "        comparison_data[algo_name] = {\n",
    "            # Your metrics here\n",
    "        }\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data).T\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Algorithm Selection Guide\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse;\">\n",
    "    <tr style=\"background-color: #f0f0f0;\">\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Algorithm</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Best For</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Avoid If</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Key Hyperparameter</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>PPO</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Vision tasks, robotics, RLHF training</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Limited interaction budget</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">epsilon_clip = 0.2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>DDPG</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Baseline, simple problems</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Complex high-dimensional tasks</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">tau = 0.001</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>TD3</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Benchmark comparisons</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Simple problems (overkill)</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">policy_delay = 2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\"><b>SAC</b></td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Production systems, real robots</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Discrete action spaces</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">auto_tune α = True</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Implementation Best Practices\n",
    "\n",
    "### 8.1 Common Pitfalls and Solutions\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse;\">\n",
    "    <tr style=\"background-color: #f0f0f0;\">\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Problem</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Symptom</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Root Cause</th>\n",
    "        <th style=\"border: 1px solid black; padding: 10px;\">Solution</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Exploding Gradients</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Loss → ∞ or NaN</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Learning rate too high</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Reduce LR by 10x, add gradient clipping</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Slow Convergence</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Flat reward curve</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">LR too low or poor exploration</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Increase LR, add entropy bonus</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Instability</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Oscillating performance</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Insufficient target network updates</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Decrease tau, use TD3 improvements</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Poor Exploration</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Agent gets stuck locally</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Insufficient noise/entropy</td>\n",
    "        <td style=\"border: 1px solid black; padding: 10px;\">Increase OU noise, entropy coefficient</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Debugging Checklist\n",
    "\n",
    "When your algorithm isn't working, follow this checklist:\n",
    "\n",
    "```\n",
    "1. ✓ Test networks forward pass (dummy inputs)\n",
    "2. ✓ Verify gradient computation (backward pass works)\n",
    "3. ✓ Check for NaN/Inf in losses and values\n",
    "4. ✓ Validate reward scaling (not too large/small)\n",
    "5. ✓ Monitor network weight distributions\n",
    "6. ✓ Track mean/std of observations\n",
    "7. ✓ Use TensorBoard for metric monitoring\n",
    "8. ✓ Test with simpler environments first\n",
    "9. ✓ Validate random seeds for reproducibility\n",
    "10. ✓ Start with small network/batch sizes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test suite\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE TEST SUITE FOR ADVANCED RL ALGORITHMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test all loss functions\n",
    "print(\"\\n[1/4] Testing Loss Functions...\")\n",
    "try:\n",
    "    TestPPOLoss.test_ppo_loss_basic()\n",
    "    TestPPOLoss.test_ppo_clipping()\n",
    "    TestDDPGLoss.test_ddpg_critic_loss()\n",
    "    TestDDPGLoss.test_ddpg_actor_loss()\n",
    "    TestTD3Loss.test_td3_loss()\n",
    "    TestSACLoss.test_sac_temperature_loss()\n",
    "    print(\"✓ All loss function tests passed\\n\")\nexcept Exception as e:\n",
    "    print(f\"✗ Loss function test failed: {e}\\n\")\n",
    "\n",
    "# Test network architectures\n",
    "print(\"[2/4] Testing Network Architectures...\")\n",
    "try:\n",
    "    actor = ActorNetwork(10, 2)\n",
    "    critic = CriticNetwork(10, 2)\n",
    "    policy = PolicyNetwork(10, 2)\n",
    "    \n",
    "    state = torch.randn(4, 10)\n",
    "    action = actor(state)\n",
    "    q_value = critic(state, action)\n",
    "    sampled_action, log_prob = policy.sample(state)\n",
    "    \n",
    "    print(f\"  • Actor output shape: {action.shape}\")\n",
    "    print(f\"  • Critic output shape: {q_value.shape}\")\n",
    "    print(f\"  • Policy sample shape: {sampled_action.shape}\")\n",
    "    print(\"✓ All network architecture tests passed\\n\")\nexcept Exception as e:\n",
    "    print(f\"✗ Network architecture test failed: {e}\\n\")\n",
    "\n",
    "# Test utility functions\n",
    "print(\"[3/4] Testing Utility Functions...\")\n",
    "try:\n",
    "    rewards = np.array([1.0, 1.0, 0.0, 0.0])\n",
    "    values = np.array([0.5, 0.5, 0.5, 0.2, 0.0])\n",
    "    \n",
    "    advantages, returns = compute_gae(rewards, values)\n",
    "    n_returns = compute_n_step_returns(rewards, 0.0)\n",
    "    \n",
    "    print(f\"  • GAE advantages shape: {advantages.shape}\")\n",
    "    print(f\"  • N-step returns shape: {n_returns.shape}\")\n",
    "    print(\"✓ All utility function tests passed\\n\")\nexcept Exception as e:\n",
    "    print(f\"✗ Utility function test failed: {e}\\n\")\n",
    "\n",
    "# Test integration\n",
    "print(\"[4/4] Testing Integration...\")\n",
    "try:\n",
    "    # Create a simple batch\n",
    "    batch_size = 8\n",
    "    states = torch.randn(batch_size, 10)\n",
    "    actions = torch.randn(batch_size, 2)\n",
    "    rewards = torch.randn(batch_size, 1)\n",
    "    next_states = torch.randn(batch_size, 10)\n",
    "    dones = torch.zeros(batch_size, 1)\n",
    "    \n",
    "    # Test PPO loss\n",
    "    log_probs_new = torch.randn(batch_size, requires_grad=True)\n",
    "    log_probs_old = log_probs_new.clone().detach()\n",
    "    advantages = torch.randn(batch_size)\n",
    "    ppo_loss_val = ppo_loss(advantages, log_probs_new, log_probs_old)\n",
    "    \n",
    "    print(f\"  • PPO loss: {ppo_loss_val.item():.4f}\")\n",
    "    print(\"✓ Integration test passed\\n\")\nexcept Exception as e:\n",
    "    print(f\"✗ Integration test failed: {e}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALL TESTS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9 - Summary and References\n",
    "\n",
    "### 9.1 Key Takeaways\n",
    "\n",
    "You've now learned the four most important SOTA algorithms in deep reinforcement learning:\n",
    "\n",
    "1. **PPO**: Simple, stable, versatile. Great for getting started.\n",
    "2. **DDPG**: Elegant actor-critic. Foundation for off-policy continuous control.\n",
    "3. **TD3**: Production-ready improvements. Stability matters in practice.\n",
    "4. **SAC**: State-of-the-art. Entropy-based exploration is powerful.\n",
    "\n",
    "### 9.2 Evolution of Ideas\n",
    "\n",
    "The progression shows how research builds on previous work:\n",
    "\n",
    "```\n",
    "Policy Gradients (REINFORCE, A3C)\n",
    "    ↓\n",
    "Trust Region Methods (TRPO)\n",
    "    ↓\n",
    "Simplified Trust Region (PPO) ← On-policy branch\n",
    "    \n",
    "DQN (deep value learning)\n",
    "    ↓\n",
    "Deterministic Actor-Critic (DDPG) ← Off-policy, continuous\n",
    "    ↓\n",
    "Better Stability (TD3) - Add: Double Q, Delayed Updates\n",
    "    ↓\n",
    "Maximum Entropy (SAC) - Add: Stochastic + Auto-tuning\n",
    "```\n",
    "\n",
    "### 9.3 References\n",
    "\n",
    "1. **PPO**: Schulman et al. (2017) - \"Proximal Policy Optimization Algorithms\"\n",
    "   - Paper: https://arxiv.org/abs/1707.06347\n",
    "   - Votes: 3500+ citations, extremely popular in practice\n",
    "\n",
    "2. **DDPG**: Lillicrap et al. (2016) - \"Continuous control with deep reinforcement learning\"\n",
    "   - Paper: https://arxiv.org/abs/1509.02971\n",
    "   - Foundation for continuous control in deep RL\n",
    "\n",
    "3. **TD3**: Fujimoto et al. (2018) - \"Addressing Function Approximation Error in Actor-Critic Methods\"\n",
    "   - Paper: https://arxiv.org/abs/1802.09477\n",
    "   - Shows the importance of addressing overestimation\n",
    "\n",
    "4. **SAC**: Haarnoja et al. (2018) - \"Soft Actor-Critic: Off-Policy Deep Reinforcement Learning\"\n",
    "   - Paper: https://arxiv.org/abs/1801.01290\n",
    "   - Current SOTA for continuous control\n",
    "\n",
    "### 9.4 Additional Resources\n",
    "\n",
    "- **Stable Baselines3**: Professional implementations - https://github.com/DLR-RM/stable-baselines3\n",
    "- **OpenAI Spinning Up**: Excellent tutorials - https://spinningup.openai.com/\n",
    "- **Ray RLlib**: Scalable training - https://docs.ray.io/en/latest/rllib/\n",
    "- **DeepMind Control Suite**: Benchmark environments\n",
    "- **RoboNet**: Large-scale robot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**Congratulations!** You have completed the Advanced SOTA Algorithms in Deep Reinforcement Learning tutorial. \n",
    "\n",
    "You can now:\n",
    "- Implement PPO's clipped objective from scratch\n",
    "- Build DDPG actor-critic architectures\n",
    "- Understand TD3's improvements over DDPG\n",
    "- Implement SAC with automatic temperature tuning\n",
    "- Compare algorithms systematically\n",
    "- Debug and troubleshoot RL implementations\n",
    "\n",
    "**Next Steps:**\n",
    "1. Implement these algorithms in a complete training loop\n",
    "2. Apply them to different environments\n",
    "3. Experiment with hyperparameter tuning\n",
    "4. Read the original papers for deeper understanding\n",
    "5. Contribute to open-source RL projects\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
