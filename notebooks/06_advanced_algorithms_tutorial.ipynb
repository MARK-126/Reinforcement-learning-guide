{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos SOTA Avanzados en Deep Reinforcement Learning\n",
    "\n",
    "## Guía Completa: PPO, DDPG, TD3 y SAC\n",
    "\n",
    "Este notebook explora los algoritmos de Reinforcement Learning más modernos y efectivos, preparándote para meta-learning y aplicaciones avanzadas.\n",
    "\n",
    "**Autores**: MARK-126  \n",
    "**Fecha**: 2024  \n",
    "**Nivel**: Avanzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción a Algoritmos SOTA (3 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ¿Qué son los algoritmos SOTA (State-of-the-Art)?\n",
    "\n",
    "Los algoritmos SOTA representan la frontera actual del conocimiento en Deep RL. Estos algoritmos:\n",
    "\n",
    "1. **PPO (Proximal Policy Optimization)**\n",
    "   - Algoritmo on-policy basado en policy gradient\n",
    "   - Usa clipping para asegurar updates seguros\n",
    "   - Versátil: funciona en espacios discretos y continuos\n",
    "   - Usado en: OpenAI Five, ChatGPT training (RLHF)\n",
    "\n",
    "2. **DDPG (Deep Deterministic Policy Gradient)**\n",
    "   - Primer algoritmo actor-critic para control continuo\n",
    "   - Política determinista + ruido para exploración\n",
    "   - Off-policy: sample efficient\n",
    "   - Base para TD3 y SAC\n",
    "\n",
    "3. **TD3 (Twin Delayed DDPG)**\n",
    "   - Mejora significativa sobre DDPG\n",
    "   - Twin critics reducen sobreestimación\n",
    "   - Delayed policy updates reducen varianza\n",
    "   - Target policy smoothing para robustez\n",
    "\n",
    "4. **SAC (Soft Actor-Critic)**\n",
    "   - Maximum entropy RL framework\n",
    "   - Auto-tuning de temperatura\n",
    "   - Stochastic policy más exploratoria\n",
    "   - SOTA actual para control continuo\n",
    "\n",
    "**Clasificación**:\n",
    "- **On-policy vs Off-policy**: PPO es on-policy; DDPG, TD3, SAC son off-policy\n",
    "- **Deterministic vs Stochastic**: DDPG y TD3 son deterministas; PPO y SAC son estocásticos\n",
    "- **Discrete vs Continuous**: PPO soporta ambas; DDPG, TD3, SAC solo continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comparación Conceptual\n",
    "\n",
    "| Algoritmo | Tipo | Acción | Estabilidad | Exploración | Velocidad | Uso Ideal |\n",
    "|-----------|------|--------|-------------|-------------|-----------|----------|\n",
    "| PPO | On-Policy | Ambas | Media | Entropy | Rápida | Robotics, RL-Training |\n",
    "| DDPG | Off-Policy | Continua | Baja | Noise | Rápida | Baseline |\n",
    "| TD3 | Off-Policy | Continua | Alta | Noise | Media | Benchmark |\n",
    "| SAC | Off-Policy | Continua | Alta | Entropy | Media | SOTA Actual |\n",
    "\n",
    "**Diagrama de Evolución**:\n",
    "```\nPolicy Gradient (REINFORCE)\n    ↓\nA3C (Asynchronous Advantage Actor-Critic)\n    ↓\nTRPO (Trust Region Policy Optimization)\n    ↓\n┌─→ PPO (on-policy, versátil)\n│\n└─→ DQN\n    ├─→ DDPG\n    │   └─→ TD3 (twin critics, delayed updates)\n    │\n    └─→ SAC (maximum entropy)\n```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports generales\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Ambiente preparado:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO: Proximal Policy Optimization Detallado (12 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fundamentos de PPO\n",
    "\n",
    "PPO mejora sobre TRPO eliminando el constraint de KL divergence complejo y usando un objetivo clipped más simple:\n",
    "\n",
    "**Objective de PPO-Clip**:\n",
    "```\nL^CLIP(θ) = E[min(r_t(θ)A_t, clip(r_t(θ), 1-ε, 1+ε)A_t)]\n```\n",
    "\n",
    "donde:\n",
    "- `r_t(θ) = π_θ(a|s) / π_θ_old(a|s)` es el ratio de probabilidades\n",
    "- `A_t` es la ventaja (advantage)\n",
    "- `ε` es el epsilon de clipping (típicamente 0.2)\n",
    "\n",
    "**Clave**: El clipping previene que la política se actualice demasiado rápido, asegurando estabilidad sin requerir KL divergence.\n",
    "\n",
    "**Características**:\n",
    "1. GAE (Generalized Advantage Estimation) para estimar ventajas\n",
    "2. Value function clipping opcional\n",
    "3. Entropy bonus para exploración\n",
    "4. Mini-batch training con múltiples epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar PPO desde el módulo local\n",
    "import sys\n",
    "sys.path.insert(0, '/home/user/Reinforcement-learning-guide')\n",
    "\n",
    "from notebookx03_deep_rl.advanced.ppo import PPOAgent, evaluate_agent as ppo_evaluate, plot_training_results as ppo_plot\n",
    "\n",
    "print(\"PPO Agent importado exitosamente\")\n",
    "print(f\"Métodos disponibles: {[m for m in dir(PPOAgent) if not m.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Entrenamiento PPO en LunarLander\n",
    "\n",
    "LunarLander es un entorno clásico para control continuo:\n",
    "- **Estado**: 8 dimensiones (posición, velocidad, ángulo, etc.)\n",
    "- **Acciones**: 4 acciones discretas (sin motor, motor izq, motor central, motor der)\n",
    "- **Objetivo**: Aterrizar con suavidad\n",
    "- **Recompensa**: Hasta 200 puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear entorno LunarLander\n",
    "env_lunar = gym.make('LunarLander-v2')\n",
    "state_dim = env_lunar.observation_space.shape[0]\n",
    "action_dim = env_lunar.action_space.n\n",
    "\n",
    "print(f\"Entorno: LunarLander-v2\")\n",
    "print(f\"Dimensión de estado: {state_dim}\")\n",
    "print(f\"Dimensión de acciones: {action_dim} (discretas)\")\n",
    "print(f\"Espacio de acciones: {env_lunar.action_space}\")\n",
    "\n",
    "# Crear agente PPO\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    continuous=False,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    epsilon_clip=0.2,\n",
    "    value_clip=0.2,\n",
    "    entropy_coef=0.01,\n",
    "    n_epochs=4,\n",
    "    batch_size=64,\n",
    "    hidden_dims=[64, 64]\n",
    ")\n",
    "\n",
    "print(f\"\\nAgente PPO creado con {sum(p.numel() for p in ppo_agent.actor.parameters())} parámetros (actor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar PPO (versión reducida para demostración)\n",
    "print(\"Iniciando entrenamiento PPO...\")\nhistory_ppo = ppo_agent.train(\n",
    "    env=env_lunar,\n",
    "    n_episodes=100,  # Reducido para tiempo de demostración\n",
    "    max_steps=1000,\n",
    "    update_interval=2048,\n",
    "    print_every=20,\n",
    "    save_every=0  # No guardar para ahorrar espacio\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de PPO\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "rewards = history_ppo['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) > 10:\n",
    "    window = min(20, len(rewards) // 5)\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Recompensas PPO en LunarLander')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Losses\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history_ppo['actor_losses'], label='Actor Loss', alpha=0.6)\n",
    "ax.plot(history_ppo['critic_losses'], label='Critic Loss', alpha=0.6)\n",
    "ax.set_xlabel('Update')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Actor & Critic Losses')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Clipping Fraction\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history_ppo['clip_fractions'], alpha=0.6, label='Clip Fraction')\n",
    "ax.axhline(y=0.2, color='r', linestyle='--', alpha=0.5, label='Target (20%)')\n",
    "ax.set_xlabel('Update')\n",
    "ax.set_ylabel('Fraction')\n",
    "ax.set_title('Clipping Fraction (debería estar cerca de 20%)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropía\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history_ppo['entropies'], alpha=0.6, label='Policy Entropy')\n",
    "ax.set_xlabel('Update')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.set_title('Policy Entropy (exploración)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/ppo_results.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Última recompensa: {rewards[-1]:.2f}\")\n",
    "print(f\"Recompensa promedio (últimas 20): {np.mean(rewards[-20:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Objetivo Clipped de PPO - Análisis Detallado\n",
    "\n",
    "El clipping es lo que diferencia PPO de TRPO. Veamos cómo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el objetivo clipped de PPO\n",
    "epsilon = 0.2\n",
    "ratio = np.linspace(0.5, 1.5, 100)  # Probability ratio\n",
    "advantage = 1.0  # Ventaja positiva\n",
    "\n",
    "# Términos del objetivo\n",
    "surr1 = ratio * advantage  # Sin clipping\n",
    "clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)\n",
    "surr2 = clipped_ratio * advantage  # Con clipping\n",
    "ppo_loss = -np.minimum(surr1, surr2)  # Min de ambos\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Grafo 1: Comparación de términos\n",
    "ax = axes[0]\n",
    "ax.plot(ratio, surr1, label='r(θ) * A (sin clipping)', linewidth=2)\n",
    "ax.plot(ratio, surr2, label='clip(r(θ), 1±ε) * A (con clipping)', linewidth=2)\n",
    "ax.plot(ratio, ppo_loss, label='min(surr1, surr2) [PPO Loss]', linewidth=2.5, linestyle='--')\n",
    "ax.axvline(x=1.0, color='red', linestyle=':', alpha=0.5, label='r = 1 (sin cambio)')\n",
    "ax.axvline(x=1-epsilon, color='green', linestyle=':', alpha=0.5)\n",
    "ax.axvline(x=1+epsilon, color='green', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Probability Ratio r(θ)')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Objetivo Clipped de PPO (Advantage > 0)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafo 2: Importancia del epsilon\n",
    "ax = axes[1]\n",
    "epsilons = [0.1, 0.2, 0.3]\n",
    "for eps in epsilons:\n",
    "    clipped_ratio_eps = np.clip(ratio, 1 - eps, 1 + eps)\n",
    "    surr2_eps = clipped_ratio_eps * advantage\n",
    "    ppo_loss_eps = -np.minimum(surr1, surr2_eps)\n",
    "    ax.plot(ratio, ppo_loss_eps, label=f'ε = {eps}', linewidth=2)\n",
    "ax.axvline(x=1.0, color='red', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Probability Ratio r(θ)')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Efecto de Epsilon en el Clipping')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretación del Clipping:\")\n",
    "print(f\"1. Cuando r(θ) < 1-ε: Política se hace menos probable (bad action) → gradiente cero\")\n",
    "print(f\"2. Cuando 1-ε < r(θ) < 1: Aumentar probabilidad, pero limitado\")\n",
    "print(f\"3. Cuando 1 < r(θ) < 1+ε: Disminuir probabilidad, pero limitado\")\n",
    "print(f\"4. Cuando r(θ) > 1+ε: Política se hace demasiado probable → gradiente cero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar política entrenada\n",
    "print(\"Evaluando política PPO entrenada...\")\nmean_reward, std_reward = ppo_evaluate(ppo_agent, env_lunar, n_episodes=20, render=False)\nprint(f\"\\nRecompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\nprint(f\"Umbral de éxito en LunarLander: 200 puntos\")\nprint(f\"Performance: {'Excelente' if mean_reward > 200 else 'Buena' if mean_reward > 0 else 'Pobre'}\")\n\nenv_lunar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DDPG: Control Determinista Profundo (10 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fundamentos de DDPG\n",
    "\n",
    "DDPG (Deep Deterministic Policy Gradient) fue el primer algoritmo actor-critic para espacios continuos.\n",
    "\n",
    "**Características Clave**:\n",
    "1. **Actor Determinista**: μ(s) - mapea directamente estado a acción\n",
    "2. **Critic Q-learning**: Q(s,a) - estima valor de par estado-acción\n",
    "3. **Experience Replay**: rompe correlación temporal\n",
    "4. **Target Networks**: copias lentas para estabilidad\n",
    "5. **Exploration Noise**: Ornstein-Uhlenbeck o Gaussian\n",
    "\n",
    "**Actualizaciones**:\n",
    "```\nActual loss: Q(s,a) - (r + γQ'(s', μ'(s'))²\nActor loss: -E[Q(s, μ(s))]\n```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar DDPG\n",
    "from notebookx03_deep_rl.advanced.ddpg import DDPGAgent, evaluate_agent as ddpg_evaluate, plot_training_results as ddpg_plot\n",
    "\n",
    "# Crear entorno Pendulum\n",
    "env_pendulum = gym.make('Pendulum-v1')\n",
    "state_dim = env_pendulum.observation_space.shape[0]\n",
    "action_dim = env_pendulum.action_space.shape[0]\n",
    "\n",
    "print(f\"Entorno: Pendulum-v1\")\n",
    "print(f\"Dimensión de estado: {state_dim}\")\n",
    "print(f\"Dimensión de acciones: {action_dim} (continua)\")\n",
    "print(f\"Rango de acciones: [{env_pendulum.action_space.low[0]:.2f}, {env_pendulum.action_space.high[0]:.2f}]\")\n",
    "\n",
    "# Crear agente DDPG\n",
    "ddpg_agent = DDPGAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    tau=0.001,\n",
    "    buffer_size=100000,\n",
    "    batch_size=64,\n",
    "    noise_type='ou',\n",
    "    noise_std=0.2,\n",
    "    hidden_dims=[400, 300]\n",
    ")\n",
    "\n",
    "print(f\"\\nAgente DDPG creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar DDPG\n",
    "print(\"Iniciando entrenamiento DDPG en Pendulum...\")\nhistory_ddpg = ddpg_agent.train(\n",
    "    env=env_pendulum,\n",
    "    n_episodes=50,  # Reducido para demostración\n",
    "    max_steps=200,\n",
    "    warmup_steps=1000,\n",
    "    noise_decay=0.9995,\n",
    "    min_noise=0.1,\n",
    "    print_every=10,\n",
    "    save_every=0\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar DDPG\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "rewards = history_ddpg['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) > 5:\n",
    "    window = min(10, len(rewards) // 5)\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Recompensas DDPG en Pendulum')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Losses\n",
    "ax = axes[0, 1]\n",
    "if history_ddpg['actor_losses']:\n",
    "    ax.plot(history_ddpg['actor_losses'], label='Actor Loss', alpha=0.6)\n",
    "    ax.plot(history_ddpg['critic_losses'], label='Critic Loss', alpha=0.6)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Actor & Critic Losses')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-values\n",
    "ax = axes[1, 0]\n",
    "if history_ddpg['q_values']:\n",
    "    ax.plot(history_ddpg['q_values'], alpha=0.6, label='Mean Q-value')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Q-value')\n",
    "ax.set_title('Estimación de Valor')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Lengths\n",
    "ax = axes[1, 1]\n",
    "lengths = history_ddpg['episode_lengths']\n",
    "ax.plot(lengths, alpha=0.3, label='Episode Length')\n",
    "if len(lengths) > 5:\n",
    "    window = min(10, len(lengths) // 5)\n",
    "    moving_avg = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Length')\n",
    "ax.set_title('Duración de Episodios')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Última recompensa: {rewards[-1]:.2f}\")\nprint(f\"Mejor recompensa: {max(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exploración en DDPG: Ornstein-Uhlenbeck vs Gaussian\n",
    "\n",
    "DDPG requiere ruido para exploración. Comparemos dos tipos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar tipos de ruido\n",
    "from notebookx03_deep_rl.advanced.ddpg import OrnsteinUhlenbeckNoise\n",
    "\n",
    "# Generar ruido OU\n",
    "ou_noise = OrnsteinUhlenbeckNoise(size=1, mu=0.0, theta=0.15, sigma=0.2)\n",
    "ou_samples = [ou_noise.sample()[0] for _ in range(200)]\n",
    "\n",
    "# Generar ruido Gaussiano\n",
    "gaussian_samples = np.random.randn(200) * 0.2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ruido OU\n",
    "ax = axes[0]\n",
    "ax.plot(ou_samples, alpha=0.7, label='OU Noise')\n",
    "ax.set_title('Ornstein-Uhlenbeck Noise (correlacionado temporalmente)')\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('Noise Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Ruido Gaussiano\n",
    "ax = axes[1]\n",
    "ax.plot(gaussian_samples, alpha=0.7, label='Gaussian Noise', color='orange')\n",
    "ax.set_title('Gaussian Noise (independiente)')\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('Noise Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Análisis de Ruido:\")\nprint(f\"\\nOU Noise:\")\nprint(f\"  - Correlación temporal (smooth)\")\nprint(f\"  - Útil para problemas con inercia física\")\nprint(f\"  - Media: {np.mean(ou_samples):.4f}, Std: {np.std(ou_samples):.4f}\")\nprint(f\"\\nGaussian Noise:\")\nprint(f\"  - Independiente, aleatorio puro\")\nprint(f\"  - Más simple, igual de efectivo en muchos casos\")\nprint(f\"  - Media: {np.mean(gaussian_samples):.4f}, Std: {np.std(gaussian_samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar DDPG\n",
    "print(\"Evaluando política DDPG...\")\nmean_reward, std_reward = ddpg_evaluate(ddpg_agent, env_pendulum, n_episodes=10)\nprint(f\"Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\nprint(f\"Objetivo en Pendulum: > -300 puntos\")\n\nenv_pendulum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TD3: Twin Delayed DDPG (10 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tres Mejoras Clave de TD3\n",
    "\n",
    "TD3 resuelve problemas de inestabilidad en DDPG:\n",
    "\n",
    "**1. Twin Q-networks (Clipped Double Q-learning)**\n",
    "```\nTarget = min(Q₁'(s', a'), Q₂'(s', a'))\n```\n",
    "- Reduce sobreestimación que causa inestabilidad\n",
    "\n",
    "**2. Delayed Policy Updates**\n",
    "- Actor se actualiza cada d steps\n",
    "- Critics se actualizan en cada step\n",
    "- Reduce varianza del gradiente de política\n",
    "\n",
    "**3. Target Policy Smoothing**\n",
    "```\na' = clip(μ'(s') + ε, action_min, action_max)\nε ~ N(0, σ²)\n```\n",
    "- Suaviza superficie de Q-value\n",
    "- Más robusto a errores de aproximación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar TD3\n",
    "from notebookx03_deep_rl.advanced.td3 import TD3Agent, evaluate_agent as td3_evaluate\n",
    "\n",
    "# Crear entorno\n",
    "env_td3 = gym.make('Pendulum-v1')\n",
    "state_dim = env_td3.observation_space.shape[0]\n",
    "action_dim = env_td3.action_space.shape[0]\n",
    "max_action = float(env_td3.action_space.high[0])\n",
    "\n",
    "print(f\"Entorno: Pendulum-v1 (para TD3)\")\n",
    "\n",
    "# Crear agente TD3\n",
    "td3_agent = TD3Agent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    max_action=max_action,\n",
    "    actor_lr=3e-4,\n",
    "    critic_lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    policy_noise=0.2,\n",
    "    noise_clip=0.5,\n",
    "    policy_delay=2,\n",
    "    buffer_size=1000000,\n",
    "    batch_size=256,\n",
    "    exploration_noise=0.1,\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "print(f\"\\nAgente TD3 creado con Twin Critics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar TD3\n",
    "print(\"Iniciando entrenamiento TD3...\")\nhistory_td3 = td3_agent.train(\n",
    "    env=env_td3,\n",
    "    n_episodes=50,\n",
    "    max_steps=200,\n",
    "    warmup_steps=1000,\n",
    "    noise_decay=0.999,\n",
    "    min_noise=0.1,\n",
    "    print_every=10,\n",
    "    save_every=0\n",
    ")\n",
    "\n",
    "print(\"Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar TD3\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "rewards = history_td3['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) > 5:\n",
    "    window = min(10, len(rewards) // 5)\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Recompensas TD3')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Losses\n",
    "ax = axes[0, 1]\n",
    "if history_td3['actor_losses'] and history_td3['critic_losses']:\n",
    "    ax.plot(history_td3['actor_losses'], label='Actor Loss', alpha=0.6)\n",
    "    ax.plot(history_td3['critic_losses'], label='Critic Loss', alpha=0.6)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Losses')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Twin Q-values\n",
    "ax = axes[0, 2]\n",
    "if history_td3['q1_values'] and history_td3['q2_values']:\n",
    "    ax.plot(history_td3['q1_values'], label='Q1', alpha=0.6)\n",
    "    ax.plot(history_td3['q2_values'], label='Q2', alpha=0.6)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Q-value')\n",
    "ax.set_title('Twin Q-values (reducen sobreestimación)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Q1 - Q2 (debe ser cercano a 0)\n",
    "ax = axes[1, 0]\n",
    "if history_td3['q1_values'] and history_td3['q2_values']:\n",
    "    q_diff = np.array(history_td3['q1_values']) - np.array(history_td3['q2_values'])\n",
    "    ax.plot(q_diff, alpha=0.6, label='Q1 - Q2')\n",
    "    ax.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Q Difference')\n",
    "ax.set_title('Diferencia Q1-Q2 (convergencia)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Duración\n",
    "ax = axes[1, 1]\n",
    "lengths = history_td3['episode_lengths']\n",
    "ax.plot(lengths, alpha=0.3, label='Episode Length')\n",
    "if len(lengths) > 5:\n",
    "    window = min(10, len(lengths) // 5)\n",
    "    moving_avg = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Length')\n",
    "ax.set_title('Duración de Episodios')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuición de recompensas\n",
    "ax = axes[1, 2]\n",
    "if len(rewards) >= 10:\n",
    "    recent = rewards[-min(30, len(rewards)):]\n",
    "    ax.hist(recent, bins=15, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(np.mean(recent), color='r', linestyle='--', linewidth=2, label=f'Mean: {np.mean(recent):.1f}')\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribución de Recompensas')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Última recompensa: {rewards[-1]:.2f}\")\nprint(f\"Mejor recompensa: {max(rewards):.2f}\")\nprint(f\"Promedio (últimas 10): {np.mean(rewards[-10:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mejoras de TD3 vs DDPG\n",
    "\n",
    "Visualicemos el impacto de las mejoras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar DDPG vs TD3 (mismo número de episodios)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Recompensas comparativas\n",
    "ax = axes[0]\n",
    "ax.plot(history_ddpg['episode_rewards'], label='DDPG', alpha=0.7, linewidth=1.5)\n",
    "ax.plot(history_td3['episode_rewards'], label='TD3', alpha=0.7, linewidth=1.5)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Comparación de Recompensas: DDPG vs TD3')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Estadísticas\n",
    "ax = axes[1]\n",
    "methods = ['DDPG', 'TD3']\n",
    "means = [np.mean(history_ddpg['episode_rewards']), np.mean(history_td3['episode_rewards'])]\n",
    "stds = [np.std(history_ddpg['episode_rewards']), np.std(history_td3['episode_rewards'])]\n",
    "x = np.arange(len(methods))\n",
    "bars = ax.bar(x, means, yerr=stds, capsize=10, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Recompensa Promedio')\n",
    "ax.set_title('Recompensas Promedio y Desviación Estándar')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 5, f'{mean:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"DDPG - Media: {np.mean(history_ddpg['episode_rewards']):.2f}, Std: {np.std(history_ddpg['episode_rewards']):.2f}\")\nprint(f\"TD3  - Media: {np.mean(history_td3['episode_rewards']):.2f}, Std: {np.std(history_td3['episode_rewards']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar TD3\n",
    "print(\"Evaluando política TD3...\")\nmean_reward, std_reward = td3_evaluate(td3_agent, env_td3, n_episodes=10)\nprint(f\"Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\n\nenv_td3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SAC: Soft Actor-Critic (12 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Maximum Entropy Reinforcement Learning\n",
    "\n",
    "SAC introduce el concepto de máxima entropía, maximizando tanto recompensas como incertidumbre:\n",
    "\n",
    "**Objetivo**:\n",
    "```\nJ(π) = E[Σ γᵗ(r_t + αH(π(·|s_t)))]\n```\n",
    "\n",
    "donde:\n",
    "- r_t: recompensa\n",
    "- α: temperature (controlada automáticamente)\n",
    "- H(π): entropía de la política\n",
    "\n",
    "**Ventajas de máxima entropía**:\n",
    "1. Exploración natural\n",
    "2. Políticas multimodales (múltiples soluciones)\n",
    "3. Robustez a cambios en el entorno\n",
    "4. Mejor distanciamiento de política anterior (para fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar SAC\n",
    "from notebookx03_deep_rl.advanced.sac import SACAgent, evaluate_agent as sac_evaluate\n",
    "\n",
    "# Crear entorno\n",
    "env_sac = gym.make('Pendulum-v1')\n",
    "state_dim = env_sac.observation_space.shape[0]\n",
    "action_dim = env_sac.action_space.shape[0]\n",
    "\n",
    "print(f\"Entorno: Pendulum-v1 (para SAC)\")\n",
    "\n",
    "# Crear agente SAC\n",
    "sac_agent = SACAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    actor_lr=3e-4,\n",
    "    critic_lr=3e-4,\n",
    "    alpha_lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    alpha=0.2,\n",
    "    auto_tune=True,\n",
    "    target_entropy=None,  # Será -action_dim\n",
    "    buffer_size=1000000,\n",
    "    batch_size=256,\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "print(f\"\\nAgente SAC creado con auto-tuning de temperatura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar SAC\n",
    "print(\"Iniciando entrenamiento SAC con auto-tuning de α...\")\nhistory_sac = sac_agent.train(\n",
    "    env=env_sac,\n",
    "    n_episodes=50,\n",
    "    max_steps=200,\n",
    "    warmup_steps=1000,\n",
    "    updates_per_step=1,\n",
    "    print_every=10,\n",
    "    save_every=0\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar SAC\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Recompensas\n",
    "ax = axes[0, 0]\n",
    "rewards = history_sac['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) > 5:\n",
    "    window = min(10, len(rewards) // 5)\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Recompensas SAC')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Losses\n",
    "ax = axes[0, 1]\n",
    "if history_sac['actor_losses'] and history_sac['critic_losses']:\n",
    "    ax.plot(history_sac['actor_losses'], label='Actor Loss', alpha=0.6)\n",
    "    ax.plot(history_sac['critic_losses'], label='Critic Loss', alpha=0.6)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Actor & Critic Losses')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Temperatura α\n",
    "ax = axes[0, 2]\n",
    "if history_sac['alphas']:\n",
    "    ax.plot(history_sac['alphas'], alpha=0.6, label='α (auto-tuned)')\n",
    "    ax.axhline(y=0.2, color='r', linestyle='--', alpha=0.3, label='Initial α')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('α')\n",
    "ax.set_title('Auto-tuning de Temperatura')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropía\n",
    "ax = axes[1, 0]\n",
    "if history_sac['entropies']:\n",
    "    ax.plot(history_sac['entropies'], alpha=0.6, label='Policy Entropy')\n",
    "    target_entropy = -action_dim  # Objetivo de entropía\n",
    "    ax.axhline(y=target_entropy, color='r', linestyle='--', alpha=0.3, label=f'Target ({target_entropy:.2f})')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.set_title('Policy Entropy (exploración)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Twin Q-values\n",
    "ax = axes[1, 1]\n",
    "if history_sac['q1_values'] and history_sac['q2_values']:\n",
    "    ax.plot(history_sac['q1_values'], label='Q1', alpha=0.6)\n",
    "    ax.plot(history_sac['q2_values'], label='Q2', alpha=0.6)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Q-value')\n",
    "ax.set_title('Twin Q-values')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Duración\n",
    "ax = axes[1, 2]\n",
    "lengths = history_sac['episode_lengths']\n",
    "ax.plot(lengths, alpha=0.3, label='Episode Length')\n",
    "if len(lengths) > 5:\n",
    "    window = min(10, len(lengths) // 5)\n",
    "    moving_avg = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths)), moving_avg, label=f'MA({window})', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Length')\n",
    "ax.set_title('Duración de Episodios')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Última recompensa: {rewards[-1]:.2f}\")\nprint(f\"Mejor recompensa: {max(rewards):.2f}\")\nprint(f\"Promedio: {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Auto-tuning de Temperatura en SAC\n",
    "\n",
    "SAC ajusta automáticamente α para mantener entropía objetivo. Veamos cómo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar relación entre α, entropía y recompensa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Relación entre α y Entropía\n",
    "ax = axes[0]\n",
    "ax.plot(history_sac['alphas'], label='Temperature α', alpha=0.6, linewidth=2)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(history_sac['entropies'], label='Policy Entropy', color='orange', alpha=0.6, linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('α', color='blue')\n",
    "ax2.set_ylabel('Entropy', color='orange')\n",
    "ax.set_title('Auto-tuning: α controla Entropía')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Scatter plot: α vs Entropía\n",
    "ax = axes[1]\n",
    "if history_sac['alphas'] and history_sac['entropies']:\n",
    "    scatter = ax.scatter(history_sac['alphas'], history_sac['entropies'], \n",
    "                         c=range(len(history_sac['alphas'])), cmap='viridis', alpha=0.6, s=50)\n",
    "    target_entropy = -action_dim\n",
    "    ax.axhline(y=target_entropy, color='r', linestyle='--', alpha=0.5, linewidth=2, label=f'Target entropía ({target_entropy:.2f})')\n",
    "    ax.set_xlabel('Temperature α')\n",
    "    ax.set_ylabel('Policy Entropy')\n",
    "    ax.set_title('Relación entre α y Entropía')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretación del Auto-tuning:\")\nprint(f\"\\n1. Si entropía < objetivo: α aumenta (fomentar exploración)\")\nprint(f\"2. Si entropía > objetivo: α disminuye (enfocarse en recompensas)\")\nprint(f\"3. Temperatura final: {history_sac['alphas'][-1]:.4f}\")\nprint(f\"4. Entropía final: {history_sac['entropies'][-1]:.4f}\")\nprint(f\"5. Entropía objetivo: {-action_dim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar SAC\n",
    "print(\"Evaluando política SAC...\")\nmean_reward, std_reward = sac_evaluate(sac_agent, env_sac, n_episodes=10)\nprint(f\"Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\n\nenv_sac.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparación Completa de los 4 Algoritmos (8 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Análisis de Rendimiento Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para comparación\n",
    "algorithms = {\n",
    "    'PPO': {\n",
    "        'rewards': history_ppo['episode_rewards'],\n",
    "        'color': 'blue',\n",
    "        'type': 'On-policy'\n",
    "    },\n",
    "    'DDPG': {\n",
    "        'rewards': history_ddpg['episode_rewards'],\n",
    "        'color': 'orange',\n",
    "        'type': 'Off-policy (deterministic)'\n",
    "    },\n",
    "    'TD3': {\n",
    "        'rewards': history_td3['episode_rewards'],\n",
    "        'color': 'green',\n",
    "        'type': 'Off-policy (deterministic)'\n",
    "    },\n",
    "    'SAC': {\n",
    "        'rewards': history_sac['episode_rewards'],\n",
    "        'color': 'red',\n",
    "        'type': 'Off-policy (stochastic)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Comparación de recompensas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Trayectorias de entrenamiento\n",
    "ax = axes[0, 0]\n",
    "for algo, data in algorithms.items():\n",
    "    rewards = data['rewards']\n",
    "    ax.plot(rewards, label=algo, color=data['color'], alpha=0.6)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Trayectorias de Entrenamiento')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Media móvil (últimos 10 episodios)\n",
    "ax = axes[0, 1]\n",
    "for algo, data in algorithms.items():\n",
    "    rewards = data['rewards']\n",
    "    window = min(10, len(rewards) // 3)\n",
    "    if window > 1:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(rewards)), moving_avg, label=algo, \n",
    "               color=data['color'], alpha=0.7, linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward (Moving Average)')\n",
    "ax.set_title('Media Móvil (Convergencia)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Estadísticas\n",
    "ax = axes[1, 0]\n",
    "stats_data = {}\n",
    "for algo, data in algorithms.items():\n",
    "    rewards = data['rewards']\n",
    "    stats_data[algo] = {\n",
    "        'mean': np.mean(rewards),\n",
    "        'std': np.std(rewards),\n",
    "        'max': np.max(rewards),\n",
    "        'min': np.min(rewards)\n",
    "    }\n",
    "\n",
    "x = np.arange(len(algorithms))\n",
    "means = [stats_data[algo]['mean'] for algo in algorithms.keys()]\n",
    "stds = [stats_data[algo]['std'] for algo in algorithms.keys()]\n",
    "colors = [data['color'] for data in algorithms.values()]\n",
    "\n",
    "bars = ax.bar(x, means, yerr=stds, capsize=10, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Recompensa Promedio')\n",
    "ax.set_title('Rendimiento Promedio ± Desviación Estándar')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(algorithms.keys())\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 2, f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Box plot\n",
    "ax = axes[1, 1]\n",
    "box_data = [data['rewards'] for data in algorithms.values()]\n",
    "bp = ax.boxplot(box_data, labels=list(algorithms.keys()), patch_artist=True)\n",
    "for patch, data in zip(bp['boxes'], algorithms.values()):\n",
    "    patch.set_facecolor(data['color'])\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Distribución de Recompensas')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Estadísticas de Rendimiento:\")\nprint(\"\\n{:<10} {:<12} {:<12} {:<12} {:<12}\".format(\"Algoritmo\", \"Media\", \"Std\", \"Max\", \"Min\"))\nprint(\"-\" * 58)\nfor algo, stats in stats_data.items():\n    print(\"{:<10} {:<12.2f} {:<12.2f} {:<12.2f} {:<12.2f}\".format(\n        algo, stats['mean'], stats['std'], stats['max'], stats['min']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Tabla Comparativa Detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla comparativa\nimport pandas as pd\n",
    "\ncomparison_data = {\n",
    "    'Algoritmo': ['PPO', 'DDPG', 'TD3', 'SAC'],\n",
    "    'Tipo': ['On-Policy', 'Off-Policy', 'Off-Policy', 'Off-Policy'],\n",
    "    'Acción': ['Discreta/Continua', 'Continua', 'Continua', 'Continua'],\n",
    "    'Política': ['Estocástica', 'Determinista', 'Determinista', 'Estocástica'],\n",
    "    'Estabilidad': ['Media', 'Baja', 'Alta', 'Alta'],\n",
    "    'Exploración': ['Entropy', 'Noise', 'Noise', 'Max Entropy'],\n",
    "    'Sample Eff.': ['Baja', 'Alta', 'Alta', 'Alta'],\n",
    "    'Complejidad': ['Baja', 'Media', 'Media-Alta', 'Alta'],\n",
    "    'Mejor Para': ['Robotics, Vision', 'Baseline', 'Benchmark', 'SOTA Apps'],\n",
    "    'Año': [2017, 2016, 2018, 2018]\n",
    "}\n",
    "\ndf_comparison = pd.DataFrame(comparison_data)\nprint(\"\\nTABLA COMPARATIVA - ALGORITMOS SOTA\")\nprint(\"=\"*100)\nprint(df_comparison.to_string(index=False))\nprint(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Selección de Algoritmo Según Problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de decisión\nprint(\"\\nGUÍA DE SELECCIÓN DE ALGORITMO\")\nprint(\"=\"*80)\n\nselection_guide = {\n    'PPO': {\n        'description': 'Proximal Policy Optimization',\n        'pros': [\n            '✓ Versátil (discreto y continuo)',\n            '✓ Fácil de implementar y tunear',\n            '✓ Estable sin target networks',\n            '✓ SOTA en visión y robotics',\n            '✓ Usado en ChatGPT (RLHF)'\n        ],\n        'cons': [\n            '✗ On-policy: muestra ineficiente',\n            '✗ Requiere muchos samples',\n            '✗ No ideal para off-line RL'\n        ],\n        'use_case': 'Cuando tienes abundancia de data y quieres entrenamiento estable'\n    },\n    'DDPG': {\n        'description': 'Deep Deterministic Policy Gradient',\n        'pros': [\n            '✓ Primer algoritmo para control continuo',\n            '✓ Sample efficient (off-policy)',\n            '✓ Rápido de entrenar',\n            '✓ Buena baseline'\n        ],\n        'cons': [\n            '✗ Inestable en problemas complejos',\n            '✗ Sensible a hiperparámetros',\n            '✗ Puede diverger'\n        ],\n        'use_case': 'Experimentos iniciales, problemas simples continuos'\n    },\n    'TD3': {\n        'description': 'Twin Delayed DDPG',\n        'pros': [\n            '✓ Más estable que DDPG',\n            '✓ Mejoras demostradas empíricamente',\n            '✓ Buen benchmark',\n            '✓ Twin critics reducen sesgo'\n        ],\n        'cons': [\n            '✗ Más complejo que DDPG',\n            '✗ Más lento que DDPG',\n            '✗ Todavía puede ser inestable'\n        ],\n        'use_case': 'Comparación de baselines, cuando necesitas mejor rendimiento que DDPG'\n    },\n    'SAC': {\n        'description': 'Soft Actor-Critic',\n        'pros': [\n            '✓ SOTA actual para control continuo',\n            '✓ Auto-tuning de temperatura',\n            '✓ Robusto y estable',\n            '✓ Excelente para aplicaciones reales',\n            '✓ Políticas exploratorias'\n        ],\n        'cons': [\n            '✗ Más complejo de implementar',\n            '✗ Más parámetros a tunear',\n            '✗ Computacionalmente más costoso'\n        ],\n        'use_case': 'Aplicaciones en producción, cuando necesitas lo mejor disponible'\n    }\n}\n\nfor algo, info in selection_guide.items():\n    print(f\"\\n{algo}: {info['description']}\")\n    print(\"-\" * 80)\n    print(\"\\nVentajas:\")\n    for pro in info['pros']:\n        print(f\"  {pro}\")\n    print(\"\\nDesventajas:\")\n    for con in info['cons']:\n        print(f\"  {con}\")\n    print(f\"\\nCuándo usar: {info['use_case']}\")\n    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Convergencia y Dinamics de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar velocidad de convergencia\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# 1. Tasa de mejora\nax = axes[0, 0]\nfor algo, data in algorithms.items():\n",
    "    rewards = data['rewards']\n",
    "    # Calcular mejora acumulativa\n",
    "    improvement = np.cumsum(np.diff(rewards, prepend=rewards[0]))\n",
    "    ax.plot(improvement, label=algo, color=data['color'], alpha=0.7, linewidth=1.5)\nax.set_xlabel('Episode')\nax.set_ylabel('Cumulative Improvement')\nax.set_title('Tasa de Mejora Acumulativa')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 2. Velocidad de convergencia a la media\nax = axes[0, 1]\nfor algo, data in algorithms.items():\n",
    "    rewards = np.array(data['rewards'])\n",
    "    final_mean = np.mean(rewards[-10:])\n",
    "    # Distancia a la media final\n",
    "    distance = np.abs(rewards - final_mean)\n",
    "    # Suavizar\n",
    "    window = max(1, len(distance) // 10)\n",
    "    smoothed = np.convolve(distance, np.ones(window)/window, mode='same')\n",
    "    ax.plot(smoothed, label=algo, color=data['color'], alpha=0.7, linewidth=1.5)\nax.set_xlabel('Episode')\nax.set_ylabel('|Reward - Final Mean|')\nax.set_title('Convergencia a Media Final')\nax.set_yscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 3. Variabilidad (rolling std)\nax = axes[1, 0]\nfor algo, data in algorithms.items():\n",
    "    rewards = data['rewards']\n",
    "    window = max(1, len(rewards) // 10)\n",
    "    rolling_std = pd.Series(rewards).rolling(window=window, center=True).std().values\n",
    "    ax.plot(rolling_std, label=algo, color=data['color'], alpha=0.7, linewidth=1.5)\nax.set_xlabel('Episode')\nax.set_ylabel('Rolling Std Dev')\nax.set_title('Variabilidad (Rolling Std Dev)')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 4. Cambio de episodio a episodio\nax = axes[1, 1]\nfor algo, data in algorithms.items():\n",
    "    rewards = np.array(data['rewards'])\n",
    "    changes = np.abs(np.diff(rewards))\n",
    "    # Suavizar\n",
    "    window = max(1, len(changes) // 10)\n",
    "    smoothed = np.convolve(changes, np.ones(window)/window, mode='same')\n",
    "    ax.plot(smoothed, label=algo, color=data['color'], alpha=0.7, linewidth=1.5)\nax.set_xlabel('Episode')\nax.set_ylabel('|Δ Reward|')\nax.set_title('Cambios Episodio-a-Episodio (Volatilidad)')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Análisis de Convergencia:\")\nprint(\"\\nVelocidad de convergencia (menor es mejor):\")\nfor algo, data in algorithms.items():\n",
    "    rewards = np.array(data['rewards'])\n",
    "    final_mean = np.mean(rewards[-10:])\n",
    "    # Episodios hasta alcanzar 90% de la media final\n",
    "    threshold = final_mean * 0.9\n",
    "    reached_idx = np.where(rewards > threshold)[0]\n",
    "    if len(reached_idx) > 0:\n",
    "        episodes_to_converge = reached_idx[0]\n",
    "    else:\n",
    "        episodes_to_converge = len(rewards)\n",
    "    print(f\"{algo}: ~{episodes_to_converge} episodios para alcanzar 90% de media final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejercicios Prácticos (5 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Ejercicio 1: Ajuste de Hiperparámetros\n",
    "\n",
    "**Objetivo**: Experimentar con hiperparámetros de PPO para mejorar rendimiento en LunarLander\n",
    "\n",
    "**Tarea**: \n",
    "1. Crea un agente PPO con diferentes valores de `epsilon_clip` (0.1, 0.2, 0.3)\n",
    "2. Entrena cada uno por 50 episodios\n",
    "3. Compara sus recompensas finales\n",
    "4. ¿Cuál es el mejor valor? ¿Por qué?\n",
    "\n",
    "**Hints**:\n",
    "- epsilon_clip controla cuánto puede cambiar la política por update\n",
    "- Valores más altos = cambios más grandes\n",
    "- Trade-off entre velocidad y estabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Tu código aquí\nprint(\"Ejercicio 1: Ajuste de Hiperparámetros en PPO\")\nprint(\"=\"*50)\nprint(\"\\nImplementa una búsqueda de hiperparámetros para epsilon_clip\")\nprint(\"\\n# TODO: Modifica los valores de epsilon_clip y entrena\")\nprint(\"# TODO: Compara los resultados\")\n\n# Solución plantilla:\nepsilon_values = [0.1, 0.2, 0.3]\nresults = {}\n\nfor eps in epsilon_values:\n    print(f\"\\nEntrenando con epsilon_clip={eps}...\")\n    # env = gym.make('LunarLander-v2')\n    # agent = PPOAgent(..., epsilon_clip=eps, ...)\n    # history = agent.train(...)\n    # results[eps] = np.mean(history['episode_rewards'][-10:])\n    # print(f\"Recompensa media (últimas 10): {results[eps]:.2f}\")\n\nprint(\"\\n# Análisis: ¿Cuál fue el mejor epsilon_clip?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ejercicio 2: Comparación de Políticas (Determinista vs Estocástica)\n",
    "\n",
    "**Objetivo**: Entender diferencias entre políticas deterministas y estocásticas\n",
    "\n",
    "**Tarea**:\n",
    "1. Toma un agente DDPG (determinista) entrenado\n",
    "2. Ejecuta 10 episodios y registra las acciones\n",
    "3. Compara con SAC (estocástica) ejecutando 10 episodios\n",
    "4. ¿Cuáles son las diferencias en variabilidad de acciones?\n",
    "\n",
    "**Análisis**:\n",
    "- Determinista: misma acción para mismo estado\n",
    "- Estocástica: variación incluso para mismo estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: Comparar Determinista vs Estocástica\nprint(\"Ejercicio 2: Políticas Deterministas vs Estocásticas\")\nprint(\"=\"*50)\nprint(\"\\n# TODO: Extrae acciones de DDPG y SAC para el mismo estado\")\nprint(\"# TODO: Visualiza la variabilidad\")\nprint(\"# TODO: Analiza las diferencias\")\n\nprint(\"\\nObservaciones esperadas:\")\nprint(\"- DDPG: acciones muy similares para repetir el mismo estado\")\nprint(\"- SAC: acciones varían incluso para el mismo estado\")\nprint(\"- SAC es más exploratoria naturalmente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Ejercicio 3: Eficiencia de Muestras (Sample Efficiency)\n",
    "\n",
    "**Objetivo**: Comparar eficiencia de muestras entre algoritmos\n",
    "\n",
    "**Tarea**:\n",
    "1. Para cada algoritmo, registra el número total de steps de ambiente ejecutados\n",
    "2. Calcula: recompensa / número de steps\n",
    "3. ¿Cuál es más eficiente?\n",
    "4. ¿Por qué algunos son más eficientes que otros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3: Sample Efficiency\nprint(\"Ejercicio 3: Eficiencia de Muestras\")\nprint(\"=\"*50)\n\ntotal_steps = {\n    'PPO': sum(history_ppo['episode_lengths']),\n",
    "    'DDPG': sum(history_ddpg['episode_lengths']),\n",
    "    'TD3': sum(history_td3['episode_lengths']),\n",
    "    'SAC': sum(history_sac['episode_lengths'])\n}\n\nfinal_rewards = {\n",
    "    'PPO': np.mean(history_ppo['episode_rewards'][-10:]),\n",
    "    'DDPG': np.mean(history_ddpg['episode_rewards'][-10:]),\n",
    "    'TD3': np.mean(history_td3['episode_rewards'][-10:]),\n",
    "    'SAC': np.mean(history_sac['episode_rewards'][-10:])\n",
    "}\n",
    "\nprint(\"\\nEficiencia de Muestras (Reward / Steps):\")\nprint(\"-\" * 50)\nfor algo in ['PPO', 'DDPG', 'TD3', 'SAC']:\n",
    "    efficiency = final_rewards[algo] / total_steps[algo] if total_steps[algo] > 0 else 0\n",
    "    print(f\"{algo:5} - Steps: {total_steps[algo]:5}, Reward: {final_rewards[algo]:7.2f}, Efficiency: {efficiency:.6f}\")\n\nprint(\"\\n# Análisis:\")\nprint(\"# On-policy (PPO) vs Off-policy (DDPG, TD3, SAC)\")\nprint(\"# Off-policy reutiliza experiencias del replay buffer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Ejercicio 4: Transferencia de Política (Meta-Learning Prep)\n",
    "\n",
    "**Objetivo**: Preparación para meta-learning - transferir política\n",
    "\n",
    "**Tarea**:\n",
    "1. Entrena un agente PPO en LunarLander\n",
    "2. Guarda los pesos\n",
    "3. Crea un nuevo agente con los mismos pesos\n",
    "4. Fine-tunea en un entorno similar (CartPole)\n",
    "5. Compara con entrenamiento desde cero\n",
    "\n",
    "**Concepto**: Meta-learning = aprender a aprender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4: Transfer Learning / Meta-Learning Prep\nprint(\"Ejercicio 4: Transferencia de Política (Prep para Meta-Learning)\")\nprint(\"=\"*60)\n\nprint(\"\\n# TODO: Guarda los pesos del agente entrenado\")\nprint(\"# TODO: Crea un nuevo agente y carga los pesos\")\nprint(\"# TODO: Fine-tunea en un entorno nuevo\")\nprint(\"# TODO: Compara velocidad de aprendizaje\")\n\nprint(\"\\nCódigo de ejemplo:\")\nprint(\"\"\"\n# Guardar\nppo_agent.save('ppo_lunarlander.pth')\n\n# Cargar en nuevo agente\nppo_agent_new = PPOAgent(...)\nppo_agent_new.load('ppo_lunarlander.pth')\n\n# Fine-tune en CartPole\nhistory_transfer = ppo_agent_new.train(env_cartpole, n_episodes=50, ...)\n\n# Comparar con entrenamiento desde cero\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Ejercicio 5: Diagnóstico de Problemas\n",
    "\n",
    "**Objetivo**: Aprender a diagnosticar problemas comunes en RL\n",
    "\n",
    "**Problemas comunes y soluciones**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 5: Diagnóstico de Problemas\nprint(\"Ejercicio 5: Diagnóstico de Problemas Comunes en RL\")\nprint(\"=\"*60)\n\ndiagnosis = {\n    'Problema': [\n        'Recompensas no mejoran',\n        'Loss sube y sube',\n        'Política diverge',\n        'Entrenamiento muy lento',\n        'Q-values muy negativos',\n        'Mucho ruido en recompensas'\n    ],\n    'Síntoma': [\n        'rewards = constante plana',\n        'loss > 100 y creciendo',\n        'gradientes NaN o infinitos',\n        'poca mejora en 1000+ steps',\n        'q_values < -1000',\n        'alta varianza en rewards'\n    ],\n    'Posible Causa': [\n        'Entorno demasiado complejo / learning rate bajo',\n        'Reward scaling / gradient explosion',\n        'Batch norm issues / learning rate alto',\n        'Hiperparámetros conservadores',\n        'Reward scaling / entorno mal normalizado',\n        'Policy exploration insuficiente'\n    ],\n    'Solución': [\n        'Aumentar learning rate / Simplificar entorno / Reward shaping',\n        'Reducir learning rate / Normalizar rewards / Gradient clipping',\n        'Reducir lr / Usar batch norm / Gradient clipping',\n        'Aumentar batch size / lr / epsilon_clip / entropy coef',\n        'Normalizar rewards / Usar smaller action magnitudes',\n        'Aumentar entropy coef / Exploration noise / Target entropy'\n    ]\n}\n\nimport pandas as pd\ndf_diagnosis = pd.DataFrame(diagnosis)\nprint(\"\\n\" + df_diagnosis.to_string(index=False))\n\nprint(\"\\n\\nTu turno: ¿Qué diagnóstico harías para cada caso?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones y Próximos Pasos (4 celdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Resumen de Conceptos Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESUMEN DE CONCEPTOS CLAVE\")\nprint(\"=\"*80)\n\nsummary = {\n    \"PPO\": {\n        \"Insight Principal\": \"Clipping previene updates demasiado grandes\",\n        \"Ecuación Clave\": \"L = E[min(rA, clip(r,1±ε)A)]\",\n        \"Fortaleza\": \"Versatilidad y estabilidad\",\n        \"Debilidad\": \"On-policy: muestra ineficiente\",\n        \"Casos de Uso\": \"Robotics, Vision, RLHF\"\n    },\n    \"DDPG\": {\n        \"Insight Principal\": \"Actor determinista + ruido = exploración eficiente\",\n        \"Ecuación Clave\": \"Actor: -E[Q(s, μ(s))]\",\n        \"Fortaleza\": \"Primer algoritmo para control continuo\",\n        \"Debilidad\": \"Inestable, sensible a hiperparámetros\",\n        \"Casos de Uso\": \"Baseline, problemas simples\"\n    },\n    \"TD3\": {\n        \"Insight Principal\": \"Twin critics + delayed updates = robustez\",\n        \"Ecuación Clave\": \"Target = min(Q₁', Q₂')\",\n        \"Fortaleza\": \"Estable y robusto\",\n        \"Debilidad\": \"Más complejo, más lento\",\n        \"Casos de Uso\": \"Benchmarks, comparación de baselines\"\n    },\n    \"SAC\": {\n        \"Insight Principal\": \"Máxima entropía = exploración + robustez automática\",\n        \"Ecuación Clave\": \"J = E[r + αH(π)] con α auto-tuning\",\n        \"Fortaleza\": \"SOTA, auto-tuning, robusto\",\n        \"Debilidad\": \"Más parámetros, computacionalmente costoso\",\n        \"Casos de Uso\": \"Aplicaciones en producción, SOTA\"\n    }\n}\n\nfor algo, props in summary.items():\n    print(f\"\\n{algo}\")\n    print(\"-\" * 80)\n    for key, value in props.items():\n        print(f\"  {key:<25}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Tendencias Futuras y Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FUTURO DEL APRENDIZAJE POR REFUERZO\")\nprint(\"=\"*80)\n\nfuture_topics = {\n    \"Meta-Learning\": {\n        \"Descripción\": \"Aprender a aprender - quick adaptation a nuevas tareas\",\n        \"Métodos\": [\"MAML (Model-Agnostic Meta-Learning)\", \"Few-shot RL\", \"Multi-task RL\"],\n        \"Relevancia\": \"Crítica para sistemas generales\"\n    },\n    \"Offline RL\": {\n        \"Descripción\": \"Aprender de datos recolectados sin interacción online\",\n        \"Métodos\": [\"Conservative Q-learning\", \"IQL\", \"CQL\"],\n        \"Relevancia\": \"Esencial para aplicaciones reales\"\n    },\n    \"Hierarchical RL\": {\n        \"Descripción\": \"Aprendizaje con abstracciones de múltiples niveles\",\n        \"Métodos\": [\"HRL\", \"Options Framework\", \"Feudal Networks\"],\n        \"Relevancia\": \"Para problemas complejos a largo plazo\"\n    },\n    \"Model-Based RL\": {\n        \"Descripción\": \"Aprender modelos del entorno para mejor planning\",\n        \"Métodos\": [\"Dreamer\", \"PlaNet\", \"Latent-Space Models\"],\n        \"Relevancia\": \"Mejor sample efficiency\"\n    },\n    \"Multi-Agent RL\": {\n        \"Descripción\": \"Múltiples agentes compitiendo o cooperando\",\n        \"Métodos\": [\"QMIX\", \"MADDPG\", \"CommNet\"],\n        \"Relevancia\": \"Sistemas complejos y distribuidos\"\n    },\n    \"Vision + RL\": {\n        \"Descripción\": \"Aprendizaje de políticas a partir de imágenes\",\n        \"Métodos\": [\"DrQ\", \"SLAC\", \"Data Augmentation\"],\n        \"Relevancia\": \"Problemas de visión real\"\n    }\n}\n\nfor topic, info in future_topics.items():\n    print(f\"\\n{topic}\")\n    print(\"-\" * 80)\n    print(f\"  Descripción: {info['Descripción']}\")\n    print(f\"  Métodos: {', '.join(info['Métodos'])}\")\n    print(f\"  Relevancia: {info['Relevancia']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Guía de Implementación: Próximos Pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUTA DE APRENDIZAJE RECOMENDADA\")\nprint(\"=\"*80)\n\nruta = {\n    \"Fase 1: Fundamentos (1-2 semanas)\": {\n        \"Objetivo\": \"Entender conceptos básicos\",\n        \"Actividades\": [\n            \"Implementa PPO desde cero\",\n            \"Experimenta con diferentes entornos\",\n            \"Entiende GAE y clipping\",\n            \"Visualiza training dynamics\"\n        ],\n        \"Recursos\": \"Notebooks 01-05 del repositorio\"\n    },\n    \"Fase 2: Algoritmos Avanzados (2-3 semanas)\": {\n        \"Objetivo\": \"Dominar SOTA algorithms\",\n        \"Actividades\": [\n            \"Estudia DDPG, TD3, SAC en profundidad\",\n            \"Implementa modificaciones (tuning, arquitecturas)\",\n            \"Compara rendimiento sistemáticamente\",\n            \"Entiende trade-offs\"\n        ],\n        \"Recursos\": \"Este notebook + papers originales\"\n    },\n    \"Fase 3: Aplicaciones (3-4 semanas)\": {\n        \"Objetivo\": \"Aplicar a problemas reales\",\n        \"Actividades\": [\n            \"Robótica simulada (MuJoCo, PyBullet)\",\n            \"Entornos con imágenes (Atari, Control Suite)\",\n            \"Fine-tuning y transfer learning\",\n            \"Debugging y diagnostics\"\n        ],\n        \"Recursos\": \"Entornos de OpenAI + documentación\"\n    },\n    \"Fase 4: Meta-Learning (4-6 semanas)\": {\n        \"Objetivo\": \"Algoritmos que aprenden a aprender\",\n        \"Actividades\": [\n            \"MAML para quick adaptation\",\n            \"Few-shot learning en RL\",\n            \"Multi-task RL\",\n            \"Proyectos integradores\"\n        ],\n        \"Recursos\": \"Papers de meta-learning + implementaciones\"\n    }\n}\n\nfor fase, info in ruta.items():\n    print(f\"\\n{fase}\")\n    print(\"-\" * 80)\n    print(f\"Objetivo: {info['Objetivo']}\")\n    print(f\"Actividades:\")\n    for act in info['Actividades']:\n        print(f\"  - {act}\")\n    print(f\"Recursos: {info['Recursos']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Referencias y Recursos Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REFERENCIAS Y RECURSOS\")\nprint(\"=\"*80)\n\nresources = {\n    \"Papers Clave\": [\n        \"[1] Schulman et al. (2017) - PPO: https://arxiv.org/abs/1707.06347\",\n        \"[2] Lillicrap et al. (2016) - DDPG: https://arxiv.org/abs/1509.02971\",\n        \"[3] Fujimoto et al. (2018) - TD3: https://arxiv.org/abs/1802.09477\",\n        \"[4] Haarnoja et al. (2018) - SAC: https://arxiv.org/abs/1801.01290\"\n    ],\n    \"Librerías Recomendadas\": [\n        \"OpenAI Gym / Gymnasium: entornos estándar\",\n        \"PyTorch / TensorFlow: redes neuronales\",\n        \"Stable-Baselines3: implementaciones de referencia\",\n        \"RLlib: entrenamiento distribuido\",\n        \"Acme: frameworks modulares\"\n    ],\n    \"Datasets y Benchmarks\": [\n        \"Atari 2600: juegos clásicos\",\n        \"MuJoCo: control continuo\",\n        \"DeepMind Control Suite: robótica\",\n        \"D4RL: offline RL benchmarks\",\n        \"Procedural Generation: entornos infinitos\"\n    ],\n    \"Comunidad y Eventos\": [\n        \"OpenAI Spinning Up: cursos libres\",\n        \"ICML, ICLR, NeurIPS: conferencias principales\",\n        \"RL Discord Communities: comunidad activa\",\n        \"Hugging Face Hub: modelos pre-entrenados\",\n        \"Kaggle Competitions: desafíos prácticos\"\n    ]\n}\n\nfor category, items in resources.items():\n    print(f\"\\n{category}\")\n    print(\"-\" * 80)\n    for item in items:\n        print(f\"  {item}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\n¡Felicitaciones! Has completado el tutorial de Algoritmos SOTA en Deep RL.\")\nprint(\"\\nPróximos pasos:\")\nprint(\"  1. Implementa tus propias versiones desde cero\")\nprint(\"  2. Experimenta con nuevos entornos\")\nprint(\"  3. Lee los papers originales\")\nprint(\"  4. Contribuye a la comunidad open-source\")\nprint(\"  5. ¡Sigue aprendiendo y explorando!\")\nprint(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
