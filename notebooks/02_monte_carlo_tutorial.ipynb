{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial Completo: M\u00e9todos Monte Carlo en Reinforcement Learning",
        "",
        "## Objetivos de Aprendizaje",
        "",
        "En este notebook aprender\u00e1s:",
        "1. Los fundamentos matem\u00e1ticos de los m\u00e9todos Monte Carlo",
        "2. La diferencia entre First-Visit y Every-Visit MC",
        "3. MC Prediction para evaluaci\u00f3n de pol\u00edticas",
        "4. MC Control On-Policy con \u03b5-greedy",
        "5. MC Control Off-Policy con Importance Sampling",
        "6. Aplicaciones pr\u00e1cticas: Blackjack, GridWorld, Cliff Walking",
        "",
        "**Duraci\u00f3n estimada**: 3-4 horas",
        "",
        "**Prerequisitos**: Haber completado el notebook de Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import pandas as pd",
        "from collections import defaultdict",
        "import warnings",
        "warnings.filterwarnings(\"ignore\")",
        "",
        "# Configuraci\u00f3n de visualizaci\u00f3n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")",
        "sns.set_palette(\"husl\")",
        "%matplotlib inline",
        "",
        "# Importar nuestras implementaciones",
        "sys.path.append(\"/home/user/Reinforcement-learning-guide\")",
        "",
        "from algoritmos_clasicos.monte_carlo.mc_prediction import (",
        "    MCPrediction, SimpleGridWorld, SimpleBlackjack",
        ")",
        "from algoritmos_clasicos.monte_carlo.mc_control import (",
        "    MCControlOnPolicy, MCControlOffPolicy, CliffWalking",
        ")",
        "",
        "print(\"\u2713 Importaciones exitosas\")",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introducci\u00f3n Te\u00f3rica",
        "",
        "### \u00bfQu\u00e9 son los M\u00e9todos Monte Carlo?",
        "",
        "Los **m\u00e9todos Monte Carlo (MC)** son una familia de algoritmos de aprendizaje por refuerzo que aprenden directamente de la experiencia sin necesidad de un modelo del entorno. A diferencia de la Programaci\u00f3n Din\u00e1mica, MC aprende valores mediante el **promedio de retornos reales** observados en episodios completos.",
        "",
        "### Caracter\u00edsticas Clave:",
        "",
        "1. **Model-Free**: No requieren conocer las probabilidades de transici\u00f3n $p(s',r|s,a)$",
        "2. **Basados en episodios**: Aprenden al finalizar episodios completos",
        "3. **Muestreo**: Usan experiencia real o simulada para estimar valores",
        "4. **Sin bootstrap**: No usan estimaciones de otros estados para actualizar",
        "5. **Unbiased estimates**: Las estimaciones convergen al valor verdadero",
        "",
        "### Diferencias con Dynamic Programming:",
        "",
        "| Aspecto | Dynamic Programming | Monte Carlo |",
        "|---------|-------------------|-------------|",
        "| **Modelo** | Requiere modelo completo | No requiere modelo (model-free) |",
        "| **Actualizaci\u00f3n** | Bootstrapping (usa V(s')) | Usa retornos reales completos |",
        "| **Convergencia** | Por sweep completo | Por episodio |",
        "| **Aplicabilidad** | Solo entornos discretos con modelo | Cualquier entorno epis\u00f3dico |",
        "| **Eficiencia** | Alta con modelo perfecto | M\u00e1s eficiente en pr\u00e1ctica sin modelo |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model-Free Learning",
        "",
        "Monte Carlo es el primer m\u00e9todo verdaderamente **model-free** que estudiaremos:",
        "",
        "**\u00bfQu\u00e9 significa model-free?**",
        "- No necesitamos las din\u00e1micas del entorno: $p(s',r|s,a)$",
        "- Aprendemos directamente de la interacci\u00f3n o simulaci\u00f3n",
        "- \u00datil cuando el modelo es desconocido o demasiado complejo",
        "",
        "**Ejemplo intuitivo:**",
        "Imagina aprender a jugar Blackjack:",
        "- **DP**: Necesitar\u00edas calcular todas las probabilidades de cada carta",
        "- **MC**: Simplemente juegas muchas manos y promedias tus ganancias",
        "",
        "### Ventajas de Monte Carlo:",
        "",
        "1. **Flexibilidad**: Funciona con cualquier entorno simulable",
        "2. **Simplicidad conceptual**: \"Intenta y promedia\"",
        "3. **Foco en estados importantes**: Solo visita estados alcanzables",
        "4. **Bajo sesgo**: Estimaciones no sesgadas (unbiased)",
        "5. **Paralelizable**: Los episodios son independientes",
        "",
        "### Desventajas:",
        "",
        "1. **Alta varianza**: Los retornos pueden variar mucho",
        "2. **Solo epis\u00f3dico**: Requiere episodios que terminen",
        "3. **Lento para converger**: Necesita muchos episodios",
        "4. **Ineficiente para estados raros**: Requiere visitarlos repetidamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aplicaciones Pr\u00e1cticas:",
        "",
        "- **Juegos**: Blackjack, Go, Poker (donde el modelo es complejo)",
        "- **Simulaci\u00f3n de sistemas**: Cuando podemos simular pero no modelar anal\u00edticamente",
        "- **Rob\u00f3tica**: Aprender de episodios de interacci\u00f3n real",
        "- **Finanzas**: Valoraci\u00f3n de opciones mediante simulaci\u00f3n",
        "- **Planificaci\u00f3n**: Problemas donde solo podemos samplear trayectorias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparaci\u00f3n Intuitiva: DP vs MC",
        "",
        "**Programaci\u00f3n Din\u00e1mica (DP)**:",
        "```",
        "Para cada estado s:",
        "    V(s) = \u03a3 p(s'|s,a)[r + \u03b3V(s')]  \u2190 Usa MODELO",
        "           \u2191",
        "    Necesita conocer todas las transiciones posibles",
        "```",
        "",
        "**Monte Carlo (MC)**:",
        "```",
        "Para cada estado s:",
        "    1. Genera episodios siguiendo pol\u00edtica \u03c0",
        "    2. Observa returns reales G\u2081, G\u2082, G\u2083, ...",
        "    3. V(s) = promedio(G\u2081, G\u2082, G\u2083, ...)  \u2190 Usa EXPERIENCIA",
        "```",
        "",
        "**Analog\u00eda del mundo real:**",
        "- **DP**: Estudiar un mapa completo antes de viajar",
        "- **MC**: Viajar muchas veces y aprender del camino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fundamentos Matem\u00e1ticos",
        "",
        "### 2.1 El Concepto de Return (Retorno)",
        "",
        "El **return** o **retorno** $G_t$ es la suma descontada de recompensas futuras desde el tiempo $t$:",
        "",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$",
        "",
        "Donde:",
        "- $G_t$: Retorno desde el tiempo $t$",
        "- $R_{t+1}$: Recompensa inmediata",
        "- $\\gamma \\in [0,1]$: Factor de descuento",
        "- $k$: Pasos hacia el futuro",
        "",
        "**Propiedad recursiva:**",
        "",
        "$$G_t = R_{t+1} + \\gamma G_{t+1}$$",
        "",
        "Esta propiedad es fundamental para el c\u00e1lculo eficiente de retornos.",
        "",
        "### 2.2 Estimaci\u00f3n de la Funci\u00f3n de Valor",
        "",
        "La funci\u00f3n de valor de un estado bajo una pol\u00edtica $\\pi$ se define como:",
        "",
        "$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s]$$",
        "",
        "**Idea clave de Monte Carlo**: Estimar $V^\\pi(s)$ mediante el **promedio emp\u00edrico** de retornos observados:",
        "",
        "$$V(s) \u0007pprox \frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i(s)$$",
        "",
        "Donde:",
        "- $N(s)$: N\u00famero de veces que visitamos $s$",
        "- $G_i(s)$: Retorno observado en la i-\u00e9sima visita a $s$",
        "",
        "**Por la Ley de Grandes N\u00fameros**: Cuando $N(s) \to \\infty$, $V(s) \to V^\\pi(s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 First-Visit vs Every-Visit Monte Carlo",
        "",
        "Cuando un episodio visita el mismo estado m\u00faltiples veces, hay dos formas de procesar los retornos:",
        "",
        "#### First-Visit MC:",
        "",
        "Solo considera la **primera vez** que se visita un estado en cada episodio.",
        "",
        "$$V(s) = \\text{promedio}\\{G_t \\mid S_t = s, \\text{ y } t \\text{ es la primera visita a } s \\text{ en el episodio}\\}$$",
        "",
        "**Propiedades:**",
        "- Cada episodio contribuye m\u00e1ximo una muestra por estado",
        "- Estimador insesgado (unbiased)",
        "- Garantiza convergencia a $V^\\pi(s)$ cuando $n \to \\infty$",
        "",
        "#### Every-Visit MC:",
        "",
        "Considera **todas las visitas** a un estado en cada episodio.",
        "",
        "$$V(s) = \\text{promedio}\\{G_t \\mid S_t = s\\}$$",
        "",
        "**Propiedades:**",
        "- Cada episodio puede contribuir m\u00faltiples muestras por estado",
        "- Tambi\u00e9n es insesgado asint\u00f3ticamente",
        "- Puede converger m\u00e1s r\u00e1pido en algunos casos",
        "- M\u00e1s datos por episodio",
        "",
        "#### Comparaci\u00f3n:",
        "",
        "| Aspecto | First-Visit | Every-Visit |",
        "|---------|------------|-------------|",
        "| **Muestras por episodio** | Una por estado | M\u00faltiples posibles |",
        "| **Sesgo** | Insesgado | Insesgado |",
        "| **Varianza** | Mayor | Menor (m\u00e1s datos) |",
        "| **Convergencia** | Garantizada | Garantizada |",
        "| **Uso t\u00edpico** | M\u00e1s com\u00fan te\u00f3ricamente | M\u00e1s com\u00fan en pr\u00e1ctica |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Ejemplo Num\u00e9rico: C\u00e1lculo de Returns",
        "",
        "Consideremos un episodio simple con $\\gamma = 0.9$:",
        "",
        "**Episodio**: $S_0 \\xrightarrow{R_1=5} S_1 \\xrightarrow{R_2=2} S_2 \\xrightarrow{R_3=-1} S_3$ (terminal)",
        "",
        "**C\u00e1lculo de retornos (hacia atr\u00e1s):**",
        "",
        "Para $t=2$ (\u00faltimo paso antes de terminal):",
        "$$G_2 = R_3 = -1$$",
        "",
        "Para $t=1$:",
        "$$G_1 = R_2 + \\gamma G_2 = 2 + 0.9 \\times (-1) = 2 - 0.9 = 1.1$$",
        "",
        "Para $t=0$ (inicio):",
        "$$G_0 = R_1 + \\gamma G_1 = 5 + 0.9 \\times 1.1 = 5 + 0.99 = 5.99$$",
        "",
        "**Interpretaci\u00f3n:**",
        "- Desde $S_0$, el agente espera obtener un retorno total de 5.99",
        "- Desde $S_1$, el agente espera obtener 1.1",
        "- Desde $S_2$, el agente obtiene -1 (penalizaci\u00f3n final)",
        "",
        "Este c\u00e1lculo se hace **despu\u00e9s** de completar el episodio, no durante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo pr\u00e1ctico: Calcular returns de un episodio",
        "",
        "def calculate_returns(rewards, gamma=0.9):",
        "    \"\"\"",
        "    Calcula los returns para cada paso del episodio.",
        "",
        "    Par\u00e1metros:",
        "    - rewards: lista de recompensas [R1, R2, R3, ...]",
        "    - gamma: factor de descuento",
        "",
        "    Retorna:",
        "    - returns: lista de returns [G0, G1, G2, ...]",
        "    \"\"\"",
        "    returns = []",
        "    G = 0",
        "",
        "    # Procesar en reversa",
        "    for r in reversed(rewards):",
        "        G = r + gamma * G",
        "        returns.insert(0, G)",
        "",
        "    return returns",
        "",
        "# Ejemplo del episodio anterior",
        "rewards = [5, 2, -1]",
        "gamma = 0.9",
        "",
        "returns = calculate_returns(rewards, gamma)",
        "",
        "print(\"Episodio: S0 \u2192 S1 \u2192 S2 \u2192 S3 (terminal)\")",
        "print(\"=\"*50)",
        "print(f\"\\nRecompensas: {rewards}\")",
        "print(f\"Gamma: {gamma}\\n\")",
        "",
        "for t, (r, G) in enumerate(zip(rewards, returns)):",
        "    print(f\"Paso t={t}:\")",
        "    print(f\"  Recompensa R_{t+1} = {r}\")",
        "    print(f\"  Return G_{t} = {G:.4f}\")",
        "    print()",
        "",
        "# Verificar manualmente",
        "print(\"Verificaci\u00f3n manual:\")",
        "print(f\"G_2 = R_3 = {rewards[2]}\")",
        "print(f\"G_1 = R_2 + \u03b3*G_2 = {rewards[1]} + {gamma}*{rewards[2]} = {rewards[1] + gamma*rewards[2]}\")",
        "print(f\"G_0 = R_1 + \u03b3*G_1 = {rewards[0]} + {gamma}*{returns[1]:.4f} = {rewards[0] + gamma*returns[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Actualizaci\u00f3n Incremental",
        "",
        "En la pr\u00e1ctica, no almacenamos todos los retornos. Usamos una **actualizaci\u00f3n incremental** m\u00e1s eficiente:",
        "",
        "$$V(s) \\leftarrow V(s) + \\alpha [G - V(s)]$$",
        "",
        "Donde:",
        "- $\\alpha$: Tasa de aprendizaje (learning rate)",
        "- $G$: Retorno observado",
        "- $V(s)$: Estimaci\u00f3n actual",
        "- $[G - V(s)]$: Error de predicci\u00f3n",
        "",
        "**Equivalencia con promedio:**",
        "Si usamos $\\alpha = \\frac{1}{N(s)}$ (inverso del n\u00famero de visitas), esto es equivalente al promedio:",
        "",
        "$$V_{n+1}(s) = V_n(s) + \\frac{1}{n}[G_n - V_n(s)] = \\frac{1}{n}\\sum_{i=1}^{n} G_i$$",
        "",
        "**Ventaja de $\\alpha$ constante:**",
        "Con $\\alpha$ fijo (ej. $\\alpha=0.1$), damos m\u00e1s peso a experiencias recientes, \u00fatil en entornos no estacionarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MC Prediction: Evaluaci\u00f3n de Pol\u00edticas",
        "",
        "### 3.1 Objetivo",
        "",
        "**MC Prediction** resuelve el problema de **evaluaci\u00f3n de pol\u00edticas**: dada una pol\u00edtica $\\pi$, estimar la funci\u00f3n de valor $V^\\pi(s)$.",
        "",
        "**Problema:** \u00bfQu\u00e9 tan buena es una pol\u00edtica dada?",
        "",
        "**Soluci\u00f3n:** Generar muchos episodios siguiendo $\\pi$ y promediar los retornos observados.",
        "",
        "### 3.2 Algoritmo First-Visit MC Prediction",
        "",
        "```",
        "Entrada:",
        "  - Pol\u00edtica \u03c0 a evaluar",
        "  - N\u00famero de episodios N",
        "",
        "Inicializar:",
        "  - V(s) = 0 para todo s",
        "  - Returns(s) = lista vac\u00eda para todo s",
        "",
        "Repetir N veces:",
        "  1. Generar episodio siguiendo \u03c0:",
        "     S\u2080, A\u2080, R\u2081, S\u2081, A\u2081, R\u2082, ..., S\u209c",
        "",
        "  2. Para cada estado s que aparece en el episodio:",
        "     a. Calcular return G desde la primera vez que apareci\u00f3 s",
        "     b. Agregar G a Returns(s)",
        "     c. V(s) \u2190 promedio(Returns(s))",
        "",
        "Retornar: V como estimaci\u00f3n de V^\u03c0",
        "```",
        "",
        "### 3.3 Propiedades Te\u00f3ricas",
        "",
        "**Teorema de Convergencia:**",
        "> First-Visit MC Prediction converge a $V^\\pi(s)$ cuando el n\u00famero de visitas a $s$ tiende a infinito:",
        "> $$\\lim_{N(s) \\to \\infty} V(s) = V^\\pi(s)$$",
        "",
        "**Tasa de convergencia:**",
        "El error est\u00e1ndar disminuye como $O(1/\\sqrt{n})$ donde $n$ es el n\u00famero de muestras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Implementaci\u00f3n: GridWorld con Pol\u00edtica Optimista",
        "",
        "Vamos a evaluar una pol\u00edtica simple en un GridWorld 5x5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear entorno GridWorld 5x5",
        "print(\"=\"*70)",
        "print(\"MC PREDICTION - GridWorld 5x5\")",
        "print(\"=\"*70)",
        "",
        "env_grid = SimpleGridWorld(size=5)",
        "",
        "print(\"\\nDescripci\u00f3n del entorno:\")",
        "print(f\"  - Tama\u00f1o: 5x5 (25 estados)\")",
        "print(f\"  - Estado inicial: {env_grid.start}\")",
        "print(f\"  - Estado meta: {env_grid.goal}\")",
        "print(f\"  - Recompensas: +1.0 en meta, -0.01 por paso\")",
        "print(f\"  - Acciones: 0=\u2191, 1=\u2192, 2=\u2193, 3=\u2190\")",
        "",
        "# Visualizar el entorno",
        "print(\"\\nEstructura del GridWorld:\")",
        "print(\"\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\")",
        "for i in range(5):",
        "    row = \"\u2502\"",
        "    for j in range(5):",
        "        if (i, j) == env_grid.start:",
        "            row += \"  S  \u2502\"",
        "        elif (i, j) == env_grid.goal:",
        "            row += \"  G  \u2502\"",
        "        else:",
        "            row += \"     \u2502\"",
        "    print(row)",
        "    if i < 4:",
        "        print(\"\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2524\")",
        "print(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\")",
        "print(\"\\nS = Start (inicio), G = Goal (meta)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir una pol\u00edtica \"optimista\" que tiende hacia la meta",
        "def create_biased_policy():",
        "    \"\"\"",
        "    Pol\u00edtica que prefiere moverse hacia abajo y derecha (hacia la meta).",
        "    70% hacia la meta, 30% aleatorio.",
        "    \"\"\"",
        "    def policy(state):",
        "        row, col = state",
        "",
        "        # Con 70% de probabilidad, moverse hacia la meta",
        "        if np.random.random() < 0.7:",
        "            # Si no estamos en la \u00faltima columna, ir a la derecha",
        "            if col < 4:",
        "                return 1  # derecha",
        "            # Si ya estamos en la \u00faltima columna, ir abajo",
        "            elif row < 4:",
        "                return 2  # abajo",
        "",
        "        # 30% aleatorio",
        "        return np.random.randint(0, 4)",
        "",
        "    return policy",
        "",
        "# Crear pol\u00edtica",
        "policy_biased = create_biased_policy()",
        "",
        "# Probar la pol\u00edtica con un episodio",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"Probando la pol\u00edtica optimista\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "state = env_grid.reset()",
        "trajectory = [state]",
        "rewards = []",
        "",
        "for step in range(20):  # M\u00e1ximo 20 pasos",
        "    action = policy_biased(state)",
        "    action_names = ['\u2191', '\u2192', '\u2193', '\u2190']",
        "    print(f\"Paso {step}: Estado {state} \u2192 Acci\u00f3n {action_names[action]}\", end=\"\")",
        "",
        "    next_state, reward, done, _ = env_grid.step(action)",
        "    rewards.append(reward)",
        "    trajectory.append(next_state)",
        "",
        "    print(f\" \u2192 Estado {next_state}, Recompensa {reward:.3f}\")",
        "",
        "    if done:",
        "        print(f\"\\n\u00a1Meta alcanzada en {step+1} pasos!\")",
        "        break",
        "",
        "    state = next_state",
        "",
        "print(f\"\\nRetorno total del episodio: {sum(rewards):.4f}\")",
        "print(f\"Trayectoria: {' \u2192 '.join([str(s) for s in trajectory])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Entrenar MC Prediction: First-Visit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear evaluador MC Prediction (First-Visit)",
        "mc_first_visit = MCPrediction(gamma=0.99, method='first-visit')",
        "",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"ENTRENAMIENTO: First-Visit MC Prediction\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "# Entrenar evaluando la pol\u00edtica",
        "results_first = mc_first_visit.evaluate_policy(",
        "    env=env_grid,",
        "    policy=policy_biased,",
        "    num_episodes=5000,",
        "    max_steps=100,",
        "    verbose=True",
        ")",
        "",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"Funci\u00f3n de Valor Aprendida (muestra de estados)\")",
        "print(\"=\"*70)",
        "",
        "# Mostrar valores de algunos estados clave",
        "states_to_show = [",
        "    (0, 0), (0, 2), (0, 4),",
        "    (2, 0), (2, 2), (2, 4),",
        "    (4, 0), (4, 2), (4, 4)",
        "]",
        "",
        "for state in states_to_show:",
        "    if state in mc_first_visit.V:",
        "        visits = mc_first_visit.visit_counts[state]",
        "        value = mc_first_visit.V[state]",
        "        print(f\"V{state} = {value:7.4f}  (visitas: {visits:5d})\")",
        "    else:",
        "        print(f\"V{state} = no visitado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Visualizaci\u00f3n de la Funci\u00f3n de Valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear visualizaci\u00f3n de la funci\u00f3n de valor como heatmap",
        "def visualize_value_function_gridworld(V, visit_counts, size=5, title=\"Funci\u00f3n de Valor\"):",
        "    \"\"\"",
        "    Visualiza la funci\u00f3n de valor de un GridWorld como heatmap.",
        "    \"\"\"",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
        "",
        "    # Crear matrices de valores y visitas",
        "    value_grid = np.zeros((size, size))",
        "    visits_grid = np.zeros((size, size))",
        "",
        "    for state, value in V.items():",
        "        row, col = state",
        "        value_grid[row, col] = value",
        "        visits_grid[row, col] = visit_counts.get(state, 0)",
        "",
        "    # Heatmap de valores",
        "    im1 = ax1.imshow(value_grid, cmap='RdYlGn', interpolation='nearest')",
        "    ax1.set_title(title, fontsize=14, fontweight='bold')",
        "    ax1.set_xlabel('Columna')",
        "    ax1.set_ylabel('Fila')",
        "",
        "    # A\u00f1adir valores num\u00e9ricos",
        "    for i in range(size):",
        "        for j in range(size):",
        "            text = ax1.text(j, i, f'{value_grid[i, j]:.3f}',",
        "                          ha='center', va='center', color='black', fontsize=9)",
        "",
        "    # Marcar inicio y meta",
        "    ax1.plot(0, 0, 'go', markersize=15, label='Inicio')",
        "    ax1.plot(size-1, size-1, 'r*', markersize=20, label='Meta')",
        "    ax1.legend(loc='upper left')",
        "    plt.colorbar(im1, ax=ax1, label='Valor V(s)')",
        "",
        "    # Heatmap de visitas (logar\u00edtmico para mejor visualizaci\u00f3n)",
        "    visits_log = np.log10(visits_grid + 1)  # +1 para evitar log(0)",
        "    im2 = ax2.imshow(visits_log, cmap='Blues', interpolation='nearest')",
        "    ax2.set_title('Frecuencia de Visitas (log\u2081\u2080)', fontsize=14, fontweight='bold')",
        "    ax2.set_xlabel('Columna')",
        "    ax2.set_ylabel('Fila')",
        "",
        "    # A\u00f1adir n\u00fameros de visitas",
        "    for i in range(size):",
        "        for j in range(size):",
        "            text = ax2.text(j, i, f'{int(visits_grid[i, j])}',",
        "                          ha='center', va='center', color='white', fontsize=9,",
        "                          fontweight='bold')",
        "",
        "    plt.colorbar(im2, ax=ax2, label='log\u2081\u2080(visitas + 1)')",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "# Visualizar resultados",
        "visualize_value_function_gridworld(",
        "    mc_first_visit.V,",
        "    mc_first_visit.visit_counts,",
        "    size=5,",
        "    title=\"Funci\u00f3n de Valor - First-Visit MC\"",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.7 Curvas de Aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar curvas de aprendizaje",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))",
        "",
        "# 1. Returns por episodio",
        "ax1 = axes[0, 0]",
        "ax1.plot(results_first['history']['mean_returns'], alpha=0.3, color='blue', label='Return crudo')",
        "# Suavizado con ventana m\u00f3vil",
        "window = 100",
        "if len(results_first['history']['mean_returns']) >= window:",
        "    smoothed = pd.Series(results_first['history']['mean_returns']).rolling(window=window).mean()",
        "    ax1.plot(smoothed, linewidth=2, color='red', label=f'Media m\u00f3vil ({window} eps)')",
        "ax1.set_xlabel('Episodio')",
        "ax1.set_ylabel('Return')",
        "ax1.set_title('Returns por Episodio', fontweight='bold')",
        "ax1.legend()",
        "ax1.grid(True, alpha=0.3)",
        "",
        "# 2. Cambios en valores",
        "ax2 = axes[0, 1]",
        "ax2.plot(results_first['history']['value_changes'], alpha=0.6, color='green')",
        "ax2.set_xlabel('Episodio')",
        "ax2.set_ylabel('Cambio m\u00e1ximo en V(s)')",
        "ax2.set_title('Cambios en la Funci\u00f3n de Valor', fontweight='bold')",
        "ax2.set_yscale('log')",
        "ax2.grid(True, alpha=0.3)",
        "",
        "# 3. Estados visitados por episodio",
        "ax3 = axes[1, 0]",
        "ax3.plot(results_first['history']['states_visited'], alpha=0.6, color='purple')",
        "ax3.set_xlabel('Episodio')",
        "ax3.set_ylabel('Estados \u00fanicos visitados')",
        "ax3.set_title('Exploraci\u00f3n del Espacio de Estados', fontweight='bold')",
        "ax3.grid(True, alpha=0.3)",
        "",
        "# 4. Distribuci\u00f3n de returns",
        "ax4 = axes[1, 1]",
        "ax4.hist(results_first['history']['mean_returns'], bins=50, alpha=0.7, color='orange', edgecolor='black')",
        "ax4.set_xlabel('Return')",
        "ax4.set_ylabel('Frecuencia')",
        "ax4.set_title('Distribuci\u00f3n de Returns', fontweight='bold')",
        "ax4.axvline(np.mean(results_first['history']['mean_returns']),",
        "           color='red', linestyle='--', linewidth=2, label=f\"Media: {np.mean(results_first['history']['mean_returns']):.3f}\")",
        "ax4.legend()",
        "ax4.grid(True, alpha=0.3, axis='y')",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(f\"\\nEstad\u00edsticas finales:\")",
        "print(f\"  Return promedio: {np.mean(results_first['history']['mean_returns']):.4f}\")",
        "print(f\"  Desviaci\u00f3n est\u00e1ndar: {np.std(results_first['history']['mean_returns']):.4f}\")",
        "print(f\"  Return m\u00e1ximo: {np.max(results_first['history']['mean_returns']):.4f}\")",
        "print(f\"  Return m\u00ednimo: {np.min(results_first['history']['mean_returns']):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.8 Comparaci\u00f3n: First-Visit vs Every-Visit MC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar Every-Visit MC para comparar",
        "mc_every_visit = MCPrediction(gamma=0.99, method='every-visit')",
        "",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"ENTRENAMIENTO: Every-Visit MC Prediction\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "results_every = mc_every_visit.evaluate_policy(",
        "    env=env_grid,",
        "    policy=policy_biased,",
        "    num_episodes=5000,",
        "    max_steps=100,",
        "    verbose=True",
        ")",
        "",
        "# Comparar First-Visit vs Every-Visit",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"COMPARACI\u00d3N: First-Visit vs Every-Visit\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "print(f\"{'Estado':<15} {'First-Visit':<15} {'Every-Visit':<15} {'Diferencia':<15}\")",
        "print(\"-\" * 65)",
        "",
        "for state in states_to_show:",
        "    if state in mc_first_visit.V and state in mc_every_visit.V:",
        "        v_first = mc_first_visit.V[state]",
        "        v_every = mc_every_visit.V[state]",
        "        diff = abs(v_first - v_every)",
        "        print(f\"{str(state):<15} {v_first:<15.4f} {v_every:<15.4f} {diff:<15.6f}\")",
        "",
        "print(f\"\\nDiferencia m\u00e1xima: {max(abs(mc_first_visit.V.get(s, 0) - mc_every_visit.V.get(s, 0)) for s in mc_first_visit.V):.6f}\")",
        "print(\"\\n\u2192 Ambos m\u00e9todos convergen a valores muy similares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.9 Ejemplo: Blackjack",
        "",
        "Ahora evaluaremos una pol\u00edtica de Blackjack:",
        "- **Estado**: (suma_jugador, carta_visible_dealer, tiene_as_utilizable)",
        "- **Acciones**: 0=plantarse (stick), 1=pedir carta (hit)",
        "- **Pol\u00edtica**: Pedir carta si suma < 20, plantarse si suma \u2265 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear entorno Blackjack y pol\u00edtica",
        "env_blackjack = SimpleBlackjack()",
        "",
        "def blackjack_policy(state):",
        "    \"\"\"Pol\u00edtica conservadora: pedir si suma < 20\"\"\"",
        "    player_sum, dealer_showing, usable_ace = state",
        "    return 1 if player_sum < 20 else 0  # 1=hit, 0=stick",
        "",
        "print(\"=\"*70)",
        "print(\"MC PREDICTION - Blackjack\")",
        "print(\"=\"*70)",
        "",
        "# Evaluar pol\u00edtica con MC",
        "mc_blackjack = MCPrediction(gamma=1.0, method='first-visit')  # gamma=1.0 para juegos finitos",
        "",
        "results_blackjack = mc_blackjack.evaluate_policy(",
        "    env=env_blackjack,",
        "    policy=blackjack_policy,",
        "    num_episodes=10000,",
        "    max_steps=50,",
        "    verbose=True",
        ")",
        "",
        "# Mostrar algunos valores interesantes",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"Funci\u00f3n de Valor para estados seleccionados de Blackjack\")",
        "print(\"=\"*70)",
        "print(f\"\\n{'Estado (suma, dealer, as)':<40} {'Valor V(s)':<15} {'Visitas':<10}\")",
        "print(\"-\" * 65)",
        "",
        "sample_states = [",
        "    (20, 10, False), (20, 7, False), (19, 10, False),",
        "    (19, 7, False), (18, 10, False), (18, 7, False),",
        "    (17, 10, False), (16, 7, False), (15, 10, False)",
        "]",
        "",
        "for state in sample_states:",
        "    if state in mc_blackjack.V:",
        "        value = mc_blackjack.V[state]",
        "        visits = mc_blackjack.visit_counts[state]",
        "        interpretation = \"Favorable\" if value > 0 else \"Desfavorable\"",
        "        print(f\"{str(state):<40} {value:<15.4f} {visits:<10} ({interpretation})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretaci\u00f3n de Resultados Blackjack:",
        "",
        "- **Valores positivos**: El estado es favorable (alta probabilidad de ganar)",
        "- **Valores negativos**: El estado es desfavorable (alta probabilidad de perder)",
        "- **Cerca de 0**: Estado neutral (probabilidades equilibradas)",
        "",
        "**Observaciones t\u00edpicas:**",
        "- Suma 20 contra cualquier carta del dealer: muy favorable (+0.6 a +0.8)",
        "- Suma 15-17 contra 10 del dealer: desfavorable (-0.3 a -0.5)",
        "- El as utilizable mejora las probabilidades en estados l\u00edmite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MC Control On-Policy: Encontrando Pol\u00edticas \u00d3ptimas",
        "",
        "### 4.1 De Prediction a Control",
        "",
        "Hasta ahora hemos usado MC Prediction para **evaluar** pol\u00edticas dadas. Ahora queremos **encontrar** la pol\u00edtica \u00f3ptima.",
        "",
        "**MC Control** alterna entre dos pasos:",
        "1. **Policy Evaluation**: Estimar $Q^\\pi(s,a)$ con MC Prediction",
        "2. **Policy Improvement**: Mejorar la pol\u00edtica siendo greedy respecto a $Q$",
        "",
        "### 4.2 El Problema de Exploraci\u00f3n",
        "",
        "Para aprender $Q(s,a)$, necesitamos visitar **todos** los pares (s,a).",
        "",
        "**Soluci\u00f3n: Pol\u00edtica \u03b5-greedy**",
        "",
        "$$\\pi(a|s) = \\begin{cases}",
        "1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{si } a = \\arg\\max_{a'} Q(s,a') \\\\",
        "\\frac{\\epsilon}{|A|} & \\text{en otro caso}",
        "\\end{cases}$$",
        "",
        "- Con probabilidad $\\epsilon$: acci\u00f3n aleatoria (exploraci\u00f3n)",
        "- Con probabilidad $1-\\epsilon$: mejor acci\u00f3n conocida (explotaci\u00f3n)",
        "",
        "### 4.3 Algoritmo MC Control On-Policy (\u03b5-greedy)",
        "",
        "```",
        "Inicializar:",
        "  - Q(s,a) = 0 para todo s,a",
        "  - \u03c0 = pol\u00edtica \u03b5-greedy respecto a Q",
        "  - Returns(s,a) = lista vac\u00eda",
        "",
        "Repetir por cada episodio:",
        "  1. Generar episodio siguiendo \u03c0:",
        "     S\u2080, A\u2080, R\u2081, S\u2081, A\u2081, R\u2082, ..., S\u209c",
        "",
        "  2. Para cada par (s,a) en el episodio:",
        "     a. G \u2190 return desde primera vez que apareci\u00f3 (s,a)",
        "     b. Agregar G a Returns(s,a)",
        "     c. Q(s,a) \u2190 promedio(Returns(s,a))",
        "",
        "  3. Para cada s en el episodio:",
        "     \u03c0(s) \u2190 \u03b5-greedy respecto a Q(s,\u00b7)",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Implementaci\u00f3n: GridWorld con \u03b5-greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear agente MC Control On-Policy",
        "print(\"=\"*70)",
        "print(\"MC CONTROL ON-POLICY - GridWorld 5x5\")",
        "print(\"=\"*70)",
        "",
        "agent_on = MCControlOnPolicy(",
        "    gamma=0.99,",
        "    epsilon=0.2,      # 20% exploraci\u00f3n inicial",
        "    epsilon_decay=0.999,  # Decaer epsilon gradualmente",
        "    epsilon_min=0.01,     # M\u00ednimo 1% exploraci\u00f3n",
        "    method='first-visit'",
        ")",
        "",
        "# Crear nuevo entorno",
        "env_control = SimpleGridWorld(size=5)",
        "",
        "print(f\"\\nPar\u00e1metros del agente:\")",
        "print(f\"  - Gamma: {agent_on.gamma}\")",
        "print(f\"  - Epsilon inicial: {agent_on.epsilon}\")",
        "print(f\"  - Epsilon decay: {agent_on.epsilon_decay}\")",
        "print(f\"  - Epsilon m\u00ednimo: {agent_on.epsilon_min}\")",
        "print(f\"  - M\u00e9todo: {agent_on.method}\")",
        "",
        "print(f\"\\nIniciando entrenamiento...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar el agente",
        "results_on = agent_on.train(",
        "    env=env_control,",
        "    num_episodes=5000,",
        "    max_steps=100,",
        "    valid_actions=[0, 1, 2, 3],",
        "    verbose=True",
        ")",
        "",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"Entrenamiento completado!\")",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Visualizaci\u00f3n de la Pol\u00edtica Aprendida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer y visualizar pol\u00edtica aprendida",
        "def visualize_learned_policy(agent, size=5, title=\"Pol\u00edtica Aprendida\"):",
        "    \"\"\"Visualiza la pol\u00edtica y Q-values aprendidos\"\"\"",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
        "",
        "    action_symbols = ['\u2191', '\u2192', '\u2193', '\u2190']",
        "",
        "    # 1. Pol\u00edtica determinista (greedy)",
        "    policy_grid = np.zeros((size, size), dtype=int)",
        "    for i in range(size):",
        "        for j in range(size):",
        "            state = (i, j)",
        "            if state in agent.Q and agent.Q[state]:",
        "                policy_grid[i, j] = max(agent.Q[state].items(), key=lambda x: x[1])[0]",
        "",
        "    ax1.set_title(title, fontsize=14, fontweight='bold')",
        "    ax1.set_xlim(-0.5, size - 0.5)",
        "    ax1.set_ylim(-0.5, size - 0.5)",
        "    ax1.set_xticks(range(size))",
        "    ax1.set_yticks(range(size))",
        "    ax1.grid(True, linewidth=2)",
        "    ax1.invert_yaxis()",
        "",
        "    for i in range(size):",
        "        for j in range(size):",
        "            action = policy_grid[i, j]",
        "            color = 'red' if (i, j) == (size-1, size-1) else 'blue'",
        "            ax1.text(j, i, action_symbols[action],",
        "                    ha='center', va='center', fontsize=24, color=color)",
        "",
        "    ax1.plot(0, 0, 'go', markersize=15, label='Inicio')",
        "    ax1.plot(size-1, size-1, 'r*', markersize=20, label='Meta')",
        "    ax1.legend(loc='upper left')",
        "",
        "    # 2. Valores Q m\u00e1ximos por estado",
        "    value_grid = np.zeros((size, size))",
        "    for i in range(size):",
        "        for j in range(size):",
        "            state = (i, j)",
        "            if state in agent.Q and agent.Q[state]:",
        "                value_grid[i, j] = max(agent.Q[state].values())",
        "",
        "    im = ax2.imshow(value_grid, cmap='RdYlGn', interpolation='nearest')",
        "    ax2.set_title('Valores Q M\u00e1ximos V(s) = max_a Q(s,a)', fontsize=14, fontweight='bold')",
        "    ax2.set_xlabel('Columna')",
        "    ax2.set_ylabel('Fila')",
        "",
        "    for i in range(size):",
        "        for j in range(size):",
        "            ax2.text(j, i, f'{value_grid[i, j]:.2f}',",
        "                    ha='center', va='center', color='black', fontsize=9)",
        "",
        "    plt.colorbar(im, ax=ax2)",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "# Visualizar",
        "visualize_learned_policy(agent_on, size=5, title=\"Pol\u00edtica Aprendida con MC Control On-Policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 An\u00e1lisis del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An\u00e1lisis de convergencia",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))",
        "",
        "# 1. Returns por episodio",
        "ax1 = axes[0, 0]",
        "ax1.plot(results_on['history']['episode_returns'], alpha=0.3, label='Return')",
        "window = 100",
        "smoothed = pd.Series(results_on['history']['episode_returns']).rolling(window=window).mean()",
        "ax1.plot(smoothed, linewidth=2, color='red', label=f'Media m\u00f3vil ({window})')",
        "ax1.set_xlabel('Episodio')",
        "ax1.set_ylabel('Return')",
        "ax1.set_title('Evoluci\u00f3n de Returns', fontweight='bold')",
        "ax1.legend()",
        "ax1.grid(True, alpha=0.3)",
        "",
        "# 2. Longitud de episodios",
        "ax2 = axes[0, 1]",
        "ax2.plot(results_on['history']['episode_lengths'], alpha=0.3, color='green', label='Longitud')",
        "smoothed_len = pd.Series(results_on['history']['episode_lengths']).rolling(window=window).mean()",
        "ax2.plot(smoothed_len, linewidth=2, color='darkgreen', label=f'Media m\u00f3vil ({window})')",
        "ax2.set_xlabel('Episodio')",
        "ax2.set_ylabel('Longitud del episodio')",
        "ax2.set_title('Longitud de Episodios', fontweight='bold')",
        "ax2.legend()",
        "ax2.grid(True, alpha=0.3)",
        "",
        "# 3. Epsilon decay",
        "ax3 = axes[1, 0]",
        "ax3.plot(results_on['history']['epsilon_history'], linewidth=2, color='orange')",
        "ax3.set_xlabel('Episodio')",
        "ax3.set_ylabel('Epsilon')",
        "ax3.set_title('Decaimiento de Epsilon (Exploraci\u00f3n)', fontweight='bold')",
        "ax3.grid(True, alpha=0.3)",
        "",
        "# 4. Cambios en Q-values",
        "ax4 = axes[1, 1]",
        "ax4.plot(results_on['history']['q_value_changes'], alpha=0.6, color='purple')",
        "ax4.set_xlabel('Episodio')",
        "ax4.set_ylabel('Cambio m\u00e1ximo en Q')",
        "ax4.set_title('Cambios en Q-Values', fontweight='bold')",
        "ax4.set_yscale('log')",
        "ax4.grid(True, alpha=0.3)",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "# Estad\u00edsticas finales",
        "print(\"\\nEstad\u00edsticas del Entrenamiento:\")",
        "print(\"=\"*50)",
        "final_returns = results_on['history']['episode_returns'][-100:]",
        "final_lengths = results_on['history']['episode_lengths'][-100:]",
        "print(f\"\u00daltimos 100 episodios:\")",
        "print(f\"  Return promedio: {np.mean(final_returns):.4f} \u00b1 {np.std(final_returns):.4f}\")",
        "print(f\"  Longitud promedio: {np.mean(final_lengths):.2f} \u00b1 {np.std(final_lengths):.2f}\")",
        "print(f\"\\nEpsilon final: {results_on['history']['epsilon_history'][-1]:.4f}\")",
        "print(f\"Estados-acci\u00f3n visitados: {sum(len(actions) for actions in agent_on.Q.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.7 Evaluaci\u00f3n de la Pol\u00edtica Aprendida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar la pol\u00edtica aprendida (100% greedy)",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"EVALUACI\u00d3N: Pol\u00edtica Aprendida (100% greedy, sin exploraci\u00f3n)\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "test_episodes = 100",
        "test_returns = []",
        "test_lengths = []",
        "test_trajectories = []",
        "",
        "for ep in range(test_episodes):",
        "    state = env_control.reset()",
        "    episode_return = 0",
        "    episode_length = 0",
        "    trajectory = [state]",
        "",
        "    for step in range(100):",
        "        # Usar pol\u00edtica greedy (sin exploraci\u00f3n)",
        "        action = agent_on.get_action(state, greedy=True)",
        "        state, reward, done, _ = env_control.step(action)",
        "        trajectory.append(state)",
        "",
        "        episode_return += reward",
        "        episode_length += 1",
        "",
        "        if done:",
        "            break",
        "",
        "    test_returns.append(episode_return)",
        "    test_lengths.append(episode_length)",
        "    if ep < 5:  # Guardar primeras 5 trayectorias",
        "        test_trajectories.append(trajectory)",
        "",
        "print(f\"Resultados de evaluaci\u00f3n ({test_episodes} episodios):\")",
        "print(f\"  Return promedio: {np.mean(test_returns):.4f} \u00b1 {np.std(test_returns):.4f}\")",
        "print(f\"  Longitud promedio: {np.mean(test_lengths):.2f} \u00b1 {np.std(test_lengths):.2f}\")",
        "print(f\"  Return m\u00e1ximo: {np.max(test_returns):.4f}\")",
        "print(f\"  Return m\u00ednimo: {np.min(test_returns):.4f}\")",
        "",
        "print(f\"\\nPrimeras 5 trayectorias:\")",
        "for i, traj in enumerate(test_trajectories, 1):",
        "    print(f\"  {i}. {' \u2192 '.join([str(s) for s in traj])} (longitud: {len(traj)-1})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. MC Control Off-Policy: Importance Sampling",
        "",
        "### 5.1 Motivaci\u00f3n",
        "",
        "**On-Policy**: Aprende sobre la pol\u00edtica que est\u00e1 siguiendo (\u03b5-greedy)",
        "- Ventaja: Convergencia estable",
        "- Desventaja: Nunca aprende la pol\u00edtica \u00f3ptima pura (siempre tiene exploraci\u00f3n)",
        "",
        "**Off-Policy**: Aprende sobre la pol\u00edtica target (\u00f3ptima) mientras sigue una behavior policy (exploratoria)",
        "- **Behavior policy** $b(a|s)$: Pol\u00edtica exploratoria que genera los episodios",
        "- **Target policy** $\\pi(a|s)$: Pol\u00edtica determinista que queremos aprender",
        "",
        "### 5.2 Importance Sampling",
        "",
        "El problema: episodios generados por $b$ no siguen la distribuci\u00f3n de $\\pi$.",
        "",
        "**Soluci\u00f3n**: Ponderar los retornos por el ratio de importancia:",
        "",
        "$$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$",
        "",
        "Este ratio corrige el sesgo introducido por seguir $b$ en lugar de $\\pi$.",
        "",
        "### 5.3 Weighted Importance Sampling",
        "",
        "Para reducir varianza, usamos **weighted importance sampling**:",
        "",
        "$$Q(s,a) = \\frac{\\sum_{i=1}^{n} \\rho_i G_i}{\\sum_{i=1}^{n} \\rho_i}$$",
        "",
        "En lugar del promedio simple, ponderamos por los ratios de importancia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Algoritmo MC Control Off-Policy",
        "",
        "```",
        "Inicializar:",
        "  - Q(s,a) = 0 para todo s,a",
        "  - C(s,a) = 0 (suma de pesos)",
        "  - \u03c0(s) = greedy respecto a Q  (target policy)",
        "  - b(s) = \u03b5-greedy respecto a Q  (behavior policy)",
        "",
        "Repetir por cada episodio:",
        "  1. Generar episodio usando b:",
        "     S\u2080, A\u2080, R\u2081, ..., S\u209c",
        "",
        "  2. G \u2190 0",
        "  3. W \u2190 1",
        "",
        "  4. Para t = T-1, T-2, ..., 0:",
        "     a. G \u2190 \u03b3G + R_{t+1}",
        "     b. C(S\u209c, A\u209c) \u2190 C(S\u209c, A\u209c) + W",
        "     c. Q(S\u209c, A\u209c) \u2190 Q(S\u209c, A\u209c) + (W/C(S\u209c, A\u209c))[G - Q(S\u209c, A\u209c)]",
        "     d. \u03c0(S\u209c) \u2190 argmax_a Q(S\u209c, a)",
        "     e. Si A\u209c \u2260 \u03c0(S\u209c): break  (episodio no sigue \u03c0)",
        "     f. W \u2190 W \u00b7 1/b(A\u209c|S\u209c)",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Implementaci\u00f3n: Cliff Walking",
        "",
        "Usaremos el problema **Cliff Walking** donde:",
        "- Hay un \"acantilado\" que da recompensa -100",
        "- On-policy aprende camino **seguro** (lejos del acantilado)",
        "- Off-policy aprende camino **\u00f3ptimo** (cerca del acantilado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear entorno Cliff Walking",
        "print(\"=\"*70)",
        "print(\"MC CONTROL OFF-POLICY - Cliff Walking\")",
        "print(\"=\"*70)",
        "",
        "env_cliff = CliffWalking()",
        "",
        "print(f\"\\nDescripci\u00f3n del entorno:\")",
        "print(f\"  - Tama\u00f1o: {env_cliff.rows}x{env_cliff.cols}\")",
        "print(f\"  - Inicio: {env_cliff.start}\")",
        "print(f\"  - Meta: {env_cliff.goal}\")",
        "print(f\"  - Acantilado: {len(env_cliff.cliff)} celdas\")",
        "print(f\"  - Caer al acantilado: -100, reinicia al inicio\")",
        "print(f\"  - Cada paso: -1\")",
        "",
        "# Visualizar",
        "print(\"\\nMapa del Cliff Walking:\")",
        "print(\"\u250c\" + \"\u2500\" * (env_cliff.cols * 4 + 1) + \"\u2510\")",
        "for i in range(env_cliff.rows):",
        "    row = \"\u2502\"",
        "    for j in range(env_cliff.cols):",
        "        pos = (i, j)",
        "        if pos == env_cliff.start:",
        "            row += \" S  \"",
        "        elif pos == env_cliff.goal:",
        "            row += \" G  \"",
        "        elif pos in env_cliff.cliff:",
        "            row += \" X  \"",
        "        else:",
        "            row += \"    \"",
        "    row += \"\u2502\"",
        "    print(row)",
        "print(\"\u2514\" + \"\u2500\" * (env_cliff.cols * 4 + 1) + \"\u2518\")",
        "print(\"\\nS=Start, G=Goal, X=Cliff (acantilado)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar agente Off-Policy",
        "agent_off = MCControlOffPolicy(",
        "    gamma=0.99,",
        "    epsilon=0.3,  # Behavior policy m\u00e1s exploratoria",
        "    method='first-visit'",
        ")",
        "",
        "print(f\"\\nPar\u00e1metros del agente Off-Policy:\")",
        "print(f\"  - Gamma: {agent_off.gamma}\")",
        "print(f\"  - Epsilon (behavior): {agent_off.epsilon}\")",
        "print(f\"  - Target policy: greedy (determinista)\")",
        "print(f\"  - Behavior policy: \u03b5-greedy (exploratoria)\")",
        "",
        "print(f\"\\nIniciando entrenamiento Off-Policy...\")",
        "",
        "results_off = agent_off.train(",
        "    env=env_cliff,",
        "    num_episodes=10000,  # M\u00e1s episodios necesarios para off-policy",
        "    max_steps=200,",
        "    valid_actions=[0, 1, 2, 3],",
        "    verbose=True",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 Comparaci\u00f3n: On-Policy vs Off-Policy en Cliff Walking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar tambi\u00e9n On-Policy para comparar",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"Entrenando On-Policy para comparaci\u00f3n...\")",
        "print(\"=\"*70)",
        "",
        "agent_on_cliff = MCControlOnPolicy(",
        "    gamma=0.99,",
        "    epsilon=0.1,",
        "    epsilon_decay=0.999,",
        "    epsilon_min=0.01",
        ")",
        "",
        "results_on_cliff = agent_on_cliff.train(",
        "    env=env_cliff,",
        "    num_episodes=5000,",
        "    max_steps=200,",
        "    valid_actions=[0, 1, 2, 3],",
        "    verbose=False",
        ")",
        "",
        "print(\"\\n\u2713 Ambos agentes entrenados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar ambas pol\u00edticas",
        "def evaluate_cliff_policy(agent, env, episodes=100, is_on_policy=True):",
        "    \"\"\"Eval\u00faa una pol\u00edtica en Cliff Walking\"\"\"",
        "    returns = []",
        "    lengths = []",
        "    falls = 0  # Contador de ca\u00eddas al acantilado",
        "",
        "    for _ in range(episodes):",
        "        state = env.reset()",
        "        ep_return = 0",
        "        ep_length = 0",
        "",
        "        for step in range(500):",
        "            if is_on_policy and hasattr(agent, 'get_action'):",
        "                action = agent.get_action(state, greedy=True)",
        "            elif hasattr(agent, 'get_action'):",
        "                action = agent.get_action(state)",
        "            else:",
        "                action = 0",
        "",
        "            state, reward, done, _ = env.step(action)",
        "            ep_return += reward",
        "            ep_length += 1",
        "",
        "            if reward == -100:",
        "                falls += 1",
        "",
        "            if done:",
        "                break",
        "",
        "        returns.append(ep_return)",
        "        lengths.append(ep_length)",
        "",
        "    return {",
        "        'mean_return': np.mean(returns),",
        "        'std_return': np.std(returns),",
        "        'mean_length': np.mean(lengths),",
        "        'falls': falls",
        "    }",
        "",
        "# Evaluar ambos",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"EVALUACI\u00d3N EN CLIFF WALKING\")",
        "print(\"=\"*70)",
        "",
        "eval_on = evaluate_cliff_policy(agent_on_cliff, env_cliff, episodes=200, is_on_policy=True)",
        "eval_off = evaluate_cliff_policy(agent_off, env_cliff, episodes=200, is_on_policy=False)",
        "",
        "print(f\"\\n{'M\u00e9trica':<25} {'On-Policy':<20} {'Off-Policy':<20}\")",
        "print(\"-\" * 65)",
        "print(f\"{'Return promedio':<25} {eval_on['mean_return']:< 20.2f} {eval_off['mean_return']:< 20.2f}\")",
        "print(f\"{'Desv. est\u00e1ndar':<25} {eval_on['std_return']:< 20.2f} {eval_off['std_return']:< 20.2f}\")",
        "print(f\"{'Longitud promedio':<25} {eval_on['mean_length']:< 20.2f} {eval_off['mean_length']:< 20.2f}\")",
        "print(f\"{'Ca\u00eddas al acantilado':<25} {eval_on['falls']:< 20} {eval_off['falls']:< 20}\")",
        "",
        "print(f\"\\n{'Interpretaci\u00f3n:'}\")",
        "print(f\"  - On-Policy: Aprende camino SEGURO (evita acantilado, m\u00e1s pasos)\")",
        "print(f\"  - Off-Policy: Aprende camino \u00d3PTIMO (cerca del acantilado, menos pasos)\")",
        "print(f\"  - Off-Policy obtiene mejor return pero es m\u00e1s arriesgado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.7 Visualizaci\u00f3n de Importance Ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar importance ratios",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))",
        "",
        "# 1. Historia de importance ratios",
        "ax1 = axes[0]",
        "ratios = results_off['history']['importance_ratios']",
        "ax1.plot(ratios, alpha=0.5, color='purple')",
        "window = 100",
        "if len(ratios) >= window:",
        "    smoothed = pd.Series(ratios).rolling(window=window).mean()",
        "    ax1.plot(smoothed, linewidth=2, color='darkviolet', label=f'Media m\u00f3vil ({window})')",
        "ax1.set_xlabel('Episodio')",
        "ax1.set_ylabel('Ratio de Importancia')",
        "ax1.set_title('Importance Ratios durante Entrenamiento', fontweight='bold')",
        "ax1.set_yscale('log')",
        "ax1.legend()",
        "ax1.grid(True, alpha=0.3)",
        "",
        "# 2. Distribuci\u00f3n de ratios",
        "ax2 = axes[1]",
        "# Filtrar valores extremos para mejor visualizaci\u00f3n",
        "filtered_ratios = [r for r in ratios if r < np.percentile(ratios, 95)]",
        "ax2.hist(filtered_ratios, bins=50, alpha=0.7, color='purple', edgecolor='black')",
        "ax2.set_xlabel('Ratio de Importancia')",
        "ax2.set_ylabel('Frecuencia')",
        "ax2.set_title('Distribuci\u00f3n de Importance Ratios', fontweight='bold')",
        "ax2.axvline(np.mean(filtered_ratios), color='red', linestyle='--',",
        "           linewidth=2, label=f'Media: {np.mean(filtered_ratios):.2f}')",
        "ax2.legend()",
        "ax2.grid(True, alpha=0.3, axis='y')",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(f\"\\nEstad\u00edsticas de Importance Ratios:\")",
        "print(f\"  Media: {np.mean(ratios):.2f}\")",
        "print(f\"  Mediana: {np.median(ratios):.2f}\")",
        "print(f\"  M\u00e1ximo: {np.max(ratios):.2f}\")",
        "print(f\"  % ratios > 10: {100 * np.mean([r > 10 for r in ratios]):.1f}%\")",
        "print(f\"\\nRatios altos indican episodios con baja probabilidad bajo behavior policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Experimentos Completos y An\u00e1lisis Comparativo",
        "",
        "### 6.1 Experimento 1: Efecto del Factor de Descuento \u03b3",
        "",
        "Investigaremos c\u00f3mo \u03b3 afecta el aprendizaje de MC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)",
        "print(\"EXPERIMENTO 1: Efecto de Gamma\")",
        "print(\"=\"*70)",
        "",
        "gammas = [0.7, 0.9, 0.99, 0.999]",
        "results_gamma = {}",
        "",
        "for gamma in gammas:",
        "    print(f\"\\nEntrenando con \u03b3={gamma}...\")",
        "    agent = MCControlOnPolicy(",
        "        gamma=gamma,",
        "        epsilon=0.2,",
        "        epsilon_decay=0.999,",
        "        epsilon_min=0.01",
        "    )",
        "",
        "    env_exp = SimpleGridWorld(size=5)",
        "    results = agent.train(",
        "        env=env_exp,",
        "        num_episodes=3000,",
        "        max_steps=100,",
        "        valid_actions=[0, 1, 2, 3],",
        "        verbose=False",
        "    )",
        "",
        "    # Evaluar",
        "    test_returns = []",
        "    for _ in range(50):",
        "        state = env_exp.reset()",
        "        ep_return = 0",
        "        for _ in range(100):",
        "            action = agent.get_action(state, greedy=True)",
        "            state, reward, done, _ = env_exp.step(action)",
        "            ep_return += reward",
        "            if done:",
        "                break",
        "        test_returns.append(ep_return)",
        "",
        "    results_gamma[gamma] = {",
        "        'agent': agent,",
        "        'mean_return': np.mean(test_returns),",
        "        'training_history': results['history']",
        "    }",
        "    print(f\"  Return promedio: {np.mean(test_returns):.4f}\")",
        "",
        "print(\"\\n\u2713 Experimento completado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar efectos de gamma",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))",
        "",
        "# 1. Returns por gamma",
        "ax1 = axes[0]",
        "gamma_values = list(results_gamma.keys())",
        "mean_returns = [results_gamma[g]['mean_return'] for g in gamma_values]",
        "ax1.bar(range(len(gamma_values)), mean_returns, color='skyblue', edgecolor='black', alpha=0.7)",
        "ax1.set_xticks(range(len(gamma_values)))",
        "ax1.set_xticklabels([f'\u03b3={g}' for g in gamma_values])",
        "ax1.set_ylabel('Return Promedio')",
        "ax1.set_title('Efecto de Gamma en Performance', fontweight='bold')",
        "ax1.grid(True, alpha=0.3, axis='y')",
        "",
        "# 2. Curvas de aprendizaje",
        "ax2 = axes[1]",
        "for gamma in gammas:",
        "    history = results_gamma[gamma]['training_history']['episode_returns']",
        "    window = 100",
        "    smoothed = pd.Series(history).rolling(window=window).mean()",
        "    ax2.plot(smoothed, label=f'\u03b3={gamma}', linewidth=2, alpha=0.7)",
        "ax2.set_xlabel('Episodio')",
        "ax2.set_ylabel('Return (suavizado)')",
        "ax2.set_title('Curvas de Aprendizaje por Gamma', fontweight='bold')",
        "ax2.legend()",
        "ax2.grid(True, alpha=0.3)",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(f\"\\nObservaciones:\")",
        "print(f\"  - \u03b3 bajo (0.7): Aprende r\u00e1pido pero pol\u00edticas miopes\")",
        "print(f\"  - \u03b3 alto (0.99-0.999): Mejor performance pero m\u00e1s lento\")",
        "print(f\"  - \u0393 \u00f3ptimo depende del horizonte del problema\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Experimento 2: Convergencia First-Visit vs Every-Visit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)",
        "print(\"EXPERIMENTO 2: First-Visit vs Every-Visit Convergencia\")",
        "print(\"=\"*70)",
        "",
        "# Entrenar ambos m\u00e9todos m\u00faltiples veces",
        "n_runs = 5",
        "episode_counts = [100, 500, 1000, 2000, 5000]",
        "results_comparison = {'first-visit': [], 'every-visit': []}",
        "",
        "for method in ['first-visit', 'every-visit']:",
        "    print(f\"\\nEntrenando {method}...\")",
        "",
        "    for n_eps in episode_counts:",
        "        run_results = []",
        "",
        "        for run in range(n_runs):",
        "            mc = MCPrediction(gamma=0.99, method=method)",
        "            env_exp = SimpleGridWorld(size=5)",
        "",
        "            mc.evaluate_policy(",
        "                env=env_exp,",
        "                policy=create_biased_policy(),",
        "                num_episodes=n_eps,",
        "                max_steps=100,",
        "                verbose=False",
        "            )",
        "",
        "            # Calcular valor promedio",
        "            avg_value = np.mean(list(mc.V.values())) if mc.V else 0",
        "            run_results.append(avg_value)",
        "",
        "        results_comparison[method].append({",
        "            'episodes': n_eps,",
        "            'mean': np.mean(run_results),",
        "            'std': np.std(run_results)",
        "        })",
        "",
        "    print(f\"  \u2713 Completado {method}\")",
        "",
        "print(\"\\n\u2713 Experimento completado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar convergencia",
        "fig, ax = plt.subplots(figsize=(10, 6))",
        "",
        "for method in ['first-visit', 'every-visit']:",
        "    data = results_comparison[method]",
        "    episodes = [d['episodes'] for d in data]",
        "    means = [d['mean'] for d in data]",
        "    stds = [d['std'] for d in data]",
        "",
        "    ax.errorbar(episodes, means, yerr=stds, marker='o', markersize=8,",
        "               linewidth=2, capsize=5, label=method.capitalize(), alpha=0.7)",
        "",
        "ax.set_xlabel('N\u00famero de Episodios', fontsize=12)",
        "ax.set_ylabel('Valor Promedio V(s)', fontsize=12)",
        "ax.set_title('Convergencia: First-Visit vs Every-Visit', fontsize=14, fontweight='bold')",
        "ax.set_xscale('log')",
        "ax.legend(fontsize=11)",
        "ax.grid(True, alpha=0.3)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(f\"\\nConclusi\u00f3n:\")",
        "print(f\"  - Ambos m\u00e9todos convergen al mismo valor\")",
        "print(f\"  - Every-Visit puede tener menor varianza (m\u00e1s datos por episodio)\")",
        "print(f\"  - First-Visit tiene garant\u00edas te\u00f3ricas m\u00e1s fuertes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizaciones Avanzadas",
        "",
        "### 7.1 Mapa de Calor de Q-Values por Acci\u00f3n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar Q-values para cada acci\u00f3n",
        "def visualize_q_values_by_action(agent, size=5):",
        "    \"\"\"Visualiza Q(s,a) para cada acci\u00f3n en GridWorld\"\"\"",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))",
        "    action_names = ['Arriba (\u2191)', 'Derecha (\u2192)', 'Abajo (\u2193)', 'Izquierda (\u2190)']",
        "",
        "    for action in range(4):",
        "        ax = axes[action // 2, action % 2]",
        "",
        "        # Crear grid de Q-values para esta acci\u00f3n",
        "        q_grid = np.zeros((size, size))",
        "        for i in range(size):",
        "            for j in range(size):",
        "                state = (i, j)",
        "                if state in agent.Q and action in agent.Q[state]:",
        "                    q_grid[i, j] = agent.Q[state][action]",
        "",
        "        # Visualizar",
        "        im = ax.imshow(q_grid, cmap='coolwarm', interpolation='nearest')",
        "        ax.set_title(f'Q-Values: {action_names[action]}', fontsize=12, fontweight='bold')",
        "        ax.set_xlabel('Columna')",
        "        ax.set_ylabel('Fila')",
        "",
        "        # A\u00f1adir valores",
        "        for i in range(size):",
        "            for j in range(size):",
        "                ax.text(j, i, f'{q_grid[i, j]:.2f}',",
        "                       ha='center', va='center', color='white', fontsize=9,",
        "                       fontweight='bold')",
        "",
        "        plt.colorbar(im, ax=ax)",
        "",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "# Visualizar Q-values del agente on-policy",
        "print(\"Q-Values por acci\u00f3n (Agente On-Policy):\\n\")",
        "visualize_q_values_by_action(agent_on, size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Comparaci\u00f3n Visual de Pol\u00edticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar pol\u00edticas aprendidas",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))",
        "action_symbols = ['\u2191', '\u2192', '\u2193', '\u2190']",
        "",
        "# Funci\u00f3n helper para extraer pol\u00edtica",
        "def extract_policy_grid(agent, size=5):",
        "    policy_grid = np.zeros((size, size), dtype=int)",
        "    for i in range(size):",
        "        for j in range(size):",
        "            state = (i, j)",
        "            if state in agent.Q and agent.Q[state]:",
        "                policy_grid[i, j] = max(agent.Q[state].items(), key=lambda x: x[1])[0]",
        "    return policy_grid",
        "",
        "# 1. Pol\u00edtica On-Policy",
        "ax1 = axes[0]",
        "policy_on = extract_policy_grid(agent_on, 5)",
        "ax1.set_title('On-Policy (\u03b5-greedy)', fontsize=14, fontweight='bold')",
        "ax1.set_xlim(-0.5, 4.5)",
        "ax1.set_ylim(-0.5, 4.5)",
        "ax1.grid(True, linewidth=2)",
        "ax1.invert_yaxis()",
        "for i in range(5):",
        "    for j in range(5):",
        "        ax1.text(j, i, action_symbols[policy_on[i, j]],",
        "                ha='center', va='center', fontsize=20, color='blue')",
        "",
        "# 2. Pol\u00edtica Off-Policy (si existe)",
        "ax2 = axes[1]",
        "if hasattr(agent_off, 'Q') and len(agent_off.Q) > 0:",
        "    policy_off = extract_policy_grid(agent_off, 5)",
        "    ax2.set_title('Off-Policy (Importance Sampling)', fontsize=14, fontweight='bold')",
        "    ax2.set_xlim(-0.5, 4.5)",
        "    ax2.set_ylim(-0.5, 4.5)",
        "    ax2.grid(True, linewidth=2)",
        "    ax2.invert_yaxis()",
        "    for i in range(5):",
        "        for j in range(5):",
        "            ax2.text(j, i, action_symbols[policy_off[i, j]],",
        "                    ha='center', va='center', fontsize=20, color='green')",
        "else:",
        "    ax2.text(0.5, 0.5, 'No disponible\\n(entrenado en Cliff Walking)',",
        "            ha='center', va='center', transform=ax2.transAxes)",
        "    ax2.set_title('Off-Policy', fontsize=14, fontweight='bold')",
        "",
        "# 3. Pol\u00edtica \u00f3ptima te\u00f3rica (para GridWorld simple)",
        "ax3 = axes[2]",
        "policy_optimal = np.array([",
        "    [1, 1, 1, 1, 2],",
        "    [1, 1, 1, 1, 2],",
        "    [1, 1, 1, 1, 2],",
        "    [1, 1, 1, 1, 2],",
        "    [1, 1, 1, 1, 0]",
        "])",
        "ax3.set_title('Pol\u00edtica \u00d3ptima Te\u00f3rica', fontsize=14, fontweight='bold')",
        "ax3.set_xlim(-0.5, 4.5)",
        "ax3.set_ylim(-0.5, 4.5)",
        "ax3.grid(True, linewidth=2)",
        "ax3.invert_yaxis()",
        "for i in range(5):",
        "    for j in range(5):",
        "        ax3.text(j, i, action_symbols[policy_optimal[i, j]],",
        "                ha='center', va='center', fontsize=20, color='red')",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\nObservaci\u00f3n: Las pol\u00edticas aprendidas deber\u00edan ser similares a la \u00f3ptima\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Ejercicios Pr\u00e1cticos",
        "",
        "### Ejercicio 1: Modificar Recompensas",
        "",
        "**Objetivo**: Entender c\u00f3mo las recompensas afectan el aprendizaje de MC.",
        "",
        "**Tarea**:",
        "1. Crea un GridWorld 5x5 con recompensas modificadas:",
        "   - step_reward = -0.1 (en lugar de -0.01)",
        "   - goal_reward = 10.0 (en lugar de 1.0)",
        "2. Entrena un agente MC Control On-Policy",
        "3. Compara la pol\u00edtica aprendida con la original",
        "",
        "**Preguntas**:",
        "- \u00bfC\u00f3mo afectan las recompensas negativas altas al comportamiento?",
        "- \u00bfEl agente aprende rutas m\u00e1s directas?",
        "- \u00bfCu\u00e1ntos episodios necesita para converger?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 1: Tu c\u00f3digo aqu\u00ed",
        "",
        "# Pista: Necesitas crear un nuevo entorno modificando SimpleGridWorld",
        "# o crear recompensas personalizadas",
        "",
        "# Tu c\u00f3digo aqu\u00ed...",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 2: An\u00e1lisis de Varianza",
        "",
        "**Objetivo**: Comparar la varianza de First-Visit y Every-Visit MC.",
        "",
        "**Tarea**:",
        "1. Implementa una funci\u00f3n que calcule la varianza de los retornos",
        "2. Ejecuta 10 entrenamientos de cada m\u00e9todo (First-Visit y Every-Visit)",
        "3. Compara las varianzas de las estimaciones de valor",
        "4. Grafica boxplots de las distribuciones",
        "",
        "**Pregunta**: \u00bfQu\u00e9 m\u00e9todo tiene menor varianza y por qu\u00e9?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 2: Tu c\u00f3digo aqu\u00ed",
        "",
        "def analyze_variance(method='first-visit', n_runs=10, n_episodes=1000):",
        "    \"\"\"",
        "    Analiza varianza de un m\u00e9todo MC.",
        "",
        "    Retorna:",
        "        - variance_per_state: dict con varianza por estado",
        "        - mean_variance: varianza promedio",
        "    \"\"\"",
        "    # Tu c\u00f3digo aqu\u00ed...",
        "    pass",
        "",
        "# Ejecutar an\u00e1lisis",
        "# results_first = analyze_variance('first-visit')",
        "# results_every = analyze_variance('every-visit')",
        "# ... comparar y visualizar ...",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 3: Blackjack \u00d3ptimo",
        "",
        "**Objetivo**: Encontrar la pol\u00edtica \u00f3ptima para Blackjack.",
        "",
        "**Tarea**:",
        "1. Usa MC Control On-Policy para aprender la pol\u00edtica \u00f3ptima de Blackjack",
        "2. Entrena por 50,000 episodios",
        "3. Visualiza la pol\u00edtica aprendida como una tabla:",
        "   - Filas: suma del jugador (12-21)",
        "   - Columnas: carta visible del dealer (1-10)",
        "   - Valores: Hit(1) o Stick(0)",
        "4. Compara con estrategias conocidas de Blackjack",
        "",
        "**Pregunta**: \u00bfEn qu\u00e9 situaciones la pol\u00edtica aprendida difiere de tu intuici\u00f3n?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 3: Tu c\u00f3digo aqu\u00ed",
        "",
        "# Pista 1: Usa MCControlOnPolicy con SimpleBlackjack",
        "# Pista 2: Para visualizar, crea una matriz de acciones por estado",
        "",
        "# Tu c\u00f3digo aqu\u00ed...",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 4: Importance Sampling en Pr\u00e1ctica",
        "",
        "**Objetivo**: Implementar y analizar importance sampling manually.",
        "",
        "**Tarea**:",
        "1. Implementa una funci\u00f3n que calcule importance ratios manualmente",
        "2. Genera 100 episodios con una pol\u00edtica aleatoria",
        "3. Calcula los retornos ponderados para una pol\u00edtica target diferente",
        "4. Compara con los retornos directos",
        "",
        "**Pregunta**: \u00bfCu\u00e1ndo los importance ratios son muy altos y por qu\u00e9 es problem\u00e1tico?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 4: Tu c\u00f3digo aqu\u00ed",
        "",
        "def manual_importance_sampling(episodes, behavior_policy, target_policy, gamma=0.99):",
        "    \"\"\"",
        "    Calcula valores usando importance sampling manualmente.",
        "",
        "    Par\u00e1metros:",
        "        - episodes: lista de episodios [(s,a,r), ...]",
        "        - behavior_policy: funci\u00f3n que da probabilidad \u03c0_b(a|s)",
        "        - target_policy: funci\u00f3n que da probabilidad \u03c0_t(a|s)",
        "        - gamma: factor de descuento",
        "",
        "    Retorna:",
        "        - V: diccionario de valores estimados",
        "        - ratios: lista de importance ratios",
        "    \"\"\"",
        "    # Tu c\u00f3digo aqu\u00ed...",
        "    pass",
        "",
        "# Ejemplo de uso:",
        "# episodes = [...]  # generar episodios",
        "# V, ratios = manual_importance_sampling(episodes, ...)",
        "# ... analizar ratios ...",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 5: Experimento de Escalabilidad",
        "",
        "**Objetivo**: Investigar c\u00f3mo MC escala con el tama\u00f1o del problema.",
        "",
        "**Tarea**:",
        "1. Entrena agentes MC Control en GridWorlds de diferentes tama\u00f1os: 3x3, 5x5, 7x7, 10x10",
        "2. Mide para cada tama\u00f1o:",
        "   - Tiempo de entrenamiento",
        "   - Episodios hasta convergencia",
        "   - Calidad de la pol\u00edtica final",
        "3. Grafica escalabilidad",
        "",
        "**Preguntas**:",
        "- \u00bfC\u00f3mo crece el tiempo de entrenamiento con el tama\u00f1o?",
        "- \u00bfMC escala mejor que Dynamic Programming?",
        "- \u00bfQu\u00e9 tama\u00f1o es el l\u00edmite pr\u00e1ctico para MC?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 5: Tu c\u00f3digo aqu\u00ed",
        "",
        "import time",
        "",
        "def scalability_experiment(sizes=[3, 5, 7, 10], n_episodes=5000):",
        "    \"\"\"",
        "    Experimenta con diferentes tama\u00f1os de GridWorld.",
        "",
        "    Retorna:",
        "        - results: dict con m\u00e9tricas por tama\u00f1o",
        "    \"\"\"",
        "    results = {}",
        "",
        "    for size in sizes:",
        "        print(f\"\\nEntrenando en grid {size}x{size}...\")",
        "",
        "        # Tu c\u00f3digo aqu\u00ed:",
        "        # 1. Crear entorno de tama\u00f1o 'size'",
        "        # 2. Entrenar agente",
        "        # 3. Medir tiempo y episodios",
        "        # 4. Evaluar pol\u00edtica",
        "",
        "        pass",
        "",
        "    return results",
        "",
        "# Ejecutar experimento",
        "# results = scalability_experiment()",
        "# ... visualizar resultados ...",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusiones y Pr\u00f3ximos Pasos",
        "",
        "### 9.1 Resumen de Conceptos Clave",
        "",
        "**M\u00e9todos Monte Carlo**:",
        "1. **Model-free**: No requieren modelo del entorno",
        "2. **Basados en episodios**: Aprenden de experiencia completa",
        "3. **Unbiased**: Estimaciones convergen a valores verdaderos",
        "4. **Alta varianza**: Requieren muchos episodios",
        "",
        "**Variantes estudiadas**:",
        "- **MC Prediction**: Evaluar pol\u00edticas dadas",
        "- **First-Visit vs Every-Visit**: Trade-off entre garant\u00edas te\u00f3ricas y eficiencia",
        "- **MC Control On-Policy**: Aprender pol\u00edtica \u00f3ptima con exploraci\u00f3n",
        "- **MC Control Off-Policy**: Aprender pol\u00edtica \u00f3ptima mientras se explora",
        "",
        "**Importance Sampling**:",
        "- Permite aprender de episodios generados por otra pol\u00edtica",
        "- Weighted IS reduce varianza",
        "- Cr\u00edtico para off-policy learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Comparaci\u00f3n: DP vs MC",
        "",
        "| Aspecto | Dynamic Programming | Monte Carlo |",
        "|---------|-------------------|-------------|",
        "| **Modelo** | Requiere p(s',r\\|s,a) | No requiere (model-free) |",
        "| **Computaci\u00f3n** | Sweeps sobre todos los estados | Solo estados visitados |",
        "| **Convergencia** | Garantizada en pocos sweeps | Requiere muchos episodios |",
        "| **Aplicabilidad** | Solo con modelo completo | Simulaci\u00f3n o experiencia real |",
        "| **Bootstrapping** | S\u00ed (usa V(s')) | No (usa retornos completos) |",
        "| **Sesgo** | Depende de V inicial | Insesgado |",
        "| **Varianza** | Baja | Alta |",
        "| **Mejor para** | Problemas peque\u00f1os con modelo | Problemas sin modelo, grandes |",
        "",
        "**\u00bfCu\u00e1ndo usar cada uno?**",
        "",
        "**Usar DP cuando**:",
        "- Tienes modelo completo y preciso",
        "- Espacio de estados peque\u00f1o/mediano",
        "- Necesitas soluci\u00f3n exacta r\u00e1pida",
        "",
        "**Usar MC cuando**:",
        "- No tienes modelo o es muy complejo",
        "- Puedes simular episodios",
        "- Solo te importan estados frecuentes",
        "- Problemas epis\u00f3dicos naturalmente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Limitaciones de Monte Carlo",
        "",
        "1. **Solo entornos epis\u00f3dicos**: MC requiere que los episodios terminen",
        "   - No funciona en tareas continuas",
        "   - Soluci\u00f3n: Temporal Difference Learning (siguiente tema)",
        "",
        "2. **Alta varianza**: Los retornos pueden variar mucho",
        "   - Requiere muchos episodios para converger",
        "   - Soluci\u00f3n: TD learning (menor varianza)",
        "",
        "3. **Aprendizaje lento**: Debe esperar al final del episodio",
        "   - No puede actualizar durante el episodio",
        "   - Soluci\u00f3n: TD learning (actualizaciones incrementales)",
        "",
        "4. **Importance sampling puede ser inestable**: Ratios muy altos",
        "   - Especialmente problem\u00e1tico con episodios largos",
        "   - Soluciones: Truncated IS, Per-decision IS",
        "",
        "### 9.4 Pr\u00f3ximos Temas",
        "",
        "Para superar las limitaciones de MC, estudiaremos:",
        "",
        "1. **Temporal Difference (TD) Learning**: Combina ventajas de DP y MC",
        "   - SARSA: On-policy TD control",
        "   - Q-Learning: Off-policy TD control",
        "   - Actualizaciones incrementales durante el episodio",
        "",
        "2. **N-step methods**: Intermedio entre MC y TD",
        "",
        "3. **Function Approximation**: Manejar espacios de estados grandes",
        "   - Representaciones compactas de V y Q",
        "   - Deep Q-Networks (DQN)",
        "",
        "4. **Policy Gradient Methods**: Optimizaci\u00f3n directa de pol\u00edticas",
        "   - REINFORCE",
        "   - Actor-Critic",
        "",
        "### 9.5 Referencias y Recursos",
        "",
        "**Libros**:",
        "- Sutton & Barto (2018): *Reinforcement Learning: An Introduction* - Cap\u00edtulos 5 y 6",
        "- Bertsekas (2019): *Reinforcement Learning and Optimal Control*",
        "",
        "**Papers Cl\u00e1sicos**:",
        "- Watkins (1989): \"Learning from Delayed Rewards\" (Q-learning)",
        "- Sutton (1988): \"Learning to Predict by the Method of Temporal Differences\"",
        "",
        "**Recursos Online**:",
        "- David Silver's RL Course (Lecture 4 & 5)",
        "- OpenAI Spinning Up",
        "- DeepMind x UCL RL Course",
        "",
        "---",
        "",
        "**\u00a1Felicidades!** Has completado el tutorial de M\u00e9todos Monte Carlo en Reinforcement Learning.",
        "",
        "**Pr\u00f3ximos pasos sugeridos**:",
        "1. Completar los ejercicios pr\u00e1cticos",
        "2. Experimentar con diferentes entornos (Gym, Gymnasium)",
        "3. Implementar variantes avanzadas (Per-decision IS, MC Tree Search)",
        "4. Continuar con el notebook de Temporal Difference Learning"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}