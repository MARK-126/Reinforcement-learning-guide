{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods in Reinforcement Learning\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "* Understand the principles of Monte Carlo (MC) methods\n",
    "* Implement First-Visit and Every-Visit MC Prediction\n",
    "* Implement MC Control with on-policy (epsilon-greedy) learning\n",
    "* Implement MC Control with off-policy learning using Importance Sampling\n",
    "* Compare MC methods with Dynamic Programming\n",
    "\n",
    "**Estimated time**: 3-4 hours\n",
    "\n",
    "**Prerequisites**: Deep understanding of Dynamic Programming and basic Reinforcement Learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Monte Carlo Introduction](#2)\n",
    "    - [2.1 - Model-Free Learning](#2-1)\n",
    "    - [2.2 - Returns and Discounting](#2-2)\n",
    "- [3 - MC Prediction](#3)\n",
    "    - [Exercise 1 - implement_first_visit_mc_prediction](#ex-1)\n",
    "    - [Exercise 2 - implement_every_visit_mc_prediction](#ex-2)\n",
    "- [4 - MC Control On-Policy](#4)\n",
    "    - [Exercise 3 - implement_mc_control_on_policy](#ex-3)\n",
    "- [5 - MC Control Off-Policy](#5)\n",
    "    - [Exercise 4 - implement_mc_control_off_policy](#ex-4)\n",
    "    - [Exercise 5 - implement_importance_sampling](#ex-5)\n",
    "- [6 - Comparative Analysis](#6)\n",
    "- [7 - Conclusion](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Import utility functions\n",
    "sys.path.append('/home/user/Reinforcement-learning-guide/notebooks')\n",
    "from mc_utils import (\n",
    "    calculate_returns, first_visit_mc_prediction, every_visit_mc_prediction,\n",
    "    mc_control_on_policy, mc_control_off_policy,\n",
    "    first_visit_mc_prediction_test, every_visit_mc_prediction_test,\n",
    "    mc_control_on_policy_test, mc_control_off_policy_test\n",
    ")\n",
    "\n",
    "print('✓ All imports successful')\n",
    "print(f'NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Monte Carlo Introduction\n",
    "\n",
    "Monte Carlo methods learn directly from experience without requiring knowledge of the environment dynamics.\n",
    "\n",
    "### Key Characteristics:\n",
    "1. **Model-Free**: No need for transition probabilities $p(s',r|s,a)$\n",
    "2. **Episode-based**: Learn at the end of complete episodes\n",
    "3. **Unbiased**: Estimates converge to true values\n",
    "4. **High variance**: Requires many episodes for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - Model-Free Learning\n",
    "\n",
    "Unlike Dynamic Programming:\n",
    "- DP uses bootstrapping: $V(s) \\leftarrow \\mathbb{E}[R + \\gamma V(s')]$\n",
    "- MC uses actual returns: $V(s) \\leftarrow \\text{average}(G_1, G_2, G_3, ...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Returns and Discounting\n",
    "\n",
    "The return $G_t$ is the discounted sum of future rewards:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots$$\n",
    "\n",
    "Key property: $G_t = R_{t+1} + \\gamma G_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate returns for an episode\n",
    "rewards = [5, 2, -1]  # Example rewards\n",
    "gamma = 0.9\n",
    "returns = calculate_returns(rewards, gamma)\n",
    "\n",
    "print(\"Episode: S0 → S1 → S2 → S3 (terminal)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Rewards: {rewards}\\nGamma: {gamma}\\n\")\n",
    "for t, (r, G) in enumerate(zip(rewards, returns)):\n",
    "    print(f\"Step t={t}: Reward R_{t+1} = {r}, Return G_{t} = {G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - MC Prediction: Policy Evaluation\n",
    "\n",
    "**Objective**: Given a policy $\\pi$, estimate the value function $V^\\pi(s)$\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$$\n",
    "\n",
    "**Approach**: Average the observed returns for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Visit vs Every-Visit\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Aspect</th>\n",
    "    <th>First-Visit</th>\n",
    "    <th>Every-Visit</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Definition</b></td>\n",
    "    <td>Average only first visit to state per episode</td>\n",
    "    <td>Average all visits to state in episode</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Samples per episode</b></td>\n",
    "    <td>Maximum one per state</td>\n",
    "    <td>Multiple possible</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Convergence</b></td>\n",
    "    <td>Guaranteed</td>\n",
    "    <td>Guaranteed</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Variance</b></td>\n",
    "    <td>Higher</td>\n",
    "    <td>Lower (more data)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - implement_first_visit_mc_prediction\n",
    "\n",
    "Implement First-Visit MC Prediction that evaluates a given policy.\n",
    "\n",
    "**Instructions:**\n",
    "- Generate episodes following the policy\n",
    "- For each state, record the return only on first visit per episode\n",
    "- Update value estimate as average of returns\n",
    "\n",
    "**Formula:**\n",
    "$$V(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_first_visit_mc_prediction\n",
    "\n",
    "def implement_first_visit_mc_prediction(env, policy, num_episodes, max_steps, gamma=0.99):\n",
    "    \"\"\"\n",
    "    First-Visit Monte Carlo Prediction.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- environment with reset() and step() methods\n",
    "    policy -- function that returns action given state\n",
    "    num_episodes -- number of episodes to generate\n",
    "    max_steps -- maximum steps per episode\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    V -- dictionary of state values\n",
    "    visit_counts -- dictionary of visit counts per state\n",
    "    \"\"\"\n",
    "    \n",
    "    V = defaultdict(float)\n",
    "    visit_counts = defaultdict(int)\n",
    "    returns = defaultdict(list)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        visited_states = set()\n",
    "        \n",
    "        # Generate episode\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append(state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                trajectory.append(next_state)\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate returns and update V (first-visit only)\n",
    "        episode_returns = calculate_returns(rewards, gamma)\n",
    "        for t, (s, G) in enumerate(zip(trajectory[:-1], episode_returns)):\n",
    "            if s not in visited_states:\n",
    "                returns[s].append(G)\n",
    "                visit_counts[s] += 1\n",
    "                V[s] = np.mean(returns[s])\n",
    "                visited_states.add(s)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dict(V), dict(visit_counts)\n",
    "\n",
    "# Test your implementation\n",
    "first_visit_mc_prediction_test(implement_first_visit_mc_prediction)\n",
    "print(\"✓ First-Visit MC Prediction implementation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - implement_every_visit_mc_prediction\n",
    "\n",
    "Implement Every-Visit MC Prediction.\n",
    "\n",
    "**Instructions:**\n",
    "- Generate episodes following the policy\n",
    "- For each state, record the return for ALL visits in the episode\n",
    "- Update value estimate as average of all returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_every_visit_mc_prediction\n",
    "\n",
    "def implement_every_visit_mc_prediction(env, policy, num_episodes, max_steps, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Every-Visit Monte Carlo Prediction.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- environment with reset() and step() methods\n",
    "    policy -- function that returns action given state\n",
    "    num_episodes -- number of episodes to generate\n",
    "    max_steps -- maximum steps per episode\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    V -- dictionary of state values\n",
    "    visit_counts -- dictionary of visit counts per state\n",
    "    \"\"\"\n",
    "    \n",
    "    V = defaultdict(float)\n",
    "    visit_counts = defaultdict(int)\n",
    "    returns = defaultdict(list)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Generate episode\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append(state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                trajectory.append(next_state)\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate returns and update V (every visit)\n",
    "        episode_returns = calculate_returns(rewards, gamma)\n",
    "        for t, (s, G) in enumerate(zip(trajectory[:-1], episode_returns)):\n",
    "            returns[s].append(G)\n",
    "            visit_counts[s] += 1\n",
    "            V[s] = np.mean(returns[s])\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dict(V), dict(visit_counts)\n",
    "\n",
    "# Test your implementation\n",
    "every_visit_mc_prediction_test(implement_every_visit_mc_prediction)\n",
    "print(\"✓ Every-Visit MC Prediction implementation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- **First-Visit MC**: Only uses first visit to each state per episode\n",
    "  - Pro: Stronger theoretical convergence guarantees\n",
    "  - Con: Less data per episode\n",
    "- **Every-Visit MC**: Uses all visits to each state per episode\n",
    "  - Pro: More data per episode, potentially faster convergence\n",
    "  - Con: Slightly weaker theoretical properties\n",
    "- Both converge to the true value function as $N(s) \\to \\infty$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - MC Control On-Policy: Learning Optimal Policies\n",
    "\n",
    "Now we move from **evaluation** to **control** - finding the optimal policy.\n",
    "\n",
    "**Key Idea**: Alternate between:\n",
    "1. **Policy Evaluation**: Estimate $Q^\\pi(s,a)$\n",
    "2. **Policy Improvement**: Make policy greedy w.r.t. $Q$\n",
    "\n",
    "### Exploration-Exploitation: ε-Greedy Policy\n",
    "\n",
    "$$\\pi(a|s) = \\begin{cases}\n",
    "1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n",
    "\\frac{\\epsilon}{|A|} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "- Probability $\\epsilon$: explore (random action)\n",
    "- Probability $1-\\epsilon$: exploit (best known action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - implement_mc_control_on_policy\n",
    "\n",
    "Implement MC Control with on-policy ε-greedy learning.\n",
    "\n",
    "**Instructions:**\n",
    "- Generate episodes using ε-greedy policy\n",
    "- Update Q-values using first-visit returns\n",
    "- Improve policy based on updated Q-values\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize Q(s,a) = 0\n",
    "2. For each episode:\n",
    "   - Generate trajectory using ε-greedy policy\n",
    "   - Calculate returns G for each (s,a) pair\n",
    "   - Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[G - Q(s,a)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_mc_control_on_policy\n",
    "\n",
    "def implement_mc_control_on_policy(env, num_episodes, epsilon=0.1, max_steps=100, gamma=0.99, alpha=None):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control (On-Policy).\n",
    "    \n",
    "    Arguments:\n",
    "    env -- environment with reset() and step() methods\n",
    "    num_episodes -- number of episodes to generate\n",
    "    epsilon -- exploration rate (epsilon-greedy)\n",
    "    max_steps -- maximum steps per episode\n",
    "    gamma -- discount factor\n",
    "    alpha -- learning rate (if None, uses incremental averaging)\n",
    "    \n",
    "    Returns:\n",
    "    Q -- dictionary of Q-values Q[state][action]\n",
    "    policy -- optimal policy derived from Q\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    returns = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        actions_taken = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Generate episode following epsilon-greedy policy\n",
    "        for step in range(max_steps):\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, 4)  # Random action (assuming 4 actions)\n",
    "            else:\n",
    "                if len(Q[state]) == 0:\n",
    "                    action = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    action = max(Q[state].items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            trajectory.append(state)\n",
    "            actions_taken.append(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                trajectory.append(next_state)\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Update Q-values (first-visit)\n",
    "        episode_returns = calculate_returns(rewards, gamma)\n",
    "        visited = set()\n",
    "        for t, (s, a, G) in enumerate(zip(trajectory[:-1], actions_taken, episode_returns)):\n",
    "            if (s, a) not in visited:\n",
    "                returns[s][a].append(G)\n",
    "                if alpha is None:\n",
    "                    Q[s][a] = np.mean(returns[s][a])\n",
    "                else:\n",
    "                    Q[s][a] += alpha * (G - Q[s][a])\n",
    "                visited.add((s, a))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Extract policy\n",
    "    policy = {}\n",
    "    for state in Q:\n",
    "        if len(Q[state]) > 0:\n",
    "            policy[state] = max(Q[state].items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return dict(Q), policy\n",
    "\n",
    "# Test your implementation\n",
    "mc_control_on_policy_test(implement_mc_control_on_policy)\n",
    "print(\"✓ MC Control On-Policy implementation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- **On-Policy Learning**: Learn about policy you're following\n",
    "  - Explores with ε-greedy probability\n",
    "  - Learns optimal policy given this exploration\n",
    "  - Never learns truly optimal (always has exploration)\n",
    "- **Epsilon Decay**: Common to start with high ε and decay over time\n",
    "  - Start: High exploration (ε = 0.3-0.5)\n",
    "  - End: Low exploration (ε = 0.01-0.05)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - MC Control Off-Policy: Learning from Exploratory Data\n",
    "\n",
    "**Problem with On-Policy**: Never learns truly optimal policy (always explores with ε)\n",
    "\n",
    "**Solution: Off-Policy Learning**\n",
    "- **Behavior Policy** $b$: Exploratory policy that generates data\n",
    "- **Target Policy** $\\pi$: Deterministic optimal policy we want to learn\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "Adjust returns to account for different policies:\n",
    "\n",
    "$$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$\n",
    "\n",
    "This ratio corrects for the probability difference between target and behavior policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - implement_mc_control_off_policy\n",
    "\n",
    "Implement MC Control with off-policy learning using importance sampling.\n",
    "\n",
    "**Instructions:**\n",
    "- Generate episodes using behavior policy (exploratory)\n",
    "- Update Q-values for target policy (greedy) using importance sampling\n",
    "- Calculate importance ratios: $\\rho = \\frac{\\pi(a|s)}{b(a|s)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_mc_control_off_policy\n",
    "\n",
    "def implement_mc_control_off_policy(env, num_episodes, epsilon=0.3, max_steps=100, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control (Off-Policy with Importance Sampling).\n",
    "    \n",
    "    Arguments:\n",
    "    env -- environment with reset() and step() methods\n",
    "    num_episodes -- number of episodes to generate\n",
    "    epsilon -- exploration rate for behavior policy\n",
    "    max_steps -- maximum steps per episode\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    Q -- dictionary of Q-values Q[state][action]\n",
    "    policy -- optimal target policy\n",
    "    importance_ratios -- list of importance ratios per episode\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    C = defaultdict(lambda: defaultdict(float))  # Cumulative weights\n",
    "    importance_ratios = []\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        actions_taken = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Generate episode using behavior policy (epsilon-greedy with high epsilon)\n",
    "        for step in range(max_steps):\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, 4)  # Random action\n",
    "            else:\n",
    "                if len(Q[state]) == 0:\n",
    "                    action = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    action = max(Q[state].items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            trajectory.append(state)\n",
    "            actions_taken.append(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                trajectory.append(next_state)\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Off-policy update with importance sampling\n",
    "        G = 0\n",
    "        W = 1  # Importance weight\n",
    "        episode_ratio = 1\n",
    "        num_actions = 4\n",
    "        \n",
    "        for t in reversed(range(len(trajectory) - 1)):\n",
    "            s = trajectory[t]\n",
    "            a = actions_taken[t]\n",
    "            r = rewards[t]\n",
    "            \n",
    "            G = r + gamma * G\n",
    "            C[s][a] += W\n",
    "            Q[s][a] += (W / C[s][a]) * (G - Q[s][a])\n",
    "            \n",
    "            # Check if action is greedy (target policy)\n",
    "            best_action = max(Q[s].items(), key=lambda x: x[1])[0] if len(Q[s]) > 0 else 0\n",
    "            if a != best_action:\n",
    "                break\n",
    "            \n",
    "            # Update importance weight: 1 / P(a|s,b)\n",
    "            W *= (1.0 / epsilon) if np.random.random() < epsilon else 1.0\n",
    "            episode_ratio *= W\n",
    "        \n",
    "        importance_ratios.append(episode_ratio)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Extract policy\n",
    "    policy = {}\n",
    "    for state in Q:\n",
    "        if len(Q[state]) > 0:\n",
    "            policy[state] = max(Q[state].items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return dict(Q), policy, importance_ratios\n",
    "\n",
    "# Test your implementation\n",
    "mc_control_off_policy_test(implement_mc_control_off_policy)\n",
    "print(\"✓ MC Control Off-Policy implementation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - implement_importance_sampling\n",
    "\n",
    "Implement the importance sampling ratio calculation.\n",
    "\n",
    "**Formula:**\n",
    "$$\\rho = \\frac{\\pi(a|s)}{b(a|s)}$$\n",
    "\n",
    "Where:\n",
    "- $\\pi(a|s)$: Target policy probability\n",
    "- $b(a|s)$: Behavior policy probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_importance_sampling\n",
    "\n",
    "def implement_importance_sampling(episode, target_policy_prob, behavior_policy_prob):\n",
    "    \"\"\"\n",
    "    Calculate importance sampling ratio for an episode.\n",
    "    \n",
    "    Arguments:\n",
    "    episode -- list of (state, action, reward) tuples\n",
    "    target_policy_prob -- probability of actions under target policy\n",
    "    behavior_policy_prob -- probability of actions under behavior policy\n",
    "    \n",
    "    Returns:\n",
    "    rho -- importance sampling ratio (product of probabilities)\n",
    "    \"\"\"\n",
    "    \n",
    "    rho = 1.0\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # For each step in episode, multiply by ratio of probabilities\n",
    "    for state, action, reward in episode:\n",
    "        # Assuming deterministic target policy (probability 1 or 0)\n",
    "        # and epsilon-greedy behavior policy\n",
    "        if target_policy_prob > 0:\n",
    "            rho *= target_policy_prob / behavior_policy_prob\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return rho\n",
    "\n",
    "# Test your implementation\n",
    "print(\"✓ Importance Sampling implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- **Off-Policy Learning**: Learn about target policy using data from behavior policy\n",
    "  - Behavior policy: Exploratory (high ε)\n",
    "  - Target policy: Greedy (optimal)\n",
    "- **Importance Sampling**: Adjust returns by ratio of policies\n",
    "  - High ratio = unlikely under behavior policy, likely under target\n",
    "  - Low ratio = likely under behavior policy\n",
    "  - Can cause high variance when ratios are large\n",
    "- **Advantage**: Learn optimal policy without always exploring\n",
    "- **Disadvantage**: Higher variance, may need more episodes\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Comparative Analysis\n",
    "\n",
    "### MC vs Dynamic Programming\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Aspect</th>\n",
    "    <th>Dynamic Programming</th>\n",
    "    <th>Monte Carlo</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Model Required</b></td>\n",
    "    <td>Yes (p(s',r|s,a))</td>\n",
    "    <td>No (model-free)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Update Type</b></td>\n",
    "    <td>Bootstrapping (uses V(s'))</td>\n",
    "    <td>Actual returns (complete episodes)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Convergence</b></td>\n",
    "    <td>Fast (few sweeps)</td>\n",
    "    <td>Slow (many episodes)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Variance</b></td>\n",
    "    <td>Low</td>\n",
    "    <td>High</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Bias</b></td>\n",
    "    <td>High (depends on V init)</td>\n",
    "    <td>None (unbiased)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Best For</b></td>\n",
    "    <td>Small problems with known model</td>\n",
    "    <td>Unknown dynamics, simulation available</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Policy vs Off-Policy\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Aspect</th>\n",
    "    <th>On-Policy</th>\n",
    "    <th>Off-Policy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Behavior Policy</b></td>\n",
    "    <td>ε-greedy (exploratory)</td>\n",
    "    <td>ε-greedy (exploratory)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Target Policy</b></td>\n",
    "    <td>ε-greedy (same as behavior)</td>\n",
    "    <td>Greedy (deterministic, optimal)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>What We Learn</b></td>\n",
    "    <td>Value of exploratory policy</td>\n",
    "    <td>Value of optimal policy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Variance</b></td>\n",
    "    <td>Normal</td>\n",
    "    <td>Very high (importance ratios)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Sample Efficiency</b></td>\n",
    "    <td>Good</td>\n",
    "    <td>Poor</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Convergence</b></td>\n",
    "    <td>Guaranteed</td>\n",
    "    <td>Guaranteed</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Conclusion and Next Steps\n",
    "\n",
    "### Summary of Key Concepts\n",
    "\n",
    "1. **MC Prediction**: Evaluates given policies by averaging observed returns\n",
    "   - First-Visit: Stronger theory, less data\n",
    "   - Every-Visit: More data, same convergence\n",
    "\n",
    "2. **MC Control On-Policy**: Finds good (but not optimal) policy with exploration\n",
    "   - Uses ε-greedy for exploration\n",
    "   - Never learns truly optimal (always explores)\n",
    "\n",
    "3. **MC Control Off-Policy**: Learns optimal policy from exploratory data\n",
    "   - Uses importance sampling for correction\n",
    "   - Very high variance\n",
    "\n",
    "### Key Advantages of MC\n",
    "- Model-free (no need for environment dynamics)\n",
    "- Can focus on important states\n",
    "- Unbiased convergence\n",
    "- Simple to understand and implement\n",
    "\n",
    "### Limitations of MC\n",
    "- Requires complete episodes\n",
    "- High variance (slow convergence)\n",
    "- Can't update during episode\n",
    "- Off-policy can have unstable importance ratios\n",
    "\n",
    "### Next Topics\n",
    "The limitations of MC led to **Temporal Difference (TD) Learning**:\n",
    "- Lower variance than MC (bootstrapping)\n",
    "- Can update before episode completion\n",
    "- Combines advantages of DP and MC\n",
    "- Examples: SARSA, Q-Learning, Expected SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Monte Carlo Methods tutorial.\n",
    "\n",
    "You can now:\n",
    "- ✓ Understand model-free learning principles\n",
    "- ✓ Implement First-Visit and Every-Visit MC Prediction\n",
    "- ✓ Implement On-Policy MC Control with ε-greedy\n",
    "- ✓ Implement Off-Policy MC Control with importance sampling\n",
    "- ✓ Compare MC with DP and other methods\n",
    "\n",
    "**Suggested Next Steps:**\n",
    "1. Implement MC methods on Gymnasium environments\n",
    "2. Experiment with different epsilon decay schedules\n",
    "3. Analyze importance sampling variance\n",
    "4. Move to Temporal Difference Learning for faster convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
