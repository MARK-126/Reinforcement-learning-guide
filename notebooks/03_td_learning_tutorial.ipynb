{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Aprendizaje por Diferencia Temporal (TD Learning)\n",
        "\n",
        "**Tutorial Completo: De la Teoría a la Práctica**\n",
        "\n",
        "---\n",
        "\n",
        "## Índice\n",
        "\n",
        "1. [Introducción a TD Learning](#1-introducción-a-td-learning)\n",
        "2. [Fundamentos Matemáticos](#2-fundamentos-matemáticos)\n",
        "3. [Q-Learning](#3-q-learning)\n",
        "4. [SARSA](#4-sarsa)\n",
        "5. [Comparación Q-Learning vs SARSA](#5-comparación-q-learning-vs-sarsa)\n",
        "6. [Cliff Walking Problem](#6-cliff-walking-problem)\n",
        "7. [Ejercicios](#7-ejercicios)\n",
        "8. [Conclusiones](#8-conclusiones)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## 1. Introducción a TD Learning\n",
        "\n",
        "### ¿Qué es TD Learning?\n",
        "\n",
        "**Temporal Difference (TD) Learning** combina ideas de:\n",
        "- **Dynamic Programming**: Actualizaciones bootstrapping (usar estimaciones futuras)\n",
        "- **Monte Carlo**: Aprender de experiencia sin modelo\n",
        "\n",
        "### Características Clave\n",
        "\n",
        "1. **Model-free**: No requiere conocer $p(s',r|s,a)$\n",
        "2. **Online**: Aprende después de cada step (no espera fin de episodio)\n",
        "3. **Bootstrapping**: Usa estimaciones de valores futuros para actualizar\n",
        "4. **Eficiente**: Combina ventajas de DP y MC\n",
        "\n",
        "### Aplicaciones\n",
        "\n",
        "- Juegos (AlphaGo usa variantes de TD)\n",
        "- Robótica (control motor)\n",
        "- Optimización de procesos\n",
        "- Finanzas (evaluación de riesgo)\n",
        "\n",
        "### Diferencia con Monte Carlo\n",
        "\n",
        "| Aspecto | Monte Carlo | TD Learning |\n",
        "|---------|-------------|-------------|\n",
        "| **Modelo** | No requiere | No requiere |\n",
        "| **Actualización** | Al final del episodio | Después de cada step |\n",
        "| **Varianza** | Alta | Baja |\n",
        "| **Sesgo** | Bajo | Medio |\n",
        "| **Velocidad** | Lenta | Rápida |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## 2. Fundamentos Matemáticos\n",
        "\n",
        "### 2.1 Error TD (TD Error)\n",
        "\n",
        "La clave de TD Learning es el **error TD**:\n",
        "\n",
        "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
        "\n",
        "Donde:\n",
        "- $R_{t+1}$: Recompensa inmediata\n",
        "- $\\gamma$: Factor de descuento\n",
        "- $V(S_t)$: Estimación actual del estado\n",
        "- $V(S_{t+1})$: Estimación del siguiente estado (bootstrap)\n",
        "\n",
        "### 2.2 Actualización TD\n",
        "\n",
        "La actualización usa el error TD:\n",
        "\n",
        "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\cdot \\delta_t$$\n",
        "\n",
        "Donde $\\alpha$ es la tasa de aprendizaje.\n",
        "\n",
        "### 2.3 TD Target vs TD Error\n",
        "\n",
        "- **TD Target**: $R_{t+1} + \\gamma V(S_{t+1})$ (lo que queremos aproximar)\n",
        "- **TD Error**: Diferencia entre target y estimación actual\n",
        "- **Actualización**: Mover la estimación hacia el target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "### 2.4 Q-Learning vs SARSA\n",
        "\n",
        "**Q-Learning (Off-policy)**:\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
        "\n",
        "Aprende acerca de la **mejor acción posible**, sin importar qué acción tome.\n",
        "\n",
        "**SARSA (On-policy)**:\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$$\n",
        "\n",
        "Aprende acerca de la **acción que realmente toma**, según su política actual."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "### 2.5 Exploración vs Explotación\n",
        "\n",
        "#### Epsilon-Greedy Policy\n",
        "\n",
        "$$\\pi(a|s) = \\begin{cases}\n",
        "1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{si } a = \\arg\\max_a Q(s,a) \\\\\n",
        "\\frac{\\epsilon}{|\\mathcal{A}|} & \\text{si } a \\neq \\arg\\max_a Q(s,a)\n",
        "\\end{cases}$$\n",
        "\n",
        "- **ε pequeño**: Principalmente explotación (usar lo que aprendí)\n",
        "- **ε grande**: Principalmente exploración (probar nuevas acciones)\n",
        "- **ε decay**: Disminuir ε con el tiempo (más exploración al principio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import gymnasium as gym\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Configurar reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "print('✓ Importaciones completadas')\n",
        "print('  - NumPy, Matplotlib, Gymnasium, Seaborn')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## 3. Q-Learning\n",
        "\n",
        "### 3.1 Algoritmo Q-Learning (Off-Policy)\n",
        "\n",
        "Q-Learning aprende la **política óptima** mientras explora con una política diferente.\n",
        "\n",
        "**Algoritmo**:\n",
        "```\n",
        "Inicializar Q(s,a) = 0 para todo s,a\n",
        "Para cada episodio:\n",
        "    s = estado inicial\n",
        "    Para cada step:\n",
        "        a = seleccionar acción con epsilon-greedy\n",
        "        ejecutar a, observar r, s'\n",
        "        Q(s,a) <- Q(s,a) + alpha[r + gamma max_a' Q(s',a') - Q(s,a)]\n",
        "        s = s'\n",
        "    hasta terminar\n",
        "```\n",
        "\n",
        "**Key**: Usa max(a') Q(s',a') en el update (mejor acción posible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"Agente Q-Learning (off-policy)\"\"\"\n",
        "    \n",
        "    def __init__(self, n_actions, alpha=0.1, gamma=0.99, epsilon=1.0,\n",
        "                 epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
        "    \n",
        "    def get_action(self, state, training=True):\n",
        "        \"\"\"Selecciona acción con epsilon-greedy\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.n_actions)\n",
        "        return np.argmax(self.Q[state])\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Actualización Q-Learning\"\"\"\n",
        "        current_q = self.Q[state][action]\n",
        "        \n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            target_q = reward + self.gamma * np.max(self.Q[next_state])\n",
        "        \n",
        "        td_error = target_q - current_q\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decaer epsilon\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "print('✓ QLearningAgent definido')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "### 3.2 Función de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(env, agent, n_episodes=500, max_steps=100, verbose=True):\n",
        "    \"\"\"Entrena agente TD Learning\"\"\"\n",
        "    rewards_history = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        agent.decay_epsilon()\n",
        "        rewards_history.append(episode_reward)\n",
        "        \n",
        "        if verbose and (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards_history[-100:])\n",
        "            print(f'Ep {episode+1}/{n_episodes} | Reward: {avg_reward:.2f} | epsilon: {agent.epsilon:.3f}')\n",
        "    \n",
        "    return rewards_history\n",
        "\n",
        "print('✓ train_agent definida')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "### 3.3 Experimentar con Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear ambiente FrozenLake (determinístico)\n",
        "env_ql = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "print('FrozenLake-v1 (determinístico)')\n",
        "print(f'  Estados: {env_ql.observation_space.n}')\n",
        "print(f'  Acciones: {env_ql.action_space.n}\\n')\n",
        "\n",
        "# Entrenar Q-Learning\n",
        "agent_ql = QLearningAgent(\n",
        "    n_actions=env_ql.action_space.n,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.995\n",
        ")\n",
        "\n",
        "print('Entrenando Q-Learning...')\n",
        "rewards_ql = train_agent(env_ql, agent_ql, n_episodes=500, verbose=True)\n",
        "\n",
        "print(f'\\nReward promedio últimos 100 episodios: {np.mean(rewards_ql[-100:]):.3f}')\n",
        "print(f'Episodios exitosos: {sum([1 for r in rewards_ql[-100:] if r > 0.5])}/100')\n",
        "\n",
        "env_ql.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "### 3.4 Visualizar Aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gráfico de aprendizaje\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Rewards por episodio\n",
        "ax1.plot(rewards_ql, alpha=0.5, label='Reward por episodio')\n",
        "\n",
        "# Promedio móvil\n",
        "window = 50\n",
        "moving_avg = np.convolve(rewards_ql, np.ones(window)/window, mode='valid')\n",
        "ax1.plot(range(window-1, len(rewards_ql)), moving_avg, 'r-', linewidth=2, \n",
        "         label=f'Promedio móvil ({window} eps)')\n",
        "\n",
        "ax1.set_xlabel('Episodio')\n",
        "ax1.set_ylabel('Reward')\n",
        "ax1.set_title('Q-Learning: Convergencia')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Histograma de rewards últimos 100\n",
        "ax2.hist(rewards_ql[-100:], bins=20, edgecolor='black', alpha=0.7)\n",
        "ax2.set_xlabel('Reward')\n",
        "ax2.set_ylabel('Frecuencia')\n",
        "ax2.set_title('Q-Learning: Distribución de Rewards (últimos 100 ep)')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Q-Learning aprendió exitosamente!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "## 4. SARSA\n",
        "\n",
        "### 4.1 Algoritmo SARSA (On-Policy)\n",
        "\n",
        "SARSA aprende sobre la **política que está siguiendo** (on-policy).\n",
        "\n",
        "**Algoritmo**:\n",
        "```\n",
        "Inicializar Q(s,a) = 0 para todo s,a\n",
        "Para cada episodio:\n",
        "    s = estado inicial\n",
        "    a = seleccionar acción con epsilon-greedy desde s\n",
        "    Para cada step:\n",
        "        ejecutar a, observar r, s'\n",
        "        a' = seleccionar acción con epsilon-greedy desde s'\n",
        "        Q(s,a) <- Q(s,a) + alpha[r + gamma Q(s',a') - Q(s,a)]\n",
        "        s = s', a = a'\n",
        "    hasta terminar\n",
        "```\n",
        "\n",
        "**Key**: Usa Q(s',a') donde a' es la acción que realmente se tomará (on-policy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SARSAAgent:\n",
        "    \"\"\"Agente SARSA (on-policy)\"\"\"\n",
        "    \n",
        "    def __init__(self, n_actions, alpha=0.1, gamma=0.99, epsilon=1.0,\n",
        "                 epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
        "    \n",
        "    def get_action(self, state, training=True):\n",
        "        \"\"\"Selecciona acción con epsilon-greedy\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.n_actions)\n",
        "        return np.argmax(self.Q[state])\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, next_action, done):\n",
        "        \"\"\"Actualización SARSA\"\"\"\n",
        "        current_q = self.Q[state][action]\n",
        "        \n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            target_q = reward + self.gamma * self.Q[next_state][next_action]\n",
        "        \n",
        "        td_error = target_q - current_q\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decaer epsilon\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "print('✓ SARSAAgent definido')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "### 4.2 Función de Entrenamiento SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_sarsa(env, agent, n_episodes=500, max_steps=100, verbose=True):\n",
        "    \"\"\"Entrena agente SARSA\"\"\"\n",
        "    rewards_history = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        action = agent.get_action(state)\n",
        "        episode_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            next_action = agent.get_action(next_state)\n",
        "            \n",
        "            agent.update(state, action, reward, next_state, next_action, done)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        agent.decay_epsilon()\n",
        "        rewards_history.append(episode_reward)\n",
        "        \n",
        "        if verbose and (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards_history[-100:])\n",
        "            print(f'Ep {episode+1}/{n_episodes} | Reward: {avg_reward:.2f} | epsilon: {agent.epsilon:.3f}')\n",
        "    \n",
        "    return rewards_history\n",
        "\n",
        "print('✓ train_sarsa definida')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {},
      "source": [
        "### 4.3 Experimentar con SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear ambiente (mismo que para Q-Learning)\n",
        "env_sarsa = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "# Entrenar SARSA\n",
        "agent_sarsa = SARSAAgent(\n",
        "    n_actions=env_sarsa.action_space.n,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.995\n",
        ")\n",
        "\n",
        "print('Entrenando SARSA...')\n",
        "rewards_sarsa = train_sarsa(env_sarsa, agent_sarsa, n_episodes=500, verbose=True)\n",
        "\n",
        "print(f'\\nReward promedio últimos 100 episodios: {np.mean(rewards_sarsa[-100:]):.3f}')\n",
        "print(f'Episodios exitosos: {sum([1 for r in rewards_sarsa[-100:] if r > 0.5])}/100')\n",
        "\n",
        "env_sarsa.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-20",
      "metadata": {},
      "source": [
        "### 4.4 Visualizar Aprendizaje SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gráfico de aprendizaje SARSA\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Rewards por episodio\n",
        "ax1.plot(rewards_sarsa, alpha=0.5, label='Reward por episodio')\n",
        "\n",
        "# Promedio móvil\n",
        "window = 50\n",
        "moving_avg = np.convolve(rewards_sarsa, np.ones(window)/window, mode='valid')\n",
        "ax1.plot(range(window-1, len(rewards_sarsa)), moving_avg, 'g-', linewidth=2,\n",
        "         label=f'Promedio móvil ({window} eps)')\n",
        "\n",
        "ax1.set_xlabel('Episodio')\n",
        "ax1.set_ylabel('Reward')\n",
        "ax1.set_title('SARSA: Convergencia')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Histograma\n",
        "ax2.hist(rewards_sarsa[-100:], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
        "ax2.set_xlabel('Reward')\n",
        "ax2.set_ylabel('Frecuencia')\n",
        "ax2.set_title('SARSA: Distribución de Rewards (últimos 100 ep)')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('SARSA aprendió exitosamente!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "## 5. Comparación Q-Learning vs SARSA\n",
        "\n",
        "### 5.1 Diferencias Teóricas\n",
        "\n",
        "| Aspecto | Q-Learning | SARSA |\n",
        "|---------|-----------|-------|\n",
        "| **Policy** | Off-policy | On-policy |\n",
        "| **Qué aprende** | Política óptima | Política actual |\n",
        "| **Target** | max(a') Q(s',a') | Q(s',a') |\n",
        "| **Riesgo** | Menor | Mayor |\n",
        "| **Convergencia** | Garantizado | Garantizado |\n",
        "| **Estabilidad** | Media | Alta |\n",
        "\n",
        "### 5.2 Cuándo Usar Cada Uno\n",
        "\n",
        "**Q-Learning**: Cuando seguridad es importante, simulación offline, exploración agresiva\n",
        "\n",
        "**SARSA**: Cuando el agente debe ser cauteloso, learning online, cuando la exploración tiene riesgo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {},
      "source": [
        "### 5.3 Comparación Empírica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar múltiples veces para obtener estadísticas\n",
        "n_runs = 5\n",
        "n_episodes = 500\n",
        "\n",
        "print(f'Ejecutando {n_runs} runs de entrenamiento...\\n')\n",
        "\n",
        "all_ql_rewards = []\n",
        "all_sarsa_rewards = []\n",
        "\n",
        "for run in range(n_runs):\n",
        "    # Q-Learning\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    agent_ql = QLearningAgent(env.action_space.n, alpha=0.1, gamma=0.99, epsilon=1.0)\n",
        "    ql_rewards = train_agent(env, agent_ql, n_episodes=n_episodes, verbose=False)\n",
        "    all_ql_rewards.append(ql_rewards)\n",
        "    env.close()\n",
        "    \n",
        "    # SARSA\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "    agent_sarsa = SARSAAgent(env.action_space.n, alpha=0.1, gamma=0.99, epsilon=1.0)\n",
        "    sarsa_rewards = train_sarsa(env, agent_sarsa, n_episodes=n_episodes, verbose=False)\n",
        "    all_sarsa_rewards.append(sarsa_rewards)\n",
        "    env.close()\n",
        "    \n",
        "    print(f'Run {run+1}/{n_runs} completado')\n",
        "\n",
        "# Calcular promedios\n",
        "avg_ql = np.mean(all_ql_rewards, axis=0)\n",
        "avg_sarsa = np.mean(all_sarsa_rewards, axis=0)\n",
        "\n",
        "print('\\n✓ Entrenamiento completado')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-25",
      "metadata": {},
      "source": [
        "### 5.4 Visualizar Comparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparación visual\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Grafica 1: Trayectorias Q-Learning\n",
        "ax = axes[0, 0]\n",
        "window = 50\n",
        "for i in range(n_runs):\n",
        "    moving_avg_ql = np.convolve(all_ql_rewards[i], np.ones(window)/window, mode='valid')\n",
        "    ax.plot(range(window-1, n_episodes), moving_avg_ql, alpha=0.3, color='blue')\n",
        "\n",
        "moving_avg_ql_mean = np.convolve(avg_ql, np.ones(window)/window, mode='valid')\n",
        "ax.plot(range(window-1, n_episodes), moving_avg_ql_mean, 'b-', linewidth=3, label='Q-Learning (promedio)')\n",
        "ax.set_xlabel('Episodio')\n",
        "ax.set_ylabel('Reward')\n",
        "ax.set_title('Q-Learning: Convergencia')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Grafica 2: Trayectorias SARSA\n",
        "ax = axes[0, 1]\n",
        "for i in range(n_runs):\n",
        "    moving_avg_sarsa = np.convolve(all_sarsa_rewards[i], np.ones(window)/window, mode='valid')\n",
        "    ax.plot(range(window-1, n_episodes), moving_avg_sarsa, alpha=0.3, color='green')\n",
        "\n",
        "moving_avg_sarsa_mean = np.convolve(avg_sarsa, np.ones(window)/window, mode='valid')\n",
        "ax.plot(range(window-1, n_episodes), moving_avg_sarsa_mean, 'g-', linewidth=3, label='SARSA (promedio)')\n",
        "ax.set_xlabel('Episodio')\n",
        "ax.set_ylabel('Reward')\n",
        "ax.set_title('SARSA: Convergencia')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Grafica 3: Comparación directa\n",
        "ax = axes[1, 0]\n",
        "ax.plot(range(window-1, n_episodes), moving_avg_ql_mean, 'b-', linewidth=2, label='Q-Learning')\n",
        "ax.plot(range(window-1, n_episodes), moving_avg_sarsa_mean, 'g-', linewidth=2, label='SARSA')\n",
        "ax.set_xlabel('Episodio')\n",
        "ax.set_ylabel('Reward (promedio móvil)')\n",
        "ax.set_title('Comparación: Q-Learning vs SARSA')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Grafica 4: Performance final\n",
        "ax = axes[1, 1]\n",
        "final_ql = np.mean(all_ql_rewards, axis=0)[-100:]\n",
        "final_sarsa = np.mean(all_sarsa_rewards, axis=0)[-100:]\n",
        "\n",
        "algorithms = ['Q-Learning', 'SARSA']\n",
        "means = [np.mean(final_ql), np.mean(final_sarsa)]\n",
        "stds = [np.std(final_ql), np.std(final_sarsa)]\n",
        "\n",
        "x = np.arange(len(algorithms))\n",
        "ax.bar(x, means, yerr=stds, capsize=10, alpha=0.7, color=['blue', 'green'])\n",
        "ax.set_ylabel('Reward Promedio')\n",
        "ax.set_title('Performance Final (últimos 100 episodios)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(algorithms)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for i, (mean, std) in enumerate(zip(means, stds)):\n",
        "    ax.text(i, mean + std + 0.05, f'{mean:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nQ-Learning promedio final: {np.mean(final_ql):.3f} ± {np.std(final_ql):.3f}')\n",
        "print(f'SARSA promedio final: {np.mean(final_sarsa):.3f} ± {np.std(final_sarsa):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-27",
      "metadata": {},
      "source": [
        "### 5.5 Conclusiones de la Comparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-28",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*60)\n",
        "print('ANÁLISIS COMPARATIVO: Q-LEARNING vs SARSA')\n",
        "print('='*60)\n",
        "\n",
        "ql_final = np.mean([np.mean(r[-100:]) for r in all_ql_rewards])\n",
        "sarsa_final = np.mean([np.mean(r[-100:]) for r in all_sarsa_rewards])\n",
        "\n",
        "print(f'\\nReward Promedio Final:')\n",
        "print(f'  Q-Learning: {ql_final:.4f}')\n",
        "print(f'  SARSA:      {sarsa_final:.4f}')\n",
        "print(f'  Diferencia: {abs(ql_final - sarsa_final):.4f}')\n",
        "\n",
        "print(f'\\nNota: En entornos determinísticos, ambos convergen similar.')\n",
        "print(f'La diferencia es más notable en entornos estocásticos.')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-29",
      "metadata": {},
      "source": [
        "## 6. Cliff Walking Problem\n",
        "\n",
        "### 6.1 Descripción del Problema\n",
        "\n",
        "El **Cliff Walking** es un entorno clásico donde:\n",
        "- Grid 4x12 (fila x columna)\n",
        "- Agente comienza en abajo-izquierda (3,0)\n",
        "- Meta en abajo-derecha (3,11)\n",
        "- Un acantilado (cliff) en (3, 1-10) con recompensa -100\n",
        "- Cada paso normal: recompensa -1\n",
        "\n",
        "El dilema:\n",
        "- **Ruta arriesgada**: Cerca del borde (rápida pero -100 si cae)\n",
        "- **Ruta segura**: Lejos del borde (lenta pero segura)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-30",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear entorno Cliff Walking personalizado\n",
        "class CliffWalkingEnv:\n",
        "    def __init__(self):\n",
        "        self.grid_shape = (4, 12)\n",
        "        self.start = (3, 0)\n",
        "        self.goal = (3, 11)\n",
        "        self.current_pos = self.start\n",
        "        self.action_names = ['up', 'right', 'down', 'left']\n",
        "        self.state = self._pos_to_state(self.start)\n",
        "    \n",
        "    def _pos_to_state(self, pos):\n",
        "        return pos[0] * self.grid_shape[1] + pos[1]\n",
        "    \n",
        "    def _state_to_pos(self, state):\n",
        "        return (state // self.grid_shape[1], state % self.grid_shape[1])\n",
        "    \n",
        "    def _is_cliff(self, pos):\n",
        "        return pos[0] == 3 and 1 <= pos[1] <= 10\n",
        "    \n",
        "    def reset(self):\n",
        "        self.current_pos = self.start\n",
        "        self.state = self._pos_to_state(self.start)\n",
        "        return self.state, {}\n",
        "    \n",
        "    def step(self, action):\n",
        "        row, col = self.current_pos\n",
        "        \n",
        "        if action == 0:  # up\n",
        "            row = max(0, row - 1)\n",
        "        elif action == 1:  # right\n",
        "            col = min(self.grid_shape[1] - 1, col + 1)\n",
        "        elif action == 2:  # down\n",
        "            row = min(self.grid_shape[0] - 1, row + 1)\n",
        "        elif action == 3:  # left\n",
        "            col = max(0, col - 1)\n",
        "        \n",
        "        new_pos = (row, col)\n",
        "        \n",
        "        if self._is_cliff(new_pos):\n",
        "            reward = -100\n",
        "            new_pos = self.start\n",
        "            done = True\n",
        "        elif new_pos == self.goal:\n",
        "            reward = 0\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        \n",
        "        self.current_pos = new_pos\n",
        "        self.state = self._pos_to_state(new_pos)\n",
        "        \n",
        "        return self.state, reward, done, False, {}\n",
        "\n",
        "env_cliff = CliffWalkingEnv()\n",
        "print('✓ Entorno Cliff Walking creado')\n",
        "print(f'Grid: {env_cliff.grid_shape[0]}x{env_cliff.grid_shape[1]}')\n",
        "print(f'Total de estados: {env_cliff.grid_shape[0] * env_cliff.grid_shape[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-31",
      "metadata": {},
      "source": [
        "### 6.2 Entrenar en Cliff Walking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar Q-Learning en Cliff Walking\n",
        "print('Entrenando Q-Learning en Cliff Walking...')\n",
        "agent_ql_cliff = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
        "\n",
        "ql_cliff_rewards = []\n",
        "for episode in range(500):\n",
        "    state, _ = env_cliff.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for step in range(100):\n",
        "        action = agent_ql_cliff.get_action(state)\n",
        "        next_state, reward, done, _, _ = env_cliff.step(action)\n",
        "        agent_ql_cliff.update(state, action, reward, next_state, done)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    agent_ql_cliff.decay_epsilon()\n",
        "    ql_cliff_rewards.append(episode_reward)\n",
        "    \n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg = np.mean(ql_cliff_rewards[-100:])\n",
        "        print(f'Episodio {episode+1}: Avg Reward = {avg:.2f}')\n",
        "\n",
        "print('\\n✓ Q-Learning entrenado')\n",
        "\n",
        "# Entrenar SARSA en Cliff Walking\n",
        "print('\\nEntrenando SARSA en Cliff Walking...')\n",
        "agent_sarsa_cliff = SARSAAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
        "\n",
        "sarsa_cliff_rewards = []\n",
        "for episode in range(500):\n",
        "    state, _ = env_cliff.reset()\n",
        "    action = agent_sarsa_cliff.get_action(state)\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for step in range(100):\n",
        "        next_state, reward, done, _, _ = env_cliff.step(action)\n",
        "        next_action = agent_sarsa_cliff.get_action(next_state)\n",
        "        agent_sarsa_cliff.update(state, action, reward, next_state, next_action, done)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    agent_sarsa_cliff.decay_epsilon()\n",
        "    sarsa_cliff_rewards.append(episode_reward)\n",
        "    \n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg = np.mean(sarsa_cliff_rewards[-100:])\n",
        "        print(f'Episodio {episode+1}: Avg Reward = {avg:.2f}')\n",
        "\n",
        "print('\\n✓ SARSA entrenado')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-33",
      "metadata": {},
      "source": [
        "### 6.3 Comparar Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar rewards\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "window = 50\n",
        "moving_avg_ql = np.convolve(ql_cliff_rewards, np.ones(window)/window, mode='valid')\n",
        "moving_avg_sarsa = np.convolve(sarsa_cliff_rewards, np.ones(window)/window, mode='valid')\n",
        "\n",
        "axes[0].plot(range(window-1, 500), moving_avg_ql, 'b-', linewidth=2, label='Q-Learning')\n",
        "axes[0].plot(range(window-1, 500), moving_avg_sarsa, 'g-', linewidth=2, label='SARSA')\n",
        "axes[0].set_xlabel('Episodio')\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].set_title('Cliff Walking: Comparación de Algoritmos')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Performance final\n",
        "final_ql = np.mean(ql_cliff_rewards[-100:])\n",
        "final_sarsa = np.mean(sarsa_cliff_rewards[-100:])\n",
        "\n",
        "algorithms = ['Q-Learning', 'SARSA']\n",
        "rewards = [final_ql, final_sarsa]\n",
        "colors = ['blue', 'green']\n",
        "\n",
        "axes[1].bar(algorithms, rewards, color=colors, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_ylabel('Reward Promedio')\n",
        "axes[1].set_title('Performance Final (últimos 100 episodios)')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for i, (algo, reward) in enumerate(zip(algorithms, rewards)):\n",
        "    axes[1].text(i, reward-5, f'{reward:.1f}', ha='center', fontweight='bold', color='white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nQ-Learning reward promedio: {final_ql:.2f}')\n",
        "print(f'SARSA reward promedio: {final_sarsa:.2f}')\n",
        "print(f'\\nObservación: Q-Learning toma ruta arriesgada (óptima).')\n",
        "print(f'SARSA es más cauteloso (evita el cliff durante entrenamiento).')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-35",
      "metadata": {},
      "source": [
        "## 7. Ejercicios\n",
        "\n",
        "### Ejercicio 1: Hiperparámetros\n",
        "Experimenta con diferentes valores de alpha, gamma y epsilon_decay.\n",
        "¿Cómo afectan la convergencia?\n",
        "\n",
        "### Ejercicio 2: Expected SARSA\n",
        "Implementa Expected SARSA que usa la esperanza sobre la política.\n",
        "¿Es mejor que SARSA?\n",
        "\n",
        "### Ejercicio 3: Escalabilidad\n",
        "¿Cómo escalan Q-Learning y SARSA con tamaños mayores del problema?\n",
        "\n",
        "### Ejercicio 4: Exploración\n",
        "¿Qué pasa con diferentes políticas de exploración?\n",
        "\n",
        "### Ejercicio 5: Análisis\n",
        "Compara convergencia en entornos determinísticos vs estocásticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 1: Hiperparámetros\n",
        "# Experimenta con diferentes alpha, gamma, epsilon_decay\n",
        "# Tu código aquí...\n",
        "\n",
        "print('Completa el Ejercicio 1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 2: Expected SARSA\n",
        "# Implementa Expected SARSA\n",
        "\n",
        "class ExpectedSARSAAgent:\n",
        "    \"\"\"Implementa Expected SARSA\"\"\"\n",
        "    # Completa esta implementación\n",
        "    pass\n",
        "\n",
        "print('Completa el Ejercicio 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 3: Escalabilidad\n",
        "# Prueba con diferentes tamaños de problema\n",
        "# Tu código aquí...\n",
        "\n",
        "print('Completa el Ejercicio 3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 4: Exploración vs Explotación\n",
        "# Analiza el impacto de diferentes valores de epsilon\n",
        "# Tu código aquí...\n",
        "\n",
        "print('Completa el Ejercicio 4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJERCICIO 5: Análisis Adicional\n",
        "# Compara comportamiento en entornos determinísticos vs estocásticos\n",
        "# Tu código aquí...\n",
        "\n",
        "print('Completa el Ejercicio 5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-41",
      "metadata": {},
      "source": [
        "## 8. Conclusiones\n",
        "\n",
        "### Puntos Clave Aprendidos\n",
        "\n",
        "1. **TD Learning combina DP y MC**:\n",
        "   - Como DP: usa bootstrapping (estimaciones futuras)\n",
        "   - Como MC: aprende de experiencia sin modelo\n",
        "\n",
        "2. **Q-Learning (off-policy)**:\n",
        "   - Aprende la política óptima\n",
        "   - Puede explorar agresivamente\n",
        "   - Mejor para offline learning\n",
        "\n",
        "3. **SARSA (on-policy)**:\n",
        "   - Aprende sobre la política que está usando\n",
        "   - Más cauteloso\n",
        "   - Mejor cuando la exploración tiene costo\n",
        "\n",
        "4. **Convergencia garantizada**:\n",
        "   - Bajo condiciones apropiadas\n",
        "   - Garantía de convergencia a Q*\n",
        "\n",
        "### Limitaciones y Extensiones\n",
        "\n",
        "Limitaciones:\n",
        "- Requiere discretización de estados\n",
        "- Lento para espacios grandes\n",
        "- Puede sobreestimar valores\n",
        "\n",
        "Extensiones:\n",
        "- Double Q-Learning: Reduce sobrestimación\n",
        "- Dueling Q-Learning: Separa valor y ventaja\n",
        "- Deep Q-Networks (DQN): Usa redes neuronales\n",
        "- Policy Gradient Methods: Aprende política directamente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-42",
      "metadata": {},
      "source": [
        "### Referencias Clave\n",
        "\n",
        "1. **Sutton & Barto (2018)**: *Reinforcement Learning: An Introduction*\n",
        "   - Capítulo 6: Temporal-Difference Learning\n",
        "   - Capítulo 7: Multi-step Bootstrapping\n",
        "\n",
        "2. **Watkins & Dayan (1992)**: \"Q-learning\" (Paper seminal)\n",
        "\n",
        "3. **Rummery & Niranjan (1994)**: \"On-line Q-learning using connectionist systems\"\n",
        "\n",
        "### Próximos Pasos\n",
        "\n",
        "Después de dominar TD Learning:\n",
        "1. Aproximación de funciones y escalabilidad\n",
        "2. Redes neuronales profundas (DQN)\n",
        "3. Métodos de gradiente de política (Policy Gradient)\n",
        "4. Actor-Critic methods\n",
        "5. Aprendizaje por refuerzo modelo-based\n",
        "\n",
        "---\n",
        "\n",
        "**¡Felicidades!** Has aprendido los algoritmos fundamentales de TD Learning."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
