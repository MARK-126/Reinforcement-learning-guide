{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this notebook, you will be able to:\n",
    "* Understand TD learning and how it combines Dynamic Programming and Monte Carlo methods\n",
    "* Implement and train Q-Learning agents (off-policy TD control)\n",
    "* Implement and train SARSA agents (on-policy TD control)\n",
    "* Implement and train Expected SARSA agents\n",
    "* Compare different TD learning algorithms and understand their trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Introduction to TD Learning](#2)\n",
    "    - [2.1 - TD Learning Fundamentals](#2-1)\n",
    "    - [2.2 - The TD Error](#2-2)\n",
    "- [3 - Q-Learning](#3)\n",
    "    - [Exercise 1 - implement_q_learning](#ex-1)\n",
    "    - [3.1 - Training Q-Learning](#3-1)\n",
    "- [4 - SARSA](#4)\n",
    "    - [Exercise 2 - implement_sarsa](#ex-2)\n",
    "    - [4.1 - Training SARSA](#4-1)\n",
    "- [5 - Expected SARSA](#5)\n",
    "    - [Exercise 3 - implement_expected_sarsa](#ex-3)\n",
    "    - [5.1 - Training Expected SARSA](#5-1)\n",
    "- [6 - Algorithm Comparison](#6)\n",
    "    - [Exercise 4 - compare_td_methods](#ex-4)\n",
    "    - [6.1 - Comprehensive Analysis](#6-1)\n",
    "- [7 - Cliff Walking Problem](#7)\n",
    "- [8 - Summary and Key Takeaways](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### v1.0\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import TD utilities\n",
    "from td_utils import (\n",
    "    QLearningAgent, SARSAAgent, ExpectedSARSAAgent,\n",
    "    train_q_learning, train_sarsa, train_expected_sarsa,\n",
    "    test_q_learning_agent, test_sarsa_agent, test_expected_sarsa_agent,\n",
    "    test_td_error_calculation, test_epsilon_decay,\n",
    "    plot_training_curves, plot_comparison_bars\n",
    ")\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Set reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('✓ All packages imported successfully')\n",
    "print('  - NumPy, Matplotlib, Seaborn, Gymnasium')\n",
    "print('  - TD Learning utilities loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Introduction to TD Learning\n",
    "\n",
    "**Temporal Difference (TD) Learning** is a fundamental paradigm in reinforcement learning that combines ideas from:\n",
    "- **Dynamic Programming**: Bootstrap with estimated future values\n",
    "- **Monte Carlo Methods**: Learn from actual experience without a model\n",
    "\n",
    "### Key Characteristics of TD Learning:\n",
    "1. **Model-free**: No need for $p(s',r|s,a)$\n",
    "2. **Online learning**: Learn after each step (not waiting for episode termination)\n",
    "3. **Bootstrapping**: Use estimates of future values to update current estimates\n",
    "4. **Efficient**: Combines advantages of DP and MC methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - TD Learning Fundamentals\n",
    "\n",
    "#### Comparison: Monte Carlo vs TD Learning\n",
    "\n",
    "| Aspect | Monte Carlo | TD Learning | Dynamic Programming |\n",
    "|--------|-------------|-------------|---------------------|\n",
    "| **Requires model** | No | No | Yes |\n",
    "| **Update timing** | End of episode | After each step | Each step |\n",
    "| **Uses bootstrapping** | No | Yes | Yes |\n",
    "| **Variance** | High | Low-Medium | Low |\n",
    "| **Bias** | Low | Medium | Medium |\n",
    "| **Convergence speed** | Slow | Fast | Fast (if model known) |\n",
    "| **Sample efficiency** | Low | Medium | High (if model perfect) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - The TD Error\n",
    "\n",
    "The core of TD learning is the **TD Error**, which measures how much our current estimate differs from the \"TD target\":\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "\n",
    "Where:\n",
    "- $R_{t+1}$: Immediate reward received\n",
    "- $\\gamma$: Discount factor (0 ≤ γ ≤ 1)\n",
    "- $V(S_t)$: Current value estimate for state $S_t$\n",
    "- $V(S_{t+1})$: Bootstrap estimate for next state (this is the key!)\n",
    "\n",
    "**The TD Update Rule:**\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\cdot \\delta_t$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "**Key Insight**: We move the current estimate towards the TD target by an amount proportional to the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run unit tests\n",
    "print(\"Testing TD Error calculation...\")\n",
    "test_td_error_calculation()\n",
    "test_epsilon_decay()\n",
    "print(\"\\n✓ Unit tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Q-Learning\n",
    "\n",
    "### 3.0 - Q-Learning Algorithm (Off-Policy)\n",
    "\n",
    "Q-Learning learns the **optimal policy** while exploring with a different policy (epsilon-greedy).\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "Initialize Q(s,a) = 0 for all s,a\n",
    "For each episode:\n",
    "    s = initial state\n",
    "    For each step:\n",
    "        a = select action using epsilon-greedy policy\n",
    "        Execute a, observe r, s'\n",
    "        Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]  ← MAX is key!\n",
    "        s = s'\n",
    "    until terminal state reached\n",
    "```\n",
    "\n",
    "**Off-Policy Definition**: The learning target uses the best possible next action (max), regardless of which action the policy actually explores.\n",
    "\n",
    "**Update Rule**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - implement_q_learning\n",
    "\n",
    "Implement the Q-Learning update rule. Your function should:\n",
    "1. Calculate the current Q-value for the state-action pair\n",
    "2. Calculate the TD target (reward + gamma * max next Q-value)\n",
    "3. Update the Q-value using the TD error\n",
    "\n",
    "**Hints:**\n",
    "- Use `self.Q[state][action]` to access/update Q-values\n",
    "- Use `np.max(self.Q[next_state])` to get the maximum Q-value for the next state\n",
    "- TD error = target - current estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_q_learning\n",
    "\n",
    "def implement_q_learning(Q, state, action, reward, next_state, done, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Implement the Q-Learning update rule.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-value table (dictionary)\n",
    "    state -- current state\n",
    "    action -- action taken\n",
    "    reward -- reward received\n",
    "    next_state -- resulting state\n",
    "    done -- whether episode terminated\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    Q -- updated Q-value table\n",
    "    td_error -- temporal difference error\n",
    "    \"\"\"\n",
    "    # (approx. 6 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    current_q = Q[state][action]\n",
    "    \n",
    "    if done:\n",
    "        target_q = reward\n",
    "    else:\n",
    "        # Q-Learning: use MAX action value\n",
    "        target_q = reward + gamma * np.max(Q[next_state])\n",
    "    \n",
    "    td_error = target_q - current_q\n",
    "    Q[state][action] += alpha * td_error\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, td_error\n",
    "\n",
    "print(\"✓ Q-Learning update rule implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "def test_implement_q_learning():\n",
    "    # Initialize Q-values\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    \n",
    "    # Test case 1: Update with non-terminal state\n",
    "    Q[0] = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "    Q[1] = np.array([0.2, 0.3, 0.5, 0.1])\n",
    "    \n",
    "    Q, td_error = implement_q_learning(Q, state=0, action=0, reward=1.0, \n",
    "                                       next_state=1, done=False, alpha=0.1, gamma=0.99)\n",
    "    \n",
    "    # Expected: target = 1.0 + 0.99 * max([0.2, 0.3, 0.5, 0.1]) = 1.0 + 0.99*0.5 = 1.495\n",
    "    # current = 0.1\n",
    "    # td_error = 1.495 - 0.1 = 1.395\n",
    "    # Q[0][0] = 0.1 + 0.1*1.395 = 0.2395\n",
    "    \n",
    "    expected_q = 0.1 + 0.1 * (1.0 + 0.99 * 0.5 - 0.1)\n",
    "    assert np.isclose(Q[0][0], expected_q), f\"Expected {expected_q}, got {Q[0][0]}\"\n",
    "    \n",
    "    # Test case 2: Terminal state\n",
    "    Q, td_error = implement_q_learning(Q, state=0, action=1, reward=2.0, \n",
    "                                       next_state=2, done=True, alpha=0.1, gamma=0.99)\n",
    "    \n",
    "    # For terminal: target = reward = 2.0\n",
    "    # Q[0][1] = 0.2 + 0.1 * (2.0 - 0.2) = 0.2 + 0.18 = 0.38\n",
    "    \n",
    "    expected_q = 0.2 + 0.1 * (2.0 - 0.2)\n",
    "    assert np.isclose(Q[0][1], expected_q), f\"Expected {expected_q}, got {Q[0][1]}\"\n",
    "    \n",
    "    print(\"✓ All Q-Learning update tests passed!\")\n",
    "\n",
    "test_implement_q_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Training Q-Learning\n",
    "\n",
    "Now let's train a Q-Learning agent on the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment (deterministic)\n",
    "env_ql = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "print('Environment: FrozenLake-v1 (deterministic)')\n",
    "print(f'  States: {env_ql.observation_space.n}')\n",
    "print(f'  Actions: {env_ql.action_space.n}')\n",
    "print(f'  Goal: Reach the frisbee without falling into holes\\n')\n",
    "\n",
    "# Initialize Q-Learning agent\n",
    "agent_ql = QLearningAgent(\n",
    "    n_actions=env_ql.action_space.n,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "print('Training Q-Learning agent...')\n",
    "rewards_ql = train_q_learning(env_ql, agent_ql, n_episodes=500, verbose=True)\n",
    "print(f'\\nTraining complete!')\n",
    "\n",
    "env_ql.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-Learning performance\n",
    "final_100_ql = rewards_ql[-100:]\n",
    "success_rate_ql = sum([1 for r in final_100_ql if r > 0.5]) / len(final_100_ql)\n",
    "\n",
    "print(f\"Q-Learning Performance (last 100 episodes):\")\n",
    "print(f\"  Average reward: {np.mean(final_100_ql):.3f}\")\n",
    "print(f\"  Std deviation: {np.std(final_100_ql):.3f}\")\n",
    "print(f\"  Success rate: {success_rate_ql*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- Q-Learning is **off-policy**: learns the optimal policy while exploring with a different policy\n",
    "- The **max operator** in the update rule is crucial for learning the optimal value function\n",
    "- Q-Learning can be aggressive because it assumes optimal behavior in the future\n",
    "- Convergence is guaranteed under appropriate conditions (GLIE = Greedy in the Limit with Infinite Exploration)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - SARSA\n",
    "\n",
    "### 4.0 - SARSA Algorithm (On-Policy)\n",
    "\n",
    "SARSA learns about the **policy being followed** during exploration (on-policy).\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "Initialize Q(s,a) = 0 for all s,a\n",
    "For each episode:\n",
    "    s = initial state\n",
    "    a = select action using epsilon-greedy policy\n",
    "    For each step:\n",
    "        Execute a, observe r, s'\n",
    "        a' = select action using epsilon-greedy policy from s'\n",
    "        Q(s,a) ← Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]  ← Uses actual next action!\n",
    "        s = s', a = a'\n",
    "    until terminal state reached\n",
    "```\n",
    "\n",
    "**On-Policy Definition**: The learning target uses the Q-value of the actual action that will be taken (a'), not necessarily the best action.\n",
    "\n",
    "**Update Rule**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R_{t+1} + \\gamma Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "Where $a'$ is the actual next action selected by the epsilon-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - implement_sarsa\n",
    "\n",
    "Implement the SARSA update rule. Your function should:\n",
    "1. Calculate the current Q-value for the state-action pair\n",
    "2. Calculate the TD target (reward + gamma * Q-value of actual next action)\n",
    "3. Update the Q-value using the TD error\n",
    "\n",
    "**Key Difference from Q-Learning**: Use `next_action` parameter instead of max over all actions.\n",
    "\n",
    "**Hints:**\n",
    "- The only difference from Q-Learning is in step 2\n",
    "- Use `self.Q[next_state][next_action]` instead of `np.max(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_sarsa\n",
    "\n",
    "def implement_sarsa(Q, state, action, reward, next_state, next_action, done, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Implement the SARSA update rule.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-value table (dictionary)\n",
    "    state -- current state\n",
    "    action -- action taken\n",
    "    reward -- reward received\n",
    "    next_state -- resulting state\n",
    "    next_action -- next action that will be taken (actual policy action)\n",
    "    done -- whether episode terminated\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    Q -- updated Q-value table\n",
    "    td_error -- temporal difference error\n",
    "    \"\"\"\n",
    "    # (approx. 6 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    current_q = Q[state][action]\n",
    "    \n",
    "    if done:\n",
    "        target_q = reward\n",
    "    else:\n",
    "        # SARSA: use actual next action's Q-value\n",
    "        target_q = reward + gamma * Q[next_state][next_action]\n",
    "    \n",
    "    td_error = target_q - current_q\n",
    "    Q[state][action] += alpha * td_error\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, td_error\n",
    "\n",
    "print(\"✓ SARSA update rule implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "def test_implement_sarsa():\n",
    "    # Initialize Q-values\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    \n",
    "    # Test case 1: Update with non-terminal state\n",
    "    Q[0] = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "    Q[1] = np.array([0.2, 0.3, 0.5, 0.1])\n",
    "    \n",
    "    next_action = 2  # Will take action 2 in next state\n",
    "    Q, td_error = implement_sarsa(Q, state=0, action=0, reward=1.0, \n",
    "                                   next_state=1, next_action=next_action, done=False, alpha=0.1, gamma=0.99)\n",
    "    \n",
    "    # Expected: target = 1.0 + 0.99 * Q[1][2] = 1.0 + 0.99*0.5 = 1.495\n",
    "    expected_q = 0.1 + 0.1 * (1.0 + 0.99 * Q[1][next_action] - 0.1)\n",
    "    assert np.isclose(Q[0][0], expected_q), f\"Expected {expected_q}, got {Q[0][0]}\"\n",
    "    \n",
    "    # Test case 2: Terminal state\n",
    "    Q, td_error = implement_sarsa(Q, state=0, action=1, reward=2.0, \n",
    "                                   next_state=2, next_action=1, done=True, alpha=0.1, gamma=0.99)\n",
    "    \n",
    "    # For terminal: target = reward = 2.0\n",
    "    expected_q = 0.2 + 0.1 * (2.0 - 0.2)\n",
    "    assert np.isclose(Q[0][1], expected_q), f\"Expected {expected_q}, got {Q[0][1]}\"\n",
    "    \n",
    "    print(\"✓ All SARSA update tests passed!\")\n",
    "\n",
    "test_implement_sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Training SARSA\n",
    "\n",
    "Now let's train a SARSA agent on the same FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment\n",
    "env_sarsa = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "# Initialize SARSA agent\n",
    "agent_sarsa = SARSAAgent(\n",
    "    n_actions=env_sarsa.action_space.n,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "print('Training SARSA agent...')\n",
    "rewards_sarsa = train_sarsa(env_sarsa, agent_sarsa, n_episodes=500, verbose=True)\n",
    "print(f'\\nTraining complete!')\n",
    "\n",
    "env_sarsa.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SARSA performance\n",
    "final_100_sarsa = rewards_sarsa[-100:]\n",
    "success_rate_sarsa = sum([1 for r in final_100_sarsa if r > 0.5]) / len(final_100_sarsa)\n",
    "\n",
    "print(f\"SARSA Performance (last 100 episodes):\")\n",
    "print(f\"  Average reward: {np.mean(final_100_sarsa):.3f}\")\n",
    "print(f\"  Std deviation: {np.std(final_100_sarsa):.3f}\")\n",
    "print(f\"  Success rate: {success_rate_sarsa*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- SARSA is **on-policy**: learns about the policy being followed during exploration\n",
    "- Uses the Q-value of the **actual next action**, not the best possible action\n",
    "- More conservative than Q-Learning because it considers exploration risk\n",
    "- Useful when the cost of failure during exploration is high\n",
    "- Convergence is guaranteed under GLIE conditions\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Expected SARSA\n",
    "\n",
    "### 5.0 - Expected SARSA Algorithm\n",
    "\n",
    "Expected SARSA combines the best of both worlds:\n",
    "- More stable than SARSA (not dependent on single next action)\n",
    "- Less aggressive than Q-Learning (considers full policy distribution)\n",
    "\n",
    "**Update Rule**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[R_{t+1} + \\gamma \\mathbb{E}[Q(s',a')] - Q(s,a)\\right]$$\n",
    "\n",
    "Where the expectation is taken over the epsilon-greedy policy:\n",
    "$$\\mathbb{E}[Q(s',a')] = (1-\\epsilon) \\max_{a'} Q(s',a') + \\frac{\\epsilon}{|\\mathcal{A}|} \\sum_{a'} Q(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - implement_expected_sarsa\n",
    "\n",
    "Implement the Expected SARSA update rule. Your function should:\n",
    "1. Calculate the current Q-value\n",
    "2. Compute the expected Q-value over the epsilon-greedy policy\n",
    "3. Calculate the TD target and update\n",
    "\n",
    "**Formula for expected value**:\n",
    "- Best action gets: $(1-\\epsilon) + \\frac{\\epsilon}{|\\mathcal{A}|}$\n",
    "- Other actions get: $\\frac{\\epsilon}{|\\mathcal{A}|}$\n",
    "\n",
    "**Hints:**\n",
    "- Find the max Q-value for the next state\n",
    "- Calculate average over all Q-values\n",
    "- Combine using the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: implement_expected_sarsa\n",
    "\n",
    "def implement_expected_sarsa(Q, state, action, reward, next_state, done, \n",
    "                             alpha=0.1, gamma=0.99, epsilon=0.1, n_actions=4):\n",
    "    \"\"\"\n",
    "    Implement the Expected SARSA update rule.\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- Q-value table (dictionary)\n",
    "    state -- current state\n",
    "    action -- action taken\n",
    "    reward -- reward received\n",
    "    next_state -- resulting state\n",
    "    done -- whether episode terminated\n",
    "    alpha -- learning rate\n",
    "    gamma -- discount factor\n",
    "    epsilon -- exploration rate\n",
    "    n_actions -- total number of actions\n",
    "    \n",
    "    Returns:\n",
    "    Q -- updated Q-value table\n",
    "    td_error -- temporal difference error\n",
    "    \"\"\"\n",
    "    # (approx. 9 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    current_q = Q[state][action]\n",
    "    \n",
    "    if done:\n",
    "        target_q = reward\n",
    "    else:\n",
    "        # Expected SARSA: compute expected value over epsilon-greedy policy\n",
    "        q_values = Q[next_state]\n",
    "        max_action = np.argmax(q_values)\n",
    "        \n",
    "        # Expected value under epsilon-greedy policy\n",
    "        expected_q = ((1 - epsilon) * q_values[max_action] + \n",
    "                     (epsilon / n_actions) * np.sum(q_values))\n",
    "        \n",
    "        target_q = reward + gamma * expected_q\n",
    "    \n",
    "    td_error = target_q - current_q\n",
    "    Q[state][action] += alpha * td_error\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Q, td_error\n",
    "\n",
    "print(\"✓ Expected SARSA update rule implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "def test_implement_expected_sarsa():\n",
    "    # Initialize Q-values\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    \n",
    "    Q[0] = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "    Q[1] = np.array([0.2, 0.3, 0.5, 0.1])\n",
    "    \n",
    "    epsilon = 0.1\n",
    "    gamma = 0.99\n",
    "    alpha = 0.1\n",
    "    n_actions = 4\n",
    "    \n",
    "    Q, td_error = implement_expected_sarsa(Q, state=0, action=0, reward=1.0, \n",
    "                                           next_state=1, done=False, \n",
    "                                           alpha=alpha, gamma=gamma, epsilon=epsilon, n_actions=n_actions)\n",
    "    \n",
    "    # Verify calculation\n",
    "    q_next = np.array([0.2, 0.3, 0.5, 0.1])\n",
    "    max_q = np.max(q_next)  # 0.5\n",
    "    expected_q = (1 - epsilon) * max_q + (epsilon / n_actions) * np.sum(q_next)\n",
    "    expected_target = 1.0 + gamma * expected_q\n",
    "    expected_q_value = 0.1 + alpha * (expected_target - 0.1)\n",
    "    \n",
    "    assert np.isclose(Q[0][0], expected_q_value), f\"Expected {expected_q_value}, got {Q[0][0]}\"\n",
    "    \n",
    "    print(\"✓ All Expected SARSA update tests passed!\")\n",
    "\n",
    "test_implement_expected_sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-1'></a>\n",
    "### 5.1 - Training Expected SARSA\n",
    "\n",
    "Now let's train an Expected SARSA agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment\n",
    "env_exp_sarsa = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "# Initialize Expected SARSA agent\n",
    "agent_exp_sarsa = ExpectedSARSAAgent(\n",
    "    n_actions=env_exp_sarsa.action_space.n,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "print('Training Expected SARSA agent...')\n",
    "rewards_exp_sarsa = train_expected_sarsa(env_exp_sarsa, agent_exp_sarsa, n_episodes=500, verbose=True)\n",
    "print(f'\\nTraining complete!')\n",
    "\n",
    "env_exp_sarsa.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Expected SARSA performance\n",
    "final_100_exp_sarsa = rewards_exp_sarsa[-100:]\n",
    "success_rate_exp_sarsa = sum([1 for r in final_100_exp_sarsa if r > 0.5]) / len(final_100_exp_sarsa)\n",
    "\n",
    "print(f\"Expected SARSA Performance (last 100 episodes):\")\n",
    "print(f\"  Average reward: {np.mean(final_100_exp_sarsa):.3f}\")\n",
    "print(f\"  Std deviation: {np.std(final_100_exp_sarsa):.3f}\")\n",
    "print(f\"  Success rate: {success_rate_exp_sarsa*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- Expected SARSA is a **hybrid approach** between Q-Learning and SARSA\n",
    "- Uses the expected value over the epsilon-greedy policy distribution\n",
    "- More stable than SARSA (not dependent on single stochastic action)\n",
    "- Less aggressive than Q-Learning (considers actual exploration policy)\n",
    "- Often provides the best balance between stability and convergence speed\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Algorithm Comparison\n",
    "\n",
    "### 6.0 - Theoretical Comparison\n",
    "\n",
    "| Aspect | Q-Learning | SARSA | Expected SARSA |\n",
    "|--------|-----------|-------|----------------|\n",
    "| **Policy Type** | Off-policy | On-policy | On-policy |\n",
    "| **What it learns** | Optimal policy | Current policy | Current policy |\n",
    "| **Target uses** | max(Q(s',a')) | Q(s',a') | E[Q(s',a')] |\n",
    "| **Stability** | Medium | High | High |\n",
    "| **Convergence** | Fast | Medium | Medium |\n",
    "| **Risk/Caution** | Aggressive | Conservative | Balanced |\n",
    "| **Best for** | Offline learning | Online w/risk | General purpose |\n",
    "| **Variance** | Medium-High | Low | Low |\n",
    "| **Bias** | Low | Medium | Medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - compare_td_methods\n",
    "\n",
    "Implement a comprehensive comparison function that:\n",
    "1. Trains all three algorithms multiple times\n",
    "2. Computes statistics (mean, std, success rate)\n",
    "3. Returns a dictionary with results\n",
    "\n",
    "**Hints**:\n",
    "- Run multiple training runs to get reliable statistics\n",
    "- Calculate final 100 episode performance\n",
    "- Store results in a dictionary with algorithm names as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compare_td_methods\n",
    "\n",
    "def compare_td_methods(n_runs=5, n_episodes=500, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare Q-Learning, SARSA, and Expected SARSA across multiple runs.\n",
    "    \n",
    "    Arguments:\n",
    "    n_runs -- number of independent training runs\n",
    "    n_episodes -- episodes per run\n",
    "    verbose -- whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "    results -- dictionary with comparison results\n",
    "    all_rewards -- dictionary with all reward histories\n",
    "    \"\"\"\n",
    "    # (approx. 40 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    all_rewards = {'Q-Learning': [], 'SARSA': [], 'Expected SARSA': []}\n",
    "    results = {}\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Q-Learning\n",
    "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        agent_ql = QLearningAgent(env.action_space.n, alpha=0.1, gamma=0.99, epsilon=1.0)\n",
    "        ql_rewards = train_q_learning(env, agent_ql, n_episodes=n_episodes, verbose=False)\n",
    "        all_rewards['Q-Learning'].append(ql_rewards)\n",
    "        env.close()\n",
    "        \n",
    "        # SARSA\n",
    "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        agent_sarsa = SARSAAgent(env.action_space.n, alpha=0.1, gamma=0.99, epsilon=1.0)\n",
    "        sarsa_rewards = train_sarsa(env, agent_sarsa, n_episodes=n_episodes, verbose=False)\n",
    "        all_rewards['SARSA'].append(sarsa_rewards)\n",
    "        env.close()\n",
    "        \n",
    "        # Expected SARSA\n",
    "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        agent_exp = ExpectedSARSAAgent(env.action_space.n, alpha=0.1, gamma=0.99, epsilon=1.0)\n",
    "        exp_rewards = train_expected_sarsa(env, agent_exp, n_episodes=n_episodes, verbose=False)\n",
    "        all_rewards['Expected SARSA'].append(exp_rewards)\n",
    "        env.close()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Run {run+1}/{n_runs} completed')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    for algo_name, rewards_list in all_rewards.items():\n",
    "        final_100_rewards = [rewards[-100:] for rewards in rewards_list]\n",
    "        \n",
    "        avg_rewards = [np.mean(r) for r in final_100_rewards]\n",
    "        success_rates = [sum([1 for r in final_100 if r > 0.5]) / len(final_100) \n",
    "                        for final_100 in final_100_rewards]\n",
    "        \n",
    "        results[algo_name] = {\n",
    "            'mean_reward': np.mean(avg_rewards),\n",
    "            'std_reward': np.std(avg_rewards),\n",
    "            'mean_success_rate': np.mean(success_rates),\n",
    "            'std_success_rate': np.std(success_rates)\n",
    "        }\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return results, all_rewards\n",
    "\n",
    "print(\"✓ compare_td_methods function implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive comparison\n",
    "print(f'Running comprehensive comparison across {5} runs...\\n')\n",
    "results, all_rewards = compare_td_methods(n_runs=5, n_episodes=500, verbose=True)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('COMPREHENSIVE TD LEARNING ALGORITHMS COMPARISON')\n",
    "print('='*70)\n",
    "\n",
    "for algo_name, metrics in results.items():\n",
    "    print(f'\\n{algo_name}:')\n",
    "    print(f'  Mean reward (final 100 eps): {metrics[\"mean_reward\"]:.4f} ± {metrics[\"std_reward\"]:.4f}')\n",
    "    print(f'  Success rate: {metrics[\"mean_success_rate\"]*100:.1f}% ± {metrics[\"std_success_rate\"]*100:.1f}%')\n",
    "\nprint('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - Comprehensive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Individual algorithm convergence curves\n",
    "window = 50\n",
    "colors = {'Q-Learning': 'blue', 'SARSA': 'green', 'Expected SARSA': 'orange'}\n",
    "\n",
    "ax = axes[0, 0]\n",
    "for algo_name in ['Q-Learning', 'SARSA', 'Expected SARSA']:\n",
    "    # Average across runs\n",
    "    avg_rewards = np.mean(all_rewards[algo_name], axis=0)\n",
    "    moving_avg = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, 500), moving_avg, linewidth=2.5, \n",
    "            label=algo_name, color=colors[algo_name])\n",
    "\nax.set_xlabel('Episode', fontsize=11)\nax.set_ylabel('Reward (Moving Avg)', fontsize=11)\nax.set_title('Convergence Curves', fontsize=12, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Plot 2: Final performance comparison\nax = axes[0, 1]\nalgo_names = ['Q-Learning', 'SARSA', 'Expected SARSA']\nfinal_rewards = [results[name]['mean_reward'] for name in algo_names]\nfinal_stds = [results[name]['std_reward'] for name in algo_names]\n\ncolors_list = ['blue', 'green', 'orange']\nax.bar(algo_names, final_rewards, yerr=final_stds, capsize=10, \n       color=colors_list, alpha=0.7, edgecolor='black', linewidth=2)\nax.set_ylabel('Mean Reward', fontsize=11)\nax.set_title('Final Performance (Last 100 Episodes)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\n\nfor i, (name, reward, std) in enumerate(zip(algo_names, final_rewards, final_stds)):\n    ax.text(i, reward + std + 0.05, f'{reward:.3f}', ha='center', fontweight='bold')\n\n# Plot 3: Success rate comparison\nax = axes[1, 0]\nsuccess_rates = [results[name]['mean_success_rate']*100 for name in algo_names]\nsuccess_stds = [results[name]['std_success_rate']*100 for name in algo_names]\n\nax.bar(algo_names, success_rates, yerr=success_stds, capsize=10,\n       color=colors_list, alpha=0.7, edgecolor='black', linewidth=2)\nax.set_ylabel('Success Rate (%)', fontsize=11)\nax.set_title('Success Rate (Last 100 Episodes)', fontsize=12, fontweight='bold')\nax.set_ylim([0, 110])\nax.grid(True, alpha=0.3, axis='y')\n\nfor i, (rate, std) in enumerate(zip(success_rates, success_stds)):\n    ax.text(i, rate + std + 2, f'{rate:.1f}%', ha='center', fontweight='bold')\n\n# Plot 4: Statistical comparison table\nax = axes[1, 1]\nax.axis('tight')\nax.axis('off')\n\ntable_data = []\nfor algo_name in algo_names:\n    metrics = results[algo_name]\n    table_data.append([\n        algo_name,\n        f\"{metrics['mean_reward']:.3f}\",\n        f\"{metrics['std_reward']:.3f}\",\n        f\"{metrics['mean_success_rate']*100:.1f}%\"\n    ])\n\ntable = ax.table(cellText=table_data,\n                colLabels=['Algorithm', 'Mean Reward', 'Std Dev', 'Success Rate'],\n                cellLoc='center',\n                loc='center',\n                colWidths=[0.25, 0.25, 0.25, 0.25])\n\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1, 2.5)\n\n# Style header\nfor i in range(4):\n    table[(0, i)].set_facecolor('#4CAF50')\n    table[(0, i)].set_text_props(weight='bold', color='white')\n\n# Alternate row colors\nfor i in range(1, 4):\n    for j in range(4):\n        if i % 2 == 0:\n            table[(i, j)].set_facecolor('#f0f0f0')\n        else:\n            table[(i, j)].set_facecolor('#ffffff')\n\nplt.tight_layout()\nplt.show()\n\nprint('✓ Comprehensive comparison visualization complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- **Q-Learning**: Off-policy, learns optimal policy, can be aggressive in exploration\n",
    "- **SARSA**: On-policy, learns current policy, conservative but stable\n",
    "- **Expected SARSA**: Hybrid approach, balances aggressiveness and stability\n",
    "- In deterministic environments, all three converge to similar performance\n",
    "- Differences become more pronounced in stochastic environments\n",
    "- Choice depends on problem characteristics: offline learning vs online with risk\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Cliff Walking Problem\n",
    "\n",
    "### 7.0 - Problem Description\n",
    "\n",
    "The **Cliff Walking** environment demonstrates the key differences between on-policy and off-policy algorithms:\n",
    "\n",
    "- Grid: 4 rows × 12 columns\n",
    "- Agent starts at (3, 0) - bottom left\n",
    "- Goal at (3, 11) - bottom right\n",
    "- Cliff along (3, 1-10) with reward -100\n",
    "- Each step reward: -1\n",
    "\n",
    "**The Dilemma**:\n",
    "- **Optimal path** (along the cliff edge): Fast but risky (-13 rewards)\n",
    "- **Safe path** (away from cliff): Slower but safe (-25 rewards)\n",
    "\n",
    "**Expected behavior**:\n",
    "- **Q-Learning**: Learns the risky optimal path\n",
    "- **SARSA**: Learns the safe path (avoids cliff during learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cliff Walking environment\n",
    "class CliffWalkingEnv:\n",
    "    \"\"\"Custom Cliff Walking environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grid_shape = (4, 12)\n",
    "        self.start = (3, 0)\n",
    "        self.goal = (3, 11)\n",
    "        self.current_pos = self.start\n",
    "        self.action_names = ['up', 'right', 'down', 'left']\n",
    "        self.state = self._pos_to_state(self.start)\n",
    "    \n",
    "    def _pos_to_state(self, pos):\n",
    "        return pos[0] * self.grid_shape[1] + pos[1]\n",
    "    \n",
    "    def _state_to_pos(self, state):\n",
    "        return (state // self.grid_shape[1], state % self.grid_shape[1])\n",
    "    \n",
    "    def _is_cliff(self, pos):\n",
    "        return pos[0] == 3 and 1 <= pos[1] <= 10\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_pos = self.start\n",
    "        self.state = self._pos_to_state(self.start)\n",
    "        return self.state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = self.current_pos\n",
    "        \n",
    "        if action == 0:  # up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # right\n",
    "            col = min(self.grid_shape[1] - 1, col + 1)\n",
    "        elif action == 2:  # down\n",
    "            row = min(self.grid_shape[0] - 1, row + 1)\n",
    "        elif action == 3:  # left\n",
    "            col = max(0, col - 1)\n",
    "        \n",
    "        new_pos = (row, col)\n",
    "        \n",
    "        if self._is_cliff(new_pos):\n",
    "            reward = -100\n",
    "            new_pos = self.start\n",
    "            done = True\n",
    "        elif new_pos == self.goal:\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        self.current_pos = new_pos\n",
    "        self.state = self._pos_to_state(new_pos)\n",
    "        \n",
    "        return self.state, reward, done, False, {}\n",
    "\nprint('✓ Cliff Walking environment created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Cliff Walking\n",
    "print('Training algorithms on Cliff Walking...\\n')\n",
    "\n",
    "cliff_results = {}\n",
    "env_cliff = CliffWalkingEnv()\n",
    "\n",
    "# Q-Learning on Cliff Walking\n",
    "agent_ql_cliff = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\nql_cliff_rewards = []\nfor episode in range(500):\n",
    "    state, _ = env_cliff.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(100):\n",
    "        action = agent_ql_cliff.get_action(state)\n",
    "        next_state, reward, done, _, _ = env_cliff.step(action)\n",
    "        agent_ql_cliff.update(state, action, reward, next_state, done)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    agent_ql_cliff.decay_epsilon()\n",
    "    ql_cliff_rewards.append(episode_reward)\n\nprint(f'Q-Learning: {np.mean(ql_cliff_rewards[-100:]):.2f} avg reward (last 100 eps)')\n\n# SARSA on Cliff Walking\nagent_sarsa_cliff = SARSAAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\nsarsa_cliff_rewards = []\nfor episode in range(500):\n    state, _ = env_cliff.reset()\n    action = agent_sarsa_cliff.get_action(state)\n    episode_reward = 0\n    \n",
    "    for step in range(100):\n",
    "        next_state, reward, done, _, _ = env_cliff.step(action)\n",
    "        next_action = agent_sarsa_cliff.get_action(next_state)\n",
    "        agent_sarsa_cliff.update(state, action, reward, next_state, next_action, done)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    agent_sarsa_cliff.decay_epsilon()\n",
    "    sarsa_cliff_rewards.append(episode_reward)\n\nprint(f'SARSA: {np.mean(sarsa_cliff_rewards[-100:]):.2f} avg reward (last 100 eps)')\nprint('\\n✓ Training on Cliff Walking complete')\n\n# Store results\ncliff_results['Q-Learning'] = np.mean(ql_cliff_rewards[-100:])\ncliff_results['SARSA'] = np.mean(sarsa_cliff_rewards[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cliff Walking results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nwindow = 50\n\n# Plot 1: Learning curves\nax = axes[0]\nmoving_avg_ql = np.convolve(ql_cliff_rewards, np.ones(window)/window, mode='valid')\nmoving_avg_sarsa = np.convolve(sarsa_cliff_rewards, np.ones(window)/window, mode='valid')\n\nax.plot(range(window-1, 500), moving_avg_ql, 'b-', linewidth=2.5, label='Q-Learning')\nax.plot(range(window-1, 500), moving_avg_sarsa, 'g-', linewidth=2.5, label='SARSA')\nax.set_xlabel('Episode', fontsize=11)\nax.set_ylabel('Reward', fontsize=11)\nax.set_title('Cliff Walking: Learning Curves', fontsize=12, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.axhline(y=-13, color='r', linestyle='--', alpha=0.5, label='Optimal path')\nax.axhline(y=-25, color='orange', linestyle='--', alpha=0.5, label='Safe path')\n\n# Plot 2: Final performance comparison\nax = axes[1]\nalgorithms = ['Q-Learning', 'SARSA']\nrewards = [cliff_results['Q-Learning'], cliff_results['SARSA']]\ncolors = ['blue', 'green']\n\nbars = ax.bar(algorithms, rewards, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\nax.set_ylabel('Average Reward', fontsize=11)\nax.set_title('Cliff Walking: Final Performance', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(y=-13, color='r', linestyle='--', alpha=0.5, linewidth=2, label='Optimal path')\nax.axhline(y=-25, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='Safe path')\nax.legend()\n\nfor bar, reward in zip(bars, rewards):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height-3,\n            f'{reward:.1f}', ha='center', va='top', fontweight='bold', color='white', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\nprint('✓ Cliff Walking analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Key Observations\n",
    "\n",
    "**Q-Learning Results**:\n",
    "- Learns the optimal policy along the cliff edge\n",
    "- Final reward: ~-13 (approaching the optimal -13)\n",
    "- Takes the risky path because it learns the best long-term policy\n",
    "\n",
    "**SARSA Results**:\n",
    "- Learns a safer path away from the cliff\n",
    "- Final reward: ~-25 (worse than optimal but safer)\n",
    "- Avoids the cliff during learning because it accounts for exploration risk\n",
    "\n",
    "**Interpretation**:\n",
    "- **Off-policy (Q-Learning)**: Can afford to explore risky paths and learn the best policy\n",
    "- **On-policy (SARSA)**: Must be cautious during exploration, so learns a safer policy\n",
    "- This demonstrates why **SARSA is preferred for real-world applications with exploration costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Core Concepts\n",
    "\n",
    "**TD Learning combines the best of both worlds:**\n",
    "1. **Like DP**: Uses bootstrapping (value estimates of future states)\n",
    "2. **Like MC**: Learns from real experience without a model\n",
    "3. **Better than both**: Online learning with lower variance than MC\n",
    "\n",
    "### 8.2 - Three Main Algorithms\n",
    "\n",
    "1. **Q-Learning (Off-Policy)**\n",
    "   - Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "   - Best for: Offline learning, when exploration cost is low\n",
    "   - Risk: Can learn dangerous policies during exploration\n",
    "\n",
    "2. **SARSA (On-Policy)**\n",
    "   - Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$\n",
    "   - Best for: Online learning, when exploration is risky\n",
    "   - Advantage: Safe, stable learning\n",
    "\n",
    "3. **Expected SARSA (On-Policy)**\n",
    "   - Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\mathbb{E}[Q(s',a')] - Q(s,a)]$\n",
    "   - Best for: General-purpose learning with good stability\n",
    "   - Advantage: Balances aggressiveness and safety\n",
    "\n",
    "### 8.3 - When to Use Each Algorithm\n",
    "\n",
    "| Scenario | Best Choice | Why |\n",
    "|----------|------------|-----|\n",
    "| Offline learning from logs | Q-Learning | Can explore aggressively |\n",
    "| Real robot, high cost of failure | SARSA | Conservative and safe |\n",
    "| General purpose learning | Expected SARSA | Good balance |\n",
    "| Need guaranteed convergence | Any (GLIE) | All work with GLIE |\n",
    "| Stochastic environment | Expected SARSA | More stable |\n",
    "| Large action space | Consider function approximation | TD learning scales with approximators |\n",
    "\n",
    "### 8.4 - Important Parameters\n",
    "\n",
    "- **Learning rate (α)**: Controls update step size. Common: 0.01 to 0.1\n",
    "- **Discount factor (γ)**: Values future rewards. 0.99 for long-horizon tasks\n",
    "- **Exploration rate (ε)**: Probability of random action. Start high (~1.0), decay to low (~0.01)\n",
    "- **Epsilon decay**: How fast ε decreases. Common: 0.995 per episode\n",
    "\n",
    "### 8.5 - Convergence Guarantees\n",
    "\n",
    "All three algorithms converge to optimal $Q^*$ under **GLIE** conditions:\n",
    "1. **Greedy in the Limit**: Eventually exploit known best action\n",
    "2. **Infinite Exploration**: Visit every state-action pair infinitely often\n",
    "\n",
    "Practical implementation: Use epsilon-greedy with decaying epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 - Common Challenges and Solutions\n",
    "\n",
    "| Challenge | Solution | Algorithm |\n",
    "|-----------|----------|----------|\n",
    "| Q-values grow unbounded | Use learning rate decay or target networks | DQN |\n",
    "| High variance in updates | Use experience replay or mini-batches | DQN, Dueling |\n",
    "| Overestimation of Q-values | Use Double Q-Learning or Expected SARSA | Double Q-Learning |\n",
    "| Sample inefficiency | Prioritized experience replay | DQN variants |\n",
    "| Discrete state space limitation | Function approximation (neural networks) | Deep Q-Learning |\n",
    "| Continuous action spaces | Policy gradient methods | A3C, PPO, TRPO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 - Next Steps in Reinforcement Learning\n",
    "\n",
    "After mastering TD Learning:\n",
    "\n",
    "1. **Function Approximation**: Use neural networks for large state spaces\n",
    "   - Deep Q-Networks (DQN)\n",
    "   - Double DQN\n",
    "   - Dueling DQN\n",
    "\n",
    "2. **Policy Gradient Methods**: Learn policy directly\n",
    "   - REINFORCE\n",
    "   - Actor-Critic methods\n",
    "   - A3C, PPO, TRPO\n",
    "\n",
    "3. **Model-Based Methods**: Learn environment model\n",
    "   - Dyna-Q\n",
    "   - World Models\n",
    "   - Planning methods\n",
    "\n",
    "4. **Multi-Agent Learning**: Competitive and cooperative settings\n",
    "\n",
    "5. **Advanced Topics**:\n",
    "   - Inverse reinforcement learning\n",
    "   - Meta-reinforcement learning\n",
    "   - Transfer learning in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "- **TD Learning** combines bootstrapping with experience-based learning\n",
    "- **Q-Learning** (off-policy) learns optimal policy aggressively\n",
    "- **SARSA** (on-policy) learns cautiously about current policy\n",
    "- **Expected SARSA** balances both approaches\n",
    "- Choice depends on problem characteristics and constraints\n",
    "- All converge under GLIE conditions with proper exploration\n",
    "- These fundamentals are building blocks for modern deep RL\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\nprint('TEMPORAL DIFFERENCE LEARNING - COMPLETE')\nprint('='*70)\nprint()\nprint('You have successfully learned:')\nprint('  ✓ TD Learning fundamentals and theory')\nprint('  ✓ Q-Learning algorithm (off-policy)')\nprint('  ✓ SARSA algorithm (on-policy)')\nprint('  ✓ Expected SARSA algorithm')\nprint('  ✓ Algorithm comparison and analysis')\nprint('  ✓ Practical applications and trade-offs')\nprint()\nprint('Next steps:')\nprint('  → Explore function approximation with neural networks')\nprint('  → Implement Deep Q-Networks (DQN)')\nprint('  → Study policy gradient methods')\nprint('  → Apply to real-world problems')\nprint()\nprint('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
