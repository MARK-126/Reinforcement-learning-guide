# CÃ¡lculo BÃ¡sico para Reinforcement Learning

## ğŸ¯ Por QuÃ© Necesitas CÃ¡lculo en Deep RL

En Deep Reinforcement Learning con redes neuronales, necesitas:
- **Derivadas**: Para saber cÃ³mo cambiar parÃ¡metros
- **Gradientes**: DirecciÃ³n de mÃ¡ximo cambio
- **Chain Rule**: Backpropagation en redes neuronales
- **Gradient Descent**: OptimizaciÃ³n de polÃ­ticas y value functions

Esta guÃ­a cubre **solo lo esencial** para Deep RL, desde cero.

---

## 1. Derivadas desde Cero

### 1.1 Â¿QuÃ© es una Derivada?

La **derivada** mide **quÃ© tan rÃ¡pido cambia** una funciÃ³n.

**AnalogÃ­a**: Si conduces un carro:
- **PosiciÃ³n** x(t): dÃ³nde estÃ¡s
- **Velocidad** v(t) = dx/dt: quÃ© tan rÃ¡pido cambias de posiciÃ³n
- **AceleraciÃ³n** a(t) = dv/dt: quÃ© tan rÃ¡pido cambia tu velocidad

### 1.2 DefiniciÃ³n Intuitiva

La derivada de f(x) en punto x es:
```
f'(x) = lim[hâ†’0] [f(x+h) - f(x)] / h
```

**InterpretaciÃ³n geomÃ©trica**: Pendiente de la recta tangente.

**Ejemplo visual**:
```
   f(x) = xÂ²

    |
  9 |       â€¢  (3, 9)
    |      /
  4 |  â€¢  /     Pendiente en x=2: f'(2) = 4
    | /  â€¢
  1 |â€¢  (2, 4)
    |________
    0  1  2  3

En x=2, la funciÃ³n estÃ¡ creciendo con pendiente 4
```

### 1.3 Notaciones de Derivada

Todas significan lo mismo:
```
f'(x)     ("f prima de x")
df/dx     ("derivada de f respecto a x")
âˆ‚f/âˆ‚x     ("derivada parcial" - veremos despuÃ©s)
```

### 1.4 Reglas BÃ¡sicas de DerivaciÃ³n

#### Constantes
```
f(x) = c
f'(x) = 0

Ejemplo: f(x) = 5  â†’  f'(x) = 0
```

#### Potencias (Regla del Poder)
```
f(x) = xâ¿
f'(x) = nÂ·xâ¿â»Â¹

Ejemplos:
f(x) = xÂ²   â†’  f'(x) = 2x
f(x) = xÂ³   â†’  f'(x) = 3xÂ²
f(x) = x    â†’  f'(x) = 1
f(x) = âˆšx   â†’  f'(x) = 1/(2âˆšx)
```

**VerificaciÃ³n con Python**:
```python
import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return x**2

def f_prima(x):
    return 2*x

x = np.linspace(-3, 3, 100)
plt.plot(x, f(x), label='f(x) = xÂ²')
plt.plot(x, f_prima(x), label="f'(x) = 2x")
plt.legend()
plt.grid()
plt.show()
```

#### Suma y Resta
```
f(x) = g(x) + h(x)
f'(x) = g'(x) + h'(x)

Ejemplo:
f(x) = 3xÂ² + 5x
f'(x) = 6x + 5
```

#### MultiplicaciÃ³n por Constante
```
f(x) = cÂ·g(x)
f'(x) = cÂ·g'(x)

Ejemplo:
f(x) = 5xÂ³
f'(x) = 5Â·3xÂ² = 15xÂ²
```

#### Regla del Producto
```
f(x) = g(x)Â·h(x)
f'(x) = g'(x)Â·h(x) + g(x)Â·h'(x)

Ejemplo:
f(x) = xÂ²Â·sin(x)
f'(x) = 2xÂ·sin(x) + xÂ²Â·cos(x)
```

#### Regla de la Cadena (Chain Rule) â­
**La mÃ¡s importante para Deep Learning**

```
f(x) = g(h(x))
f'(x) = g'(h(x))Â·h'(x)

"Derivada de la funciÃ³n exterior evaluada en la interior,
multiplicada por la derivada de la interior"
```

**Ejemplo simple**:
```
f(x) = (xÂ² + 1)Â³

Sea u = xÂ² + 1, entonces f = uÂ³

f'(x) = d/du[uÂ³]Â·d/dx[xÂ² + 1]
      = 3uÂ²Â·2x
      = 3(xÂ² + 1)Â²Â·2x
      = 6x(xÂ² + 1)Â²
```

**En Python (aproximaciÃ³n numÃ©rica)**:
```python
def derivada_numerica(f, x, h=1e-5):
    """Aproxima f'(x) usando diferencias finitas"""
    return (f(x + h) - f(x)) / h

f = lambda x: (x**2 + 1)**3
x = 2.0

# Derivada numÃ©rica
aprox = derivada_numerica(f, x)

# Derivada analÃ­tica: 6x(xÂ² + 1)Â²
exacta = 6*x*(x**2 + 1)**2

print(f"Aproximada: {aprox:.6f}")
print(f"Exacta:     {exacta:.6f}")
```

---

## 2. Funciones Importantes en RL

### 2.1 FunciÃ³n Exponencial

```
f(x) = eË£
f'(x) = eË£

Â¡La derivada es ella misma!
```

**En RL**: Softmax, Boltzmann exploration

**Python**:
```python
import numpy as np

x = np.array([0, 1, 2, -1])
y = np.exp(x)  # array([1.   , 2.718, 7.389, 0.368])
```

### 2.2 FunciÃ³n LogarÃ­tmica

```
f(x) = ln(x)   (logaritmo natural)
f'(x) = 1/x

f(x) = log(x)  (puede ser cualquier base)
```

**En RL**: Log-probabilities en policy gradient, entropÃ­a

```python
x = np.array([1, 2, np.e, 10])
y = np.log(x)  # array([0.   , 0.693, 1.   , 2.303])
```

### 2.3 Funciones de ActivaciÃ³n

#### Sigmoid (Ïƒ)
```
Ïƒ(x) = 1 / (1 + eâ»Ë£)

Rango: (0, 1)
Ïƒ'(x) = Ïƒ(x)Â·(1 - Ïƒ(x))
```

**Propiedad Ãºtil**: La derivada se expresa en tÃ©rminos de la funciÃ³n misma.

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

x = np.linspace(-6, 6, 100)
plt.plot(x, sigmoid(x), label='Ïƒ(x)')
plt.plot(x, sigmoid_derivative(x), label="Ïƒ'(x)")
plt.legend()
plt.grid()
plt.show()
```

**En RL**: Output de probabilidades, gates en LSTMs

#### Tanh
```
tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)

Rango: (-1, 1)
tanh'(x) = 1 - tanhÂ²(x)
```

```python
def tanh_derivative(x):
    t = np.tanh(x)
    return 1 - t**2
```

**En RL**: ActivaciÃ³n en actor networks (acciones continuas)

#### ReLU (Rectified Linear Unit)
```
ReLU(x) = max(0, x) = { x  si x > 0
                      { 0  si x â‰¤ 0

ReLU'(x) = { 1  si x > 0
           { 0  si x â‰¤ 0
```

**MÃ¡s utilizada en Deep RL moderno**

```python
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

x = np.linspace(-3, 3, 100)
plt.plot(x, relu(x), label='ReLU(x)')
plt.plot(x, relu_derivative(x), label="ReLU'(x)")
plt.legend()
plt.grid()
plt.show()
```

**Ventajas**: No vanishing gradient, computacionalmente eficiente

#### Softmax
```
softmax(xáµ¢) = eË£â± / Î£â±¼ eË£Ê²

Convierte vector en distribuciÃ³n de probabilidad
```

```python
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # Estabilidad numÃ©rica
    return exp_x / np.sum(exp_x)

logits = np.array([2.0, 1.0, 0.1])
probs = softmax(logits)
print(probs)  # array([0.659, 0.242, 0.099])
print(np.sum(probs))  # 1.0
```

**En RL**: Convertir Q-values en polÃ­tica estocÃ¡stica

---

## 3. Derivadas Parciales

### 3.1 Â¿QuÃ© son?

Cuando una funciÃ³n depende de **mÃºltiples variables**, la **derivada parcial** mide cÃ³mo cambia respecto a UNA variable, manteniendo las demÃ¡s constantes.

**NotaciÃ³n**:
```
f(x, y)
âˆ‚f/âˆ‚x   - derivada parcial respecto a x (y es constante)
âˆ‚f/âˆ‚y   - derivada parcial respecto a y (x es constante)
```

### 3.2 Ejemplo: FunciÃ³n de PÃ©rdida

```
f(wâ‚, wâ‚‚) = wâ‚Â² + wâ‚‚Â² + 3wâ‚wâ‚‚

âˆ‚f/âˆ‚wâ‚ = 2wâ‚ + 3wâ‚‚     (tratar wâ‚‚ como constante)
âˆ‚f/âˆ‚wâ‚‚ = 2wâ‚‚ + 3wâ‚     (tratar wâ‚ como constante)
```

**InterpretaciÃ³n**: Si estÃ¡s en punto (wâ‚, wâ‚‚), las derivadas parciales te dicen cÃ³mo cambia f si te mueves solo en direcciÃ³n wâ‚ o solo en direcciÃ³n wâ‚‚.

**Python**:
```python
def f(w1, w2):
    return w1**2 + w2**2 + 3*w1*w2

def df_dw1(w1, w2):
    return 2*w1 + 3*w2

def df_dw2(w1, w2):
    return 2*w2 + 3*w1

# Evaluar en punto (1, 2)
w1, w2 = 1, 2
print(f"f({w1}, {w2}) = {f(w1, w2)}")           # 11
print(f"âˆ‚f/âˆ‚wâ‚ = {df_dw1(w1, w2)}")             # 8
print(f"âˆ‚f/âˆ‚wâ‚‚ = {df_dw2(w1, w2)}")             # 7
```

---

## 4. Gradientes

### 4.1 DefiniciÃ³n

El **gradiente** âˆ‡f es un **vector** de todas las derivadas parciales:

```
âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]
```

**Ejemplo**:
```
f(wâ‚, wâ‚‚) = wâ‚Â² + wâ‚‚Â² + 3wâ‚wâ‚‚

âˆ‡f = [âˆ‚f/âˆ‚wâ‚, âˆ‚f/âˆ‚wâ‚‚] = [2wâ‚ + 3wâ‚‚, 2wâ‚‚ + 3wâ‚]
```

### 4.2 InterpretaciÃ³n GeomÃ©trica

El gradiente **apunta en la direcciÃ³n de mayor crecimiento** de la funciÃ³n.

```
    â†— âˆ‡f     (direcciÃ³n de subida mÃ¡s rÃ¡pida)
   /
  â€¢ (punto actual)
   \
    â†˜ -âˆ‡f    (direcciÃ³n de bajada mÃ¡s rÃ¡pida)
```

**En RL**: Para **maximizar** recompensa, seguimos +âˆ‡J (gradient ascent)
**En supervised learning**: Para **minimizar** loss, seguimos -âˆ‡L (gradient descent)

### 4.3 Ejemplo Visual

```python
import numpy as np
import matplotlib.pyplot as plt

# FunciÃ³n f(x,y) = xÂ² + yÂ²
def f(x, y):
    return x**2 + y**2

# Gradiente: âˆ‡f = [2x, 2y]
def gradient(x, y):
    return np.array([2*x, 2*y])

# Crear grid
x = np.linspace(-2, 2, 20)
y = np.linspace(-2, 2, 20)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Calcular gradientes
U = 2*X  # âˆ‚f/âˆ‚x
V = 2*Y  # âˆ‚f/âˆ‚y

plt.figure(figsize=(10, 5))

# Contour plot
plt.subplot(1, 2, 1)
plt.contour(X, Y, Z, levels=15)
plt.colorbar()
plt.title('f(x,y) = xÂ² + yÂ²')

# Quiver plot (vectores gradiente)
plt.subplot(1, 2, 2)
plt.quiver(X, Y, U, V)
plt.title('Gradiente âˆ‡f')
plt.axis('equal')
plt.show()
```

---

## 5. Chain Rule Multivariable

### 5.1 Para Redes Neuronales

En una red neuronal tÃ­pica:
```
x â†’ z = Wx + b â†’ a = Ïƒ(z) â†’ L (loss)
```

Queremos calcular **âˆ‚L/âˆ‚W** (cÃ³mo cambiar pesos para reducir pÃ©rdida).

**Chain rule**:
```
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚W
```

### 5.2 Ejemplo Concreto

**Forward pass**:
```
Capa 1: zâ‚ = wâ‚Â·x
Capa 2: aâ‚ = ReLU(zâ‚)
Capa 3: zâ‚‚ = wâ‚‚Â·aâ‚
Capa 4: L = (zâ‚‚ - y)Â²  (loss cuadrÃ¡tico)
```

**Backward pass** (calcular âˆ‚L/âˆ‚wâ‚):
```
âˆ‚L/âˆ‚zâ‚‚ = 2(zâ‚‚ - y)
âˆ‚zâ‚‚/âˆ‚aâ‚ = wâ‚‚
âˆ‚aâ‚/âˆ‚zâ‚ = ReLU'(zâ‚) = { 1 si zâ‚ > 0, 0 otherwise }
âˆ‚zâ‚/âˆ‚wâ‚ = x

Chain rule:
âˆ‚L/âˆ‚wâ‚ = âˆ‚L/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚aâ‚ Â· âˆ‚aâ‚/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚wâ‚
       = 2(zâ‚‚-y) Â· wâ‚‚ Â· ReLU'(zâ‚) Â· x
```

**Python completo**:
```python
# Forward pass
x = 2.0
w1 = 0.5
w2 = 0.3
y = 1.0  # Target

z1 = w1 * x              # 1.0
a1 = max(0, z1)          # 1.0 (ReLU)
z2 = w2 * a1             # 0.3
L = (z2 - y)**2          # 0.49

print(f"Loss: {L}")

# Backward pass
dL_dz2 = 2 * (z2 - y)                    # -1.4
dz2_da1 = w2                              # 0.3
da1_dz1 = 1 if z1 > 0 else 0            # 1
dz1_dw1 = x                               # 2.0

dL_dw1 = dL_dz2 * dz2_da1 * da1_dz1 * dz1_dw1
print(f"âˆ‚L/âˆ‚wâ‚ = {dL_dw1}")  # -0.84

# Actualizar peso (gradient descent)
learning_rate = 0.1
w1_new = w1 - learning_rate * dL_dw1
print(f"wâ‚: {w1} â†’ {w1_new}")  # 0.5 â†’ 0.584
```

---

## 6. Gradient Descent

### 6.1 Idea BÃ¡sica

Para **minimizar** una funciÃ³n f(w), iterativamente:
```
w_nuevo = w_viejo - Î±Â·âˆ‡f(w_viejo)
```

Donde:
- **Î±** (alpha): learning rate (tasa de aprendizaje)
- **âˆ‡f**: gradiente (direcciÃ³n de subida)
- **-âˆ‡f**: direcciÃ³n de bajada

### 6.2 Ejemplo: Minimizar f(w) = wÂ²

```
f(w) = wÂ²
f'(w) = 2w

Algoritmo:
1. Inicializar w = 5
2. Repetir:
   w = w - Î±Â·2w
```

**Python**:
```python
import numpy as np
import matplotlib.pyplot as plt

def f(w):
    return w**2

def df(w):
    return 2*w

# InicializaciÃ³n
w = 5.0
alpha = 0.1
history = [w]

# Gradient descent
for i in range(20):
    w = w - alpha * df(w)
    history.append(w)
    print(f"IteraciÃ³n {i+1}: w = {w:.4f}, f(w) = {f(w):.4f}")

# Visualizar
plt.plot(history, [f(w) for w in history], 'o-')
plt.xlabel('IteraciÃ³n')
plt.ylabel('f(w)')
plt.title('Convergencia de Gradient Descent')
plt.grid()
plt.show()
```

### 6.3 Gradient Descent Multidimensional

Para funciÃ³n de mÃºltiples variables:
```
w = [wâ‚, wâ‚‚, ..., wâ‚™]
âˆ‡f = [âˆ‚f/âˆ‚wâ‚, âˆ‚f/âˆ‚wâ‚‚, ..., âˆ‚f/âˆ‚wâ‚™]

ActualizaciÃ³n:
wáµ¢ = wáµ¢ - Î±Â·âˆ‚f/âˆ‚wáµ¢   para cada i
```

**Ejemplo**: Minimizar f(wâ‚, wâ‚‚) = wâ‚Â² + wâ‚‚Â²

```python
def f(w1, w2):
    return w1**2 + w2**2

def gradient(w1, w2):
    return np.array([2*w1, 2*w2])

# Inicializar
w = np.array([3.0, 4.0])
alpha = 0.1

print("Inicio:", w, "f =", f(*w))

for i in range(10):
    grad = gradient(*w)
    w = w - alpha * grad
    print(f"Iter {i+1}:", w, "f =", f(*w))

# Converge a [0, 0] (mÃ­nimo global)
```

---

## 7. Aplicaciones en Deep RL

### 7.1 Policy Gradient

Queremos maximizar retorno esperado:
```
J(Î¸) = E_Ï€[Î£ Î³áµ—râ‚œ]
```

**Policy Gradient Theorem**:
```
âˆ‡_Î¸ J(Î¸) = E_Ï€[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· Q^Ï€(s,a)]
```

**InterpretaciÃ³n**: Aumenta probabilidad de acciones con Q alto.

**Update**:
```
Î¸_nuevo = Î¸_viejo + Î±Â·âˆ‡_Î¸ J(Î¸)   (ascent, no descent!)
```

### 7.2 Q-Learning con Redes Neuronales (DQN)

**Loss**:
```
L(Î¸) = E[(r + Î³ max_a' Q(s',a';Î¸â») - Q(s,a;Î¸))Â²]
```

**Gradiente**:
```
âˆ‡_Î¸ L = -2Â·(target - Q(s,a;Î¸))Â·âˆ‡_Î¸ Q(s,a;Î¸)
```

Donde target = r + Î³ max_a' Q(s',a';Î¸â»)

### 7.3 Actor-Critic

**Actor** (polÃ­tica): Ï€_Î¸(a|s)
**Critic** (value): V_Ï†(s)

**Updates**:
```
Actor:  Î¸ = Î¸ + Î±Â·âˆ‡_Î¸ log Ï€_Î¸(a|s)Â·A(s,a)
Critic: Ï† = Ï† - Î±Â·âˆ‡_Ï†(V_Ï†(s) - target)Â²
```

---

## 8. Herramientas: Autograd

En prÃ¡ctica, **NO calculas derivadas manualmente**. Frameworks como PyTorch hacen autodiferenciaciÃ³n:

```python
import torch

# Definir parÃ¡metros (requieren gradiente)
w = torch.tensor([2.0], requires_grad=True)
x = torch.tensor([3.0])

# Forward pass
y = w * x       # y = 2*3 = 6
L = (y - 5)**2  # L = (6-5)Â² = 1

# Backward pass (automÃ¡tico!)
L.backward()

# Gradiente calculado automÃ¡ticamente
print(f"âˆ‚L/âˆ‚w = {w.grad}")  # tensor([6.])

# VerificaciÃ³n manual: âˆ‚L/âˆ‚w = 2(wx-5)Â·x = 2(6-5)Â·3 = 6 âœ“
```

**Ejemplo mÃ¡s complejo**:
```python
# Red neuronal simple
x = torch.tensor([[1.0, 2.0, 3.0]])
W1 = torch.randn(3, 4, requires_grad=True)
b1 = torch.zeros(4, requires_grad=True)
W2 = torch.randn(4, 1, requires_grad=True)
b2 = torch.zeros(1, requires_grad=True)

# Forward
h = torch.relu(x @ W1 + b1)
y_pred = h @ W2 + b2
y_true = torch.tensor([[1.0]])
loss = (y_pred - y_true)**2

# Backward
loss.backward()

# Todos los gradientes calculados!
print("âˆ‚L/âˆ‚Wâ‚:", W1.grad.shape)  # torch.Size([3, 4])
print("âˆ‚L/âˆ‚Wâ‚‚:", W2.grad.shape)  # torch.Size([4, 1])
```

---

## 9. Ejercicios PrÃ¡cticos

### Ejercicio 1: Derivadas BÃ¡sicas
Calcula f'(x) para:
1. f(x) = 3xÂ² + 2x - 5
2. f(x) = xÂ³ - 4x

<details>
<summary>Ver soluciÃ³n</summary>

1. f'(x) = 6x + 2
2. f'(x) = 3xÂ² - 4
</details>

### Ejercicio 2: Chain Rule
f(x) = (2x + 1)â´. Calcula f'(x).

<details>
<summary>Ver soluciÃ³n</summary>

Sea u = 2x + 1, entonces f = uâ´

f'(x) = 4uÂ³ Â· 2 = 8(2x + 1)Â³
</details>

### Ejercicio 3: Derivadas Parciales
f(x,y) = xÂ²y + xyÂ². Calcula âˆ‚f/âˆ‚x y âˆ‚f/âˆ‚y.

<details>
<summary>Ver soluciÃ³n</summary>

âˆ‚f/âˆ‚x = 2xy + yÂ² (tratar y como constante)
âˆ‚f/âˆ‚y = xÂ² + 2xy (tratar x como constante)
</details>

### Ejercicio 4: Gradient Descent
Minimiza f(w) = (w-3)Â² usando gradient descent con wâ‚€=0, Î±=0.1, 5 iteraciones.

<details>
<summary>Ver soluciÃ³n</summary>

```python
f'(w) = 2(w-3)

IteraciÃ³n 1: w = 0 - 0.1Â·2(0-3) = 0.6
IteraciÃ³n 2: w = 0.6 - 0.1Â·2(0.6-3) = 1.08
IteraciÃ³n 3: w = 1.08 - 0.1Â·2(1.08-3) = 1.464
IteraciÃ³n 4: w = 1.464 - 0.1Â·2(1.464-3) = 1.771
IteraciÃ³n 5: w = 1.771 - 0.1Â·2(1.771-3) = 2.017
```

Converge hacia w=3 (mÃ­nimo).
</details>

---

## 10. Conceptos Avanzados (Opcional)

### 10.1 Hessian (Segunda Derivada)

La matriz Hessiana contiene segundas derivadas:
```
H = [ âˆ‚Â²f/âˆ‚xâ‚Â²   âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚ ]
    [ âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚ âˆ‚Â²f/âˆ‚xâ‚‚Â²   ]
```

**Uso**: OptimizaciÃ³n de segundo orden (Newton's method), anÃ¡lisis de convergencia.

### 10.2 Jacobian

Para funciÃ³n vectorial f: â„â¿ â†’ â„áµ, el Jacobiano es matriz de derivadas:
```
J = [ âˆ‚fâ‚/âˆ‚xâ‚  âˆ‚fâ‚/âˆ‚xâ‚‚  ...  âˆ‚fâ‚/âˆ‚xâ‚™ ]
    [ âˆ‚fâ‚‚/âˆ‚xâ‚  âˆ‚fâ‚‚/âˆ‚xâ‚‚  ...  âˆ‚fâ‚‚/âˆ‚xâ‚™ ]
    [   ...      ...    ...    ...   ]
    [ âˆ‚fâ‚˜/âˆ‚xâ‚  âˆ‚fâ‚˜/âˆ‚xâ‚‚  ...  âˆ‚fâ‚˜/âˆ‚xâ‚™ ]
```

**En Deep RL**: Backpropagation a travÃ©s de capas mÃºltiples.

---

## 11. Cheat Sheet: Derivadas Esenciales

| FunciÃ³n | Derivada | Notas |
|---------|----------|-------|
| c | 0 | Constante |
| x | 1 | |
| xâ¿ | nxâ¿â»Â¹ | Regla del poder |
| eË£ | eË£ | Exponencial |
| ln(x) | 1/x | Logaritmo natural |
| sin(x) | cos(x) | |
| cos(x) | -sin(x) | |
| Ïƒ(x) | Ïƒ(x)(1-Ïƒ(x)) | Sigmoid |
| tanh(x) | 1-tanhÂ²(x) | |
| ReLU(x) | {1 si x>0, 0 si xâ‰¤0} | |

**Reglas**:
- Suma: (f+g)' = f' + g'
- Producto: (fg)' = f'g + fg'
- Cadena: (fâˆ˜g)' = f'(g)Â·g'

---

## 12. Recursos Adicionales

### Videos (EspaÃ±ol)
- [Khan Academy - CÃ¡lculo](https://es.khanacademy.org/math/calculus-1)
- [3Blue1Brown - Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)

### Libros
- Stewart, "Calculus"
- [Deep Learning Book - Numerical Computation](https://www.deeplearningbook.org/contents/numerical.html)

### PrÃ¡ctica
- [Brilliant.org - Calculus](https://brilliant.org/courses/calculus/)
- [PyTorch Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)

---

## 13. AutoevaluaciÃ³n

Â¿Puedes responder?

- [ ] Â¿QuÃ© mide una derivada?
- [ ] Â¿CÃ³mo aplicar la chain rule?
- [ ] Â¿QuÃ© es un gradiente?
- [ ] Â¿CÃ³mo funciona gradient descent?
- [ ] Â¿Por quÃ© usamos -âˆ‡L en optimizaciÃ³n?

Si respondiste todo, Â¡excelente! ContinÃºa con [Python y NumPy](04_python_numpy.md).

---

## PrÃ³ximos Pasos

1. **[Python y NumPy](04_python_numpy.md)** - ProgramaciÃ³n prÃ¡ctica
2. **[OptimizaciÃ³n](05_conceptos_optimizacion.md)** - Algoritmos de optimizaciÃ³n
3. **[Fundamentos de RL](../01_fundamentos/introduccion.md)** - Â¡Empezar RL!

Â¡Sigue adelante! ğŸš€
