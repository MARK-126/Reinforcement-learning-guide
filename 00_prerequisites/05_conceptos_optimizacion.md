# Conceptos de Optimizaci√≥n para Deep Reinforcement Learning

## üéØ Objetivo

En Deep RL, necesitamos **optimizar** (encontrar mejores) pol√≠ticas, value functions, y redes neuronales. Esta gu√≠a cubre los algoritmos de optimizaci√≥n m√°s importantes.

---

## 1. El Problema de Optimizaci√≥n

### 1.1 Definici√≥n

**Objetivo**: Encontrar par√°metros Œ∏ que minimicen (o maximicen) una funci√≥n objetivo J(Œ∏).

**Minimizaci√≥n** (ej: loss de red neuronal):
```
Œ∏* = argmin_Œ∏ L(Œ∏)
```

**Maximizaci√≥n** (ej: retorno esperado en RL):
```
Œ∏* = argmax_Œ∏ J(Œ∏)
```

**Truco**: Maximizar J(Œ∏) = Minimizar -J(Œ∏)

### 1.2 Ejemplos en RL

**Supervised Learning** (para comparaci√≥n):
```
Minimizar: L(Œ∏) = (1/N) Œ£·µ¢ (f_Œ∏(x·µ¢) - y·µ¢)¬≤
```

**Policy Gradient**:
```
Maximizar: J(Œ∏) = E_œÄ[Œ£ Œ≥·µór‚Çú]
```

**DQN**:
```
Minimizar: L(Œ∏) = E[(r + Œ≥ max_a' Q(s',a';Œ∏‚Åª) - Q(s,a;Œ∏))¬≤]
```

---

## 2. Gradient Descent (Descenso por Gradiente)

### 2.1 Idea B√°sica

Si quieres **minimizar** L(Œ∏), mu√©vete en direcci√≥n opuesta al gradiente:

```
Œ∏_{t+1} = Œ∏_t - Œ±¬∑‚àá_Œ∏ L(Œ∏_t)
```

Donde:
- **Œ±** (alpha): learning rate (paso)
- **‚àá_Œ∏ L**: gradiente (direcci√≥n de mayor crecimiento)

**Analog√≠a**: Bajar una monta√±a en la niebla siguiendo la pendiente m√°s inclinada.

### 2.2 Tipos de Gradient Descent

#### Batch Gradient Descent

Usa **todos** los datos para calcular gradiente:
```
Œ∏ = Œ∏ - Œ±¬∑(1/N)Œ£·µ¢‚Çå‚ÇÅ·¥∫ ‚àá_Œ∏ L(Œ∏; x·µ¢, y·µ¢)
```

**Ventajas**: Gradiente exacto, convergencia suave
**Desventajas**: Lento para datasets grandes

#### Stochastic Gradient Descent (SGD)

Usa **un solo** ejemplo cada vez:
```
Œ∏ = Œ∏ - Œ±¬∑‚àá_Œ∏ L(Œ∏; x·µ¢, y·µ¢)
```

**Ventajas**: R√°pido, puede escapar m√≠nimos locales
**Desventajas**: Ruidoso, inestable

#### Mini-Batch Gradient Descent

Usa **mini-batch** de B ejemplos:
```
Œ∏ = Œ∏ - Œ±¬∑(1/B)Œ£·µ¢‚Çå‚ÇÅ·¥Æ ‚àá_Œ∏ L(Œ∏; x·µ¢, y·µ¢)
```

**Ventajas**: Balance entre velocidad y estabilidad
**Uso t√≠pico**: B = 32, 64, 128, 256

**C√≥digo**:
```python
import numpy as np

# Datos
X = np.random.randn(1000, 10)  # 1000 ejemplos, 10 features
y = np.random.randn(1000, 1)

# Par√°metros
theta = np.random.randn(10, 1)
alpha = 0.01
batch_size = 32

# Mini-batch SGD
for epoch in range(100):
    # Shuffle datos
    indices = np.random.permutation(len(X))

    for i in range(0, len(X), batch_size):
        # Mini-batch
        batch_indices = indices[i:i+batch_size]
        X_batch = X[batch_indices]
        y_batch = y[batch_indices]

        # Forward
        y_pred = X_batch @ theta
        error = y_pred - y_batch

        # Gradiente
        gradient = (2 / batch_size) * X_batch.T @ error

        # Update
        theta -= alpha * gradient
```

### 2.3 Learning Rate (Œ±)

El learning rate controla el tama√±o del paso.

**Demasiado grande**:
```
Œ∏ ‚Üí overshooting ‚Üí divergencia
```

**Demasiado peque√±o**:
```
Œ∏ ‚Üí convergencia muy lenta
```

**Valores t√≠picos**: 0.001, 0.0001, 0.01

**Visualizaci√≥n**:
```python
import matplotlib.pyplot as plt

def f(x):
    return x**2

def df(x):
    return 2*x

x = 5.0

# Diferentes learning rates
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
learning_rates = [0.1, 0.5, 1.1]

for idx, alpha in enumerate(learning_rates):
    x_history = [x]
    x_temp = x

    for _ in range(20):
        x_temp = x_temp - alpha * df(x_temp)
        x_history.append(x_temp)

    axes[idx].plot(x_history, [f(xi) for xi in x_history], 'o-')
    axes[idx].set_title(f'Œ± = {alpha}')
    axes[idx].set_xlabel('Iteraci√≥n')
    axes[idx].set_ylabel('f(x)')
    axes[idx].grid()

plt.tight_layout()
plt.show()
```

---

## 3. Momentum (Momento)

### 3.1 Problema de SGD Vanilla

SGD puro puede:
- Oscilar en valles estrechos
- Atascarse en plateaus
- Ser muy ruidoso

### 3.2 SGD con Momentum

Agrega "inercia" acumulando gradientes pasados:

```
v_t = Œ≤¬∑v_{t-1} + ‚àá_Œ∏ L(Œ∏_t)
Œ∏_{t+1} = Œ∏_t - Œ±¬∑v_t
```

Donde:
- **v**: velocity (velocidad acumulada)
- **Œ≤** (beta): coeficiente de momento (t√≠picamente 0.9)

**Analog√≠a**: Una bola rodando cuesta abajo acumula velocidad.

**Ventajas**:
- Acelera en direcciones consistentes
- Reduce oscilaciones
- Ayuda a escapar m√≠nimos locales

**C√≥digo**:
```python
# SGD con Momentum
v = np.zeros_like(theta)
beta = 0.9
alpha = 0.01

for epoch in range(100):
    gradient = compute_gradient(theta)

    # Actualizar velocidad
    v = beta * v + gradient

    # Actualizar par√°metros
    theta -= alpha * v
```

---

## 4. Adam (Adaptive Moment Estimation)

### 4.1 Motivaci√≥n

Adam combina:
- **Momentum** (first moment)
- **RMSprop** (second moment - adapta learning rate por par√°metro)

**Es el optimizador m√°s popular en Deep RL.**

### 4.2 Algoritmo

```
m_t = Œ≤‚ÇÅ¬∑m_{t-1} + (1-Œ≤‚ÇÅ)¬∑‚àá_Œ∏ L          # First moment (momentum)
v_t = Œ≤‚ÇÇ¬∑v_{t-1} + (1-Œ≤‚ÇÇ)¬∑(‚àá_Œ∏ L)¬≤       # Second moment (varianza)

mÃÇ_t = m_t / (1-Œ≤‚ÇÅ·µó)                      # Bias correction
vÃÇ_t = v_t / (1-Œ≤‚ÇÇ·µó)

Œ∏_{t+1} = Œ∏_t - Œ±¬∑mÃÇ_t / (‚àövÃÇ_t + Œµ)
```

**Hyperpar√°metros por defecto**:
- Œ± = 0.001
- Œ≤‚ÇÅ = 0.9
- Œ≤‚ÇÇ = 0.999
- Œµ = 1e-8

**Ventajas**:
- Adapta learning rate por par√°metro
- Funciona bien out-of-the-box
- Converge r√°pido

**C√≥digo**:
```python
class Adam:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.alpha = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None  # First moment
        self.v = None  # Second moment
        self.t = 0     # Time step

    def update(self, theta, gradient):
        # Inicializar momentos
        if self.m is None:
            self.m = np.zeros_like(theta)
            self.v = np.zeros_like(theta)

        self.t += 1

        # Actualizar momentos
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradient
        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradient ** 2)

        # Bias correction
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)

        # Actualizar par√°metros
        theta -= self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)

        return theta

# Uso
optimizer = Adam(learning_rate=0.001)

for epoch in range(100):
    gradient = compute_gradient(theta)
    theta = optimizer.update(theta, gradient)
```

### 4.3 Comparaci√≥n Visual

```python
# Comparar SGD vs Momentum vs Adam
def rosenbrock(x, y):
    """Funci√≥n de prueba dif√≠cil de optimizar"""
    return (1 - x)**2 + 100 * (y - x**2)**2

# Implementar y graficar trayectorias...
```

---

## 5. Learning Rate Schedules

### 5.1 ¬øPor Qu√© Decaer Learning Rate?

- **Inicio**: Œ± grande para explorar
- **Final**: Œ± peque√±o para convergencia fina

### 5.2 Tipos de Schedules

#### Step Decay

```
Œ±_t = Œ±_0 ¬∑ Œ≥^(t // k)
```

Reduce Œ± por factor Œ≥ cada k epochs.

**Ejemplo**:
```python
def step_decay(epoch, alpha_0=0.1, drop=0.5, epochs_drop=100):
    return alpha_0 * (drop ** (epoch // epochs_drop))

# Epoch 0-99:    Œ± = 0.1
# Epoch 100-199: Œ± = 0.05
# Epoch 200-299: Œ± = 0.025
```

#### Exponential Decay

```
Œ±_t = Œ±_0 ¬∑ e^(-Œªt)
```

**C√≥digo**:
```python
def exponential_decay(epoch, alpha_0=0.1, decay_rate=0.01):
    return alpha_0 * np.exp(-decay_rate * epoch)
```

#### Polynomial Decay

```
Œ±_t = Œ±_0 ¬∑ (1 - t/T)^p
```

**C√≥digo**:
```python
def polynomial_decay(epoch, max_epochs, alpha_0=0.1, power=1.0):
    return alpha_0 * (1 - epoch / max_epochs) ** power
```

#### Cosine Annealing

```
Œ±_t = Œ±_min + (Œ±_max - Œ±_min) ¬∑ (1 + cos(œÄt/T)) / 2
```

**C√≥digo**:
```python
def cosine_annealing(epoch, max_epochs, alpha_max=0.1, alpha_min=0.0):
    return alpha_min + (alpha_max - alpha_min) * \
           (1 + np.cos(np.pi * epoch / max_epochs)) / 2
```

**Visualizar**:
```python
epochs = np.arange(0, 1000)

plt.figure(figsize=(12, 4))

plt.subplot(1, 4, 1)
plt.plot(epochs, [step_decay(e) for e in epochs])
plt.title('Step Decay')

plt.subplot(1, 4, 2)
plt.plot(epochs, [exponential_decay(e) for e in epochs])
plt.title('Exponential Decay')

plt.subplot(1, 4, 3)
plt.plot(epochs, [polynomial_decay(e, 1000) for e in epochs])
plt.title('Polynomial Decay')

plt.subplot(1, 4, 4)
plt.plot(epochs, [cosine_annealing(e, 1000) for e in epochs])
plt.title('Cosine Annealing')

plt.tight_layout()
plt.show()
```

---

## 6. Gradient Clipping

### 6.1 Problema: Exploding Gradients

En redes profundas, gradientes pueden **explotar** (volverse muy grandes).

### 6.2 Soluci√≥n: Clipear Gradientes

Limita la magnitud del gradiente:

**Clip by value**:
```python
gradient = np.clip(gradient, -clip_value, clip_value)
```

**Clip by norm** (m√°s com√∫n):
```python
def clip_gradient_norm(gradient, max_norm=1.0):
    """Clipea gradiente si su norma excede max_norm"""
    norm = np.linalg.norm(gradient)
    if norm > max_norm:
        gradient = gradient * (max_norm / norm)
    return gradient

# Uso
gradient = compute_gradient(theta)
gradient = clip_gradient_norm(gradient, max_norm=1.0)
theta -= alpha * gradient
```

**En PyTorch**:
```python
import torch

# Calcular gradientes
loss.backward()

# Clipear antes de update
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Actualizar pesos
optimizer.step()
```

---

## 7. Optimizaci√≥n en Deep RL

### 7.1 Policy Gradient

**Objetivo**: Maximizar J(Œ∏) = E_œÄ[Œ£ Œ≥·µór‚Çú]

**Update** (gradient ascent):
```
Œ∏ = Œ∏ + Œ±¬∑‚àá_Œ∏ J(Œ∏)
```

**Policy Gradient Theorem**:
```
‚àá_Œ∏ J(Œ∏) = E_œÄ[‚àá_Œ∏ log œÄ_Œ∏(a|s)¬∑Q^œÄ(s,a)]
```

**C√≥digo**:
```python
# REINFORCE algorithm
def train_policy_gradient(env, policy, optimizer, num_episodes=1000):
    for episode in range(num_episodes):
        states, actions, rewards = [], [], []

        # Generar episodio
        state = env.reset()
        done = False

        while not done:
            action = policy.select_action(state)
            next_state, reward, done, _ = env.step(action)

            states.append(state)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        # Calcular retornos
        returns = compute_returns(rewards, gamma=0.99)

        # Policy gradient update
        for t in range(len(states)):
            state = states[t]
            action = actions[t]
            G = returns[t]

            # Log probability
            log_prob = policy.log_prob(state, action)

            # Loss (negative para maximizar)
            loss = -log_prob * G

            # Backprop
            loss.backward()

        # Update
        optimizer.step()
        optimizer.zero_grad()
```

### 7.2 Q-Learning con Redes Neuronales (DQN)

**Objetivo**: Minimizar L(Œ∏) = E[(target - Q(s,a;Œ∏))¬≤]

**C√≥digo**:
```python
# DQN training loop
def train_dqn(env, q_network, target_network, optimizer,
              replay_buffer, batch_size=32):

    # Sample batch
    states, actions, rewards, next_states, dones = \
        replay_buffer.sample(batch_size)

    # Current Q-values
    q_values = q_network(states)
    q_values = q_values.gather(1, actions.unsqueeze(1))

    # Target Q-values (con target network)
    with torch.no_grad():
        next_q_values = target_network(next_states).max(1)[0]
        targets = rewards + gamma * next_q_values * (1 - dones)

    # Loss
    loss = F.mse_loss(q_values.squeeze(), targets)

    # Optimize
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(q_network.parameters(), max_norm=1.0)
    optimizer.step()
```

### 7.3 Actor-Critic

**Dos redes**:
- **Actor** œÄ_Œ∏: genera acciones
- **Critic** V_œÜ: eval√∫a estados

**Updates**:
```
Actor:  Œ∏ = Œ∏ + Œ±_actor¬∑‚àá_Œ∏ log œÄ_Œ∏(a|s)¬∑A(s,a)
Critic: œÜ = œÜ - Œ±_critic¬∑‚àá_œÜ(V_œÜ(s) - target)¬≤
```

Donde A(s,a) = Q(s,a) - V(s) es la advantage function.

---

## 8. Hiperpar√°metros: C√≥mo Elegir

### 8.1 Learning Rate (Œ±)

**Heur√≠stica**:
- **Policy Gradient**: 1e-4 a 1e-3
- **DQN**: 1e-4 a 1e-3
- **Actor-Critic**: 1e-4 (actor), 1e-3 (critic)

**Grid search**:
```python
alphas = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3]

for alpha in alphas:
    rewards = train(alpha=alpha)
    print(f"Œ±={alpha}: Mean reward = {np.mean(rewards)}")
```

### 8.2 Batch Size

**Valores t√≠picos**: 32, 64, 128, 256

**Trade-off**:
- Peque√±o: M√°s ruidoso, m√°s actualizaciones
- Grande: M√°s estable, menos actualizaciones

### 8.3 Optimizador

**Recomendaciones**:
- **Empezar**: Adam con defaults
- **Fine-tuning**: SGD con momentum
- **R√°pido**: RMSprop

### 8.4 Gamma (descuento)

**Valores t√≠picos**: 0.95, 0.99, 0.999

**Heur√≠stica**:
- Episodios cortos: Œ≥ = 0.95
- Episodios largos: Œ≥ = 0.99 o 0.999

---

## 9. Trucos Pr√°cticos

### 9.1 Normalizaci√≥n de Inputs

```python
# Normalizar estados
state_mean = np.mean(states, axis=0)
state_std = np.std(states, axis=0)

state_normalized = (state - state_mean) / (state_std + 1e-8)
```

### 9.2 Reward Scaling

```python
# Escalar recompensas a rango [-1, 1] o [0, 1]
reward_normalized = reward / reward_max

# O estandarizar
reward_normalized = (reward - reward_mean) / reward_std
```

### 9.3 Advantage Normalization

```python
# Normalizar advantages para estabilidad
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
```

### 9.4 Target Network Updates

```python
# Actualizaci√≥n soft (polyak averaging)
tau = 0.005

for target_param, param in zip(target_net.parameters(), q_net.parameters()):
    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
```

### 9.5 Entropy Bonus (Policy Gradient)

```python
# Agregar entrop√≠a para fomentar exploraci√≥n
entropy = -torch.sum(probs * torch.log(probs + 1e-8))
loss = policy_loss - entropy_coefficient * entropy
```

---

## 10. Debugging Optimization

### 10.1 M√©tricas a Monitorear

```python
# Durante entrenamiento, graficar:
- Learning rate
- Loss value
- Gradient norm
- Weight norm
- Reward por episodio
```

### 10.2 Verificar Gradientes

```python
# Verificar que gradientes no sean NaN o Inf
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: grad_norm = {grad_norm:.4f}")

        if np.isnan(grad_norm) or np.isinf(grad_norm):
            print(f"WARNING: {name} has NaN or Inf gradients!")
```

### 10.3 Loss Landscape Visualization

```python
# Graficar loss vs epochs
plt.plot(loss_history)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.yscale('log')
plt.grid()
plt.show()
```

---

## 11. Ejercicios Pr√°cticos

### Ejercicio 1: Implementar Momentum

```python
def sgd_momentum(theta, gradients, velocity, alpha=0.01, beta=0.9):
    """
    Implementa SGD con momentum

    Args:
        theta: Par√°metros actuales
        gradients: Lista de gradientes por mini-batch
        velocity: Velocidad actual
        alpha: Learning rate
        beta: Momentum coefficient

    Returns:
        theta actualizado, velocity actualizado
    """
    # TU C√ìDIGO AQU√ç
    pass
```

<details>
<summary>Ver soluci√≥n</summary>

```python
def sgd_momentum(theta, gradients, velocity, alpha=0.01, beta=0.9):
    for gradient in gradients:
        velocity = beta * velocity + gradient
        theta -= alpha * velocity

    return theta, velocity
```
</details>

### Ejercicio 2: Comparar Optimizadores

Implementa experimento que compare SGD, Momentum y Adam en una funci√≥n simple.

---

## 12. Cheat Sheet

| Optimizador | Update Rule | Pros | Contras | Uso en RL |
|-------------|-------------|------|---------|-----------|
| **SGD** | Œ∏ -= Œ±¬∑‚àáL | Simple | Lento, ruidoso | Raro |
| **SGD + Momentum** | v=Œ≤v+‚àáL; Œ∏-=Œ±v | M√°s r√°pido | Requiere tuning | Ocasional |
| **Adam** | Adaptivo | Funciona bien out-of-box | Puede overshoot | **M√ÅS COM√öN** |
| **RMSprop** | Adaptivo | Bueno para RNNs | Menos usado | Ocasional |

---

## 13. Recursos Adicionales

### Papers
- Kingma & Ba (2015). "Adam: A Method for Stochastic Optimization"
- Sutskever et al. (2013). "On the importance of initialization and momentum"

### Tutoriales
- [CS231n - Optimization](http://cs231n.github.io/neural-networks-3/)
- [Distill.pub - Why Momentum Works](https://distill.pub/2017/momentum/)

### Herramientas
- [PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html)
- [TensorFlow Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)

---

## 14. Autoevaluaci√≥n

¬øPuedes explicar...?

- [ ] Diferencia entre SGD, mini-batch SGD, y batch GD
- [ ] C√≥mo funciona momentum
- [ ] Por qu√© Adam es popular
- [ ] Cu√°ndo usar gradient clipping
- [ ] C√≥mo elegir learning rate

Si respondiste todo, ¬°est√°s listo para Deep RL! üéâ

---

## Pr√≥ximos Pasos

1. **[README de Prerequisites](00_README.md)** - Resumen de toda la preparaci√≥n
2. **[Fundamentos de RL](../01_fundamentos/introduccion.md)** - ¬°Empezar con RL!

¬°Felicitaciones por completar todos los prerequisitos! üöÄ
