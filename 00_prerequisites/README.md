# ðŸ“š Prerequisites para Reinforcement Learning

## ðŸŽ¯ Â¿Para QuiÃ©n es Esta SecciÃ³n?

Esta secciÃ³n estÃ¡ diseÃ±ada para personas con **CERO conocimientos previos** en:
- MatemÃ¡ticas (mÃ¡s allÃ¡ de aritmÃ©tica bÃ¡sica)
- ProgramaciÃ³n
- Machine Learning
- Reinforcement Learning

**Si ya tienes experiencia**, puedes saltar directamente a [Fundamentos de RL](../01_fundamentos/).

---

## ðŸ—ºï¸ Mapa de Contenido

```
00_prerequisites/
â”‚
â”œâ”€â”€ 01_matematicas_basicas.md      â† Empieza aquÃ­
â”‚   â”œâ”€â”€ Probabilidad y estadÃ­stica
â”‚   â”œâ”€â”€ NotaciÃ³n matemÃ¡tica
â”‚   â”œâ”€â”€ Valor esperado
â”‚   â”œâ”€â”€ Distribuciones
â”‚   â””â”€â”€ Descuento geomÃ©trico
â”‚
â”œâ”€â”€ 02_algebra_lineal.md
â”‚   â”œâ”€â”€ Vectores y operaciones
â”‚   â”œâ”€â”€ Matrices y multiplicaciÃ³n
â”‚   â”œâ”€â”€ Producto punto y normas
â”‚   â”œâ”€â”€ EcuaciÃ³n de Bellman matricial
â”‚   â””â”€â”€ NumPy para Ã¡lgebra lineal
â”‚
â”œâ”€â”€ 03_calculo_basico.md
â”‚   â”œâ”€â”€ Derivadas desde cero
â”‚   â”œâ”€â”€ Funciones de activaciÃ³n
â”‚   â”œâ”€â”€ Derivadas parciales
â”‚   â”œâ”€â”€ Gradientes
â”‚   â”œâ”€â”€ Chain rule
â”‚   â”œâ”€â”€ Gradient descent
â”‚   â””â”€â”€ Backpropagation
â”‚
â”œâ”€â”€ 04_python_numpy.md
â”‚   â”œâ”€â”€ Python esencial
â”‚   â”œâ”€â”€ Estructuras de datos
â”‚   â”œâ”€â”€ Control de flujo
â”‚   â”œâ”€â”€ Funciones y clases
â”‚   â”œâ”€â”€ NumPy completo
â”‚   â”œâ”€â”€ Matplotlib bÃ¡sico
â”‚   â””â”€â”€ CÃ³digo prÃ¡ctico para RL
â”‚
â””â”€â”€ 05_conceptos_optimizacion.md
    â”œâ”€â”€ Gradient descent
    â”œâ”€â”€ SGD, momentum, Adam
    â”œâ”€â”€ Learning rate schedules
    â”œâ”€â”€ Gradient clipping
    â”œâ”€â”€ OptimizaciÃ³n en Deep RL
    â””â”€â”€ HiperparÃ¡metros
```

---

## ðŸ“– Orden de Estudio Recomendado

### ðŸŸ¢ Ruta Completa (Principiante Absoluto)

**Semana 1-2**: MatemÃ¡ticas BÃ¡sicas
- [ ] [01_matematicas_basicas.md](01_matematicas_basicas.md)
- [ ] Completar todos los ejercicios
- [ ] Implementar funciones en Python
- **Tiempo estimado**: 10-15 horas

**Semana 2-3**: Ãlgebra Lineal
- [ ] [02_algebra_lineal.md](02_algebra_lineal.md)
- [ ] Practicar operaciones en NumPy
- [ ] Resolver ecuaciÃ³n de Bellman matricial
- **Tiempo estimado**: 10-15 horas

**Semana 3-4**: CÃ¡lculo
- [ ] [03_calculo_basico.md](03_calculo_basico.md)
- [ ] Entender derivadas y gradientes
- [ ] Implementar gradient descent
- **Tiempo estimado**: 10-15 horas

**Semana 4-5**: Python y NumPy
- [ ] [04_python_numpy.md](04_python_numpy.md)
- [ ] Escribir cÃ³digo prÃ¡ctico
- [ ] Implementar estructuras de datos para RL
- **Tiempo estimado**: 15-20 horas

**Semana 5-6**: OptimizaciÃ³n
- [ ] [05_conceptos_optimizacion.md](05_conceptos_optimizacion.md)
- [ ] Comparar optimizadores
- [ ] Experimentar con learning rates
- **Tiempo estimado**: 8-12 horas

**Total**: ~6 semanas (55-77 horas)

### ðŸŸ¡ Ruta Acelerada (Con Algo de Experiencia)

Si ya sabes programaciÃ³n bÃ¡sica:
1. Revisar rÃ¡pido: 01, 02, 03 (3-5 dÃ­as)
2. Enfocarse en: 04, 05 (1 semana)
3. Pasar a [Fundamentos de RL](../01_fundamentos/)

**Total**: ~2 semanas

### ðŸ”´ Solo Refresco (Experiencia en ML)

Si ya conoces ML:
1. Hojear cada documento para recordar notaciÃ³n
2. Enfocarse en diferencias especÃ­ficas de RL
3. Ir directo a tutoriales de RL

**Total**: 2-3 dÃ­as

---

## ðŸŽ“ Objetivos de Aprendizaje

Al completar esta secciÃ³n, podrÃ¡s:

### MatemÃ¡ticas
âœ… Calcular probabilidades y esperanzas
âœ… Trabajar con distribuciones (normal, uniforme, Bernoulli)
âœ… Entender notaciÃ³n matemÃ¡tica (Î£, argmax, E[Â·])
âœ… Calcular retornos descontados

### Ãlgebra Lineal
âœ… Operar con vectores y matrices
âœ… Calcular productos punto y matriciales
âœ… Usar NumPy para Ã¡lgebra lineal
âœ… Entender ecuaciÃ³n de Bellman en forma matricial

### CÃ¡lculo
âœ… Calcular derivadas bÃ¡sicas
âœ… Aplicar chain rule
âœ… Computar gradientes
âœ… Implementar gradient descent
âœ… Entender backpropagation conceptualmente

### ProgramaciÃ³n
âœ… Escribir Python funcional
âœ… Usar NumPy eficientemente
âœ… Implementar clases para agentes
âœ… Visualizar resultados con Matplotlib
âœ… Trabajar con Gymnasium (OpenAI Gym)

### OptimizaciÃ³n
âœ… Entender diferentes optimizadores (SGD, Adam)
âœ… Usar learning rate schedules
âœ… Aplicar gradient clipping
âœ… Debuggear problemas de optimizaciÃ³n

---

## ðŸ’¡ Consejos para el Estudio

### 1. Practica Activamente

âŒ **Mal**: Solo leer pasivamente
âœ… **Bien**: Implementar cada concepto en cÃ³digo

**Ejemplo**:
```python
# DespuÃ©s de leer sobre valor esperado, implemÃ©ntalo
def valor_esperado(valores, probabilidades):
    return sum(v * p for v, p in zip(valores, probabilidades))

# Test
valores = [1, 2, 3, 4, 5, 6]
probs = [1/6] * 6
print(f"E[dado] = {valor_esperado(valores, probs)}")  # 3.5
```

### 2. Haz Todos los Ejercicios

Cada documento tiene ejercicios prÃ¡cticos. **Hazlos todos.**

### 3. Usa Jupyter Notebooks

```bash
# Instala Jupyter
pip install jupyter

# Crea notebook para cada tema
jupyter notebook matematicas_basicas_practica.ipynb
```

### 4. Consulta Recursos Externos

Cada documento tiene secciÃ³n de recursos adicionales. Ãšsala.

### 5. No Te Atasques

Si algo no tiene sentido despuÃ©s de 30 minutos:
1. Toma un descanso
2. Busca explicaciÃ³n alternativa (YouTube, Khan Academy)
3. Sigue adelante y regresa despuÃ©s

### 6. Forma un Grupo de Estudio

Explica conceptos a otros. Si puedes enseÃ±arlo, lo entendiste.

---

## ðŸ› ï¸ Setup del Entorno

### InstalaciÃ³n BÃ¡sica

```bash
# Python 3.8+
python --version

# Crear entorno virtual
python -m venv rl_env
source rl_env/bin/activate  # Linux/Mac
# o
rl_env\Scripts\activate  # Windows

# Instalar dependencias
pip install numpy matplotlib jupyter
pip install gymnasium torch
```

### Verificar InstalaciÃ³n

```python
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
import torch

print("NumPy:", np.__version__)
print("PyTorch:", torch.__version__)
print("Gymnasium:", gym.__version__)

# Test
env = gym.make('CartPole-v1')
state, info = env.reset()
print("Estado inicial:", state)
```

---

## ðŸ“Š AutoevaluaciÃ³n

Antes de pasar a RL, verifica que puedes:

### Test RÃ¡pido de MatemÃ¡ticas

```python
# 1. Calcular P(A y B) si P(A)=0.3, P(B|A)=0.7
# Respuesta: 0.21

# 2. Â¿QuÃ© es argmax([0.2, 0.8, 0.5, 0.9])?
# Respuesta: 3

# 3. Calcular Î£áµ¢â‚Œâ‚âµ iÂ²
# Respuesta: 55

# 4. Si recompensas = [1, 1, 1], Î³=0.9, Â¿cuÃ¡l es Gâ‚€?
# Respuesta: 1 + 0.9 + 0.81 = 2.71
```

### Test RÃ¡pido de Ãlgebra Lineal

```python
import numpy as np

# 1. Producto punto de [1, 2, 3] y [4, 5, 6]
# Respuesta: 32

# 2. Shape de matriz A(3x4) @ vector v(4x1)
# Respuesta: (3x1)

# 3. Â¿QuÃ© hace np.argmax([[1, 2], [3, 4]])?
# Respuesta: 3 (Ã­ndice aplanado del mÃ¡ximo)
```

### Test RÃ¡pido de CÃ¡lculo

```python
# 1. Derivada de f(x) = xÂ³ + 2x
# Respuesta: f'(x) = 3xÂ² + 2

# 2. Si f(x,y) = xÂ²y, Â¿cuÃ¡l es âˆ‚f/âˆ‚x?
# Respuesta: 2xy

# 3. Â¿En quÃ© direcciÃ³n apunta el gradiente?
# Respuesta: DirecciÃ³n de mayor crecimiento
```

### Test RÃ¡pido de Python

```python
# 1. Crear array NumPy de ceros 3x4
# Respuesta: np.zeros((3, 4))

# 2. Obtener elemento mÃ¡ximo de lista
# Respuesta: max(lista) o np.max(array)

# 3. Iterar con Ã­ndice sobre lista
# Respuesta: for i, item in enumerate(lista):
```

**Si respondiste correctamente 80%+**: Â¡Listo para RL!
**Si no**: Revisa las secciones relevantes.

---

## ðŸ”— PrÃ³ximos Pasos

Una vez completados los prerequisites:

1. **[Fundamentos de RL](../01_fundamentos/introduccion.md)**
   - Â¿QuÃ© es RL?
   - MDPs
   - Ecuaciones de Bellman
   - Value functions y polÃ­ticas

2. **[Tutorial 01: Dynamic Programming](../notebooks/01_dynamic_programming_tutorial.ipynb)**
   - Policy Evaluation
   - Policy Iteration
   - Value Iteration
   - ImplementaciÃ³n prÃ¡ctica

3. **[Tutorial 02: Monte Carlo](../notebooks/02_monte_carlo_tutorial.ipynb)**
   - MÃ©todos model-free
   - On-policy vs off-policy
   - Importance sampling

---

## ðŸ“š Recursos Adicionales

### Cursos Online (Gratis)

**MatemÃ¡ticas**:
- [Khan Academy - Probabilidad](https://es.khanacademy.org/math/probability)
- [Khan Academy - EstadÃ­stica](https://es.khanacademy.org/math/statistics-probability)
- [Khan Academy - Ãlgebra Lineal](https://es.khanacademy.org/math/linear-algebra)
- [Khan Academy - CÃ¡lculo](https://es.khanacademy.org/math/calculus-1)

**Python**:
- [Python.org Tutorial](https://docs.python.org/3/tutorial/)
- [Real Python](https://realpython.com/)
- [Automate the Boring Stuff](https://automatetheboringstuff.com/)

**NumPy**:
- [NumPy Quickstart](https://numpy.org/doc/stable/user/quickstart.html)
- [NumPy Tutorial por CS231n](http://cs231n.github.io/python-numpy-tutorial/)

**Machine Learning Math**:
- [Mathematics for Machine Learning (Book)](https://mml-book.github.io/)
- [Deep Learning Book - Math Chapters](https://www.deeplearningbook.org/)

### Videos (YouTube)

- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- [3Blue1Brown - Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
- [StatQuest - Statistics](https://www.youtube.com/c/joshstarmer)

### Libros

**BÃ¡sicos**:
- Strang, Gilbert. "Introduction to Linear Algebra"
- Stewart, James. "Calculus"
- Ross, Sheldon. "A First Course in Probability"

**ML Math**:
- Deisenroth et al. "Mathematics for Machine Learning"
- Boyd & Vandenberghe. "Convex Optimization"

---

## â“ FAQ

### P: Â¿Realmente necesito todo esto para RL?

**R**: Depende de tu objetivo:
- **Implementar algoritmos bÃ¡sicos (tabular RL)**: MatemÃ¡ticas bÃ¡sicas + Python (40% del contenido)
- **Entender papers de RL**: MatemÃ¡ticas + Ãlgebra + CÃ¡lculo (70%)
- **Implementar Deep RL**: Todo el contenido (100%)

### P: Â¿CuÃ¡nto tiempo me tomarÃ¡?

**R**:
- Principiante absoluto: 4-6 semanas (10 hrs/semana)
- Con algo de experiencia: 2-3 semanas
- Solo refresco: 3-5 dÃ­as

### P: Â¿Puedo aprender RL sin cÃ¡lculo?

**R**: SÃ­, para **RL tabular** (Q-Learning, SARSA, etc.). Pero **Deep RL** requiere cÃ¡lculo para entender backpropagation.

### P: Â¿Python es obligatorio?

**R**: No estrictamente, pero es el estÃ¡ndar de facto en RL. 95% de implementaciones y papers usan Python.

### P: Â¿QuÃ© si me salto los prerequisites?

**R**: Puedes intentarlo, pero:
- No entenderÃ¡s la matemÃ¡tica detrÃ¡s de los algoritmos
- TendrÃ¡s problemas implementando cÃ³digo
- Te costarÃ¡ debuggear y mejorar modelos

### P: Â¿Hay un test final?

**R**: SÃ­, implÃ­citamente: **Implementar Q-Learning desde cero** en un ambiente simple. Si puedes hacerlo, estÃ¡s listo.

---

## ðŸ¤ Contribuir

Â¿Encontraste un error? Â¿Tienes una mejor explicaciÃ³n? Â¡Contribuye!

1. Abre un issue en GitHub
2. PropÃ³n cambios vÃ­a PR
3. Comparte feedback en discusiones

---

## ðŸ“ Notas Finales

**Recuerda**:
- No intentes memorizar todo
- EnfÃ³cate en entender conceptos
- La prÃ¡ctica es mÃ¡s importante que la teorÃ­a
- Todos estos conceptos se reforzarÃ¡n durante el estudio de RL

**Cita motivacional**:
> "You don't need to be a mathematician to do RL, but you need to understand the math behind what you're doing."
> â€” David Silver

---

## ðŸŽ¯ Tu Progreso

Usa este checklist para trackear tu progreso:

```
Prerequisites
â”œâ”€â”€ [  ] 01_matematicas_basicas.md
â”‚   â”œâ”€â”€ [  ] Leer documento completo
â”‚   â”œâ”€â”€ [  ] Completar ejercicios
â”‚   â””â”€â”€ [  ] Implementar funciones clave
â”‚
â”œâ”€â”€ [  ] 02_algebra_lineal.md
â”‚   â”œâ”€â”€ [  ] Leer documento completo
â”‚   â”œâ”€â”€ [  ] Completar ejercicios
â”‚   â””â”€â”€ [  ] Practicar con NumPy
â”‚
â”œâ”€â”€ [  ] 03_calculo_basico.md
â”‚   â”œâ”€â”€ [  ] Leer documento completo
â”‚   â”œâ”€â”€ [  ] Completar ejercicios
â”‚   â””â”€â”€ [  ] Implementar gradient descent
â”‚
â”œâ”€â”€ [  ] 04_python_numpy.md
â”‚   â”œâ”€â”€ [  ] Leer documento completo
â”‚   â”œâ”€â”€ [  ] Completar ejercicios
â”‚   â””â”€â”€ [  ] Escribir agente bÃ¡sico
â”‚
â””â”€â”€ [  ] 05_conceptos_optimizacion.md
    â”œâ”€â”€ [  ] Leer documento completo
    â”œâ”€â”€ [  ] Completar ejercicios
    â””â”€â”€ [  ] Comparar optimizadores

Proyecto Final de Prerequisites:
[  ] Implementar Q-Learning tabular en GridWorld
```

---

Â¡Buena suerte en tu viaje de aprendizaje! ðŸš€

**Siguiente**: [MatemÃ¡ticas BÃ¡sicas](01_matematicas_basicas.md) â†’
