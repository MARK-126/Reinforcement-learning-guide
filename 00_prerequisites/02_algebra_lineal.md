# Ãlgebra Lineal para Reinforcement Learning

## ğŸ¯ Por QuÃ© Necesitas Ãlgebra Lineal en RL

En Reinforcement Learning, trabajamos constantemente con:
- **Vectores**: Para representar estados (posiciÃ³n, velocidad, etc.)
- **Matrices**: Para transiciones de probabilidad, redes neuronales
- **Operaciones matriciales**: Para calcular valores, propagar gradientes

Esta guÃ­a te enseÃ±a **solo lo esencial** para RL, desde cero.

---

## 1. Vectores desde Cero

### 1.1 Â¿QuÃ© es un Vector?

Un **vector** es una lista ordenada de nÃºmeros. PiÃ©nsalo como:
- Una flecha en el espacio (tiene direcciÃ³n y magnitud)
- Coordenadas de una ubicaciÃ³n
- CaracterÃ­sticas de un estado

**Ejemplos**:
```
PosiciÃ³n 2D: v = [3, 5]
              â†‘  â†‘
              x  y

Estado CartPole: s = [x, áº‹, Î¸, Î¸Ì‡]
                      â†‘  â†‘  â†‘  â†‘
                      posiciÃ³n, velocidad, Ã¡ngulo, velocidad_angular

Vector de recompensas: r = [1.5, 2.0, -0.5, 3.2]
```

### 1.2 NotaciÃ³n

**MatemÃ¡tica**:
```
v = [vâ‚, vâ‚‚, vâ‚ƒ]  o  v = â¡vâ‚â¤
                         â¢vâ‚‚â¥
                         â£vâ‚ƒâ¦
```

**Python/NumPy**:
```python
import numpy as np

v = np.array([3, 5])           # Vector 2D
s = np.array([0.1, 0.2, 0.3])  # Vector 3D
```

### 1.3 Operaciones con Vectores

#### Suma de Vectores

Suma elemento por elemento:
```
[aâ‚]   [bâ‚]   [aâ‚ + bâ‚]
[aâ‚‚] + [bâ‚‚] = [aâ‚‚ + bâ‚‚]
[aâ‚ƒ]   [bâ‚ƒ]   [aâ‚ƒ + bâ‚ƒ]
```

**Ejemplo**:
```
vâ‚ = [1, 2, 3]
vâ‚‚ = [4, 5, 6]
vâ‚ + vâ‚‚ = [1+4, 2+5, 3+6] = [5, 7, 9]
```

**Python**:
```python
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])
suma = v1 + v2  # array([5, 7, 9])
```

#### MultiplicaciÃ³n por Escalar

Multiplica cada elemento por un nÃºmero:
```
c Â· [aâ‚]   [cÂ·aâ‚]
    [aâ‚‚] = [cÂ·aâ‚‚]
    [aâ‚ƒ]   [cÂ·aâ‚ƒ]
```

**Ejemplo**:
```
2 Â· [1, 2, 3] = [2, 4, 6]
```

**Python**:
```python
v = np.array([1, 2, 3])
resultado = 2 * v  # array([2, 4, 6])
```

**En RL**: Descontar recompensas: `Î³ * V(s')`

#### Producto Punto (Dot Product)

Multiplica elementos correspondientes y suma:
```
vâ‚ Â· vâ‚‚ = vâ‚[0]Â·vâ‚‚[0] + vâ‚[1]Â·vâ‚‚[1] + ... + vâ‚[n]Â·vâ‚‚[n]
```

**Ejemplo**:
```
vâ‚ = [1, 2, 3]
vâ‚‚ = [4, 5, 6]
vâ‚ Â· vâ‚‚ = 1Â·4 + 2Â·5 + 3Â·6 = 4 + 10 + 18 = 32
```

**Python**:
```python
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])
dot_product = np.dot(v1, v2)  # 32
```

**En RL**:
- Calcular Q-values: `Q = w Â· Ï†(s,a)` (aproximaciÃ³n lineal)
- Producto de caracterÃ­sticas y pesos en redes neuronales

#### Norma (Magnitud)

La **norma** ||v|| es la "longitud" del vector:

```
||v|| = âˆš(vâ‚Â² + vâ‚‚Â² + ... + vâ‚™Â²)
```

**Ejemplo**:
```
v = [3, 4]
||v|| = âˆš(3Â² + 4Â²) = âˆš(9 + 16) = âˆš25 = 5
```

**Python**:
```python
v = np.array([3, 4])
norma = np.linalg.norm(v)  # 5.0
```

**En RL**: Medir magnitud de gradientes, distancia entre estados

---

## 2. Matrices desde Cero

### 2.1 Â¿QuÃ© es una Matriz?

Una **matriz** es una tabla rectangular de nÃºmeros. PiÃ©nsala como:
- ColecciÃ³n de vectores
- Tabla de transiciones de probabilidad
- Pesos de una capa de red neuronal

**NotaciÃ³n**:
```
      m columnas
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
n   â”‚ aâ‚â‚ aâ‚â‚‚ â”‚
f   â”‚ aâ‚‚â‚ aâ‚‚â‚‚ â”‚  Matriz A de tamaÃ±o n Ã— m
i   â”‚ aâ‚ƒâ‚ aâ‚ƒâ‚‚ â”‚
l   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
a
s
```

**Ejemplo - Tabla de transiciones**:
```
     sâ‚  sâ‚‚  sâ‚ƒ
sâ‚ [ 0.1 0.7 0.2 ]
sâ‚‚ [ 0.3 0.3 0.4 ]
sâ‚ƒ [ 0.0 0.5 0.5 ]

P[i][j] = probabilidad de ir de estado i a estado j
```

**Python**:
```python
# Matriz 3Ã—3
A = np.array([
    [0.1, 0.7, 0.2],
    [0.3, 0.3, 0.4],
    [0.0, 0.5, 0.5]
])

print(A.shape)  # (3, 3)
print(A[0, 1])  # 0.7 (fila 0, columna 1)
```

### 2.2 Tipos de Matrices Especiales

#### Matriz Identidad (I)

Tiene 1s en la diagonal, 0s en el resto:
```
Iâ‚ƒ = [ 1  0  0 ]
     [ 0  1  0 ]
     [ 0  0  1 ]
```

**Propiedad**: A Â· I = I Â· A = A

**Python**:
```python
I = np.eye(3)  # Matriz identidad 3Ã—3
```

**En RL**: Aparece en ecuaciones de Bellman en forma matricial: `V = (I - Î³P)â»Â¹R`

#### Matriz Diagonal

Solo tiene valores distintos de cero en la diagonal:
```
D = [ 2  0  0 ]
    [ 0  5  0 ]
    [ 0  0  3 ]
```

**Python**:
```python
D = np.diag([2, 5, 3])
```

#### Matriz Transpuesta

La transpuesta Aáµ€ intercambia filas por columnas:
```
     [ 1  2  3 ]        [ 1  4 ]
A =  [ 4  5  6 ]   Aáµ€ = [ 2  5 ]
                         [ 3  6 ]
```

**Python**:
```python
A = np.array([[1, 2, 3], [4, 5, 6]])
A_transpuesta = A.T
# array([[1, 4],
#        [2, 5],
#        [3, 6]])
```

### 2.3 Operaciones con Matrices

#### Suma de Matrices

Elemento por elemento (deben tener mismo tamaÃ±o):
```
[ 1  2 ]   [ 5  6 ]   [ 6   8 ]
[ 3  4 ] + [ 7  8 ] = [ 10  12 ]
```

**Python**:
```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A + B
# array([[ 6,  8],
#        [10, 12]])
```

#### MultiplicaciÃ³n Matriz-Vector

Multiplica matriz A (nÃ—m) por vector v (mÃ—1) para obtener vector resultado (nÃ—1):

```
[ aâ‚â‚  aâ‚â‚‚ ]   [ vâ‚ ]   [ aâ‚â‚Â·vâ‚ + aâ‚â‚‚Â·vâ‚‚ ]
[ aâ‚‚â‚  aâ‚‚â‚‚ ] Â· [ vâ‚‚ ] = [ aâ‚‚â‚Â·vâ‚ + aâ‚‚â‚‚Â·vâ‚‚ ]
```

**Ejemplo**:
```
[ 1  2 ]   [ 5 ]   [ 1Â·5 + 2Â·6 ]   [ 17 ]
[ 3  4 ] Â· [ 6 ] = [ 3Â·5 + 4Â·6 ] = [ 39 ]
```

**Python**:
```python
A = np.array([[1, 2], [3, 4]])
v = np.array([5, 6])
resultado = A @ v  # array([17, 39])
# TambiÃ©n: np.dot(A, v)
```

**En RL**:
- Calcular valores: `V_new = P Â· V_old`
- PropagaciÃ³n en redes neuronales: `h = W Â· x`

#### MultiplicaciÃ³n Matriz-Matriz

Para multiplicar A (nÃ—m) por B (mÃ—p), resultado es C (nÃ—p):

```
Cáµ¢â±¼ = Î£â‚– Aáµ¢â‚– Â· Bâ‚–â±¼
```

**Ejemplo visual**:
```
[ 1  2 ]   [ 5  6 ]   [ 1Â·5+2Â·7  1Â·6+2Â·8 ]   [ 19  22 ]
[ 3  4 ] Â· [ 7  8 ] = [ 3Â·5+4Â·7  3Â·6+4Â·8 ] = [ 43  50 ]
```

**Python**:
```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A @ B  # array([[19, 22], [43, 50]])
```

**âš ï¸ IMPORTANTE**: El orden importa: AÂ·B â‰  BÂ·A (generalmente)

### 2.4 Inversa de una Matriz

La **inversa** Aâ»Â¹ satisface: A Â· Aâ»Â¹ = Aâ»Â¹ Â· A = I

**Ejemplo**:
```
     [ 4  7 ]              [ 0.6  -0.7 ]
A =  [ 2  6 ]    Aâ»Â¹ =     [ -0.2  0.4 ]

Verifica: A Â· Aâ»Â¹ = I
```

**Python**:
```python
A = np.array([[4, 7], [2, 6]])
A_inv = np.linalg.inv(A)
print(A @ A_inv)  # Aproximadamente [[1, 0], [0, 1]]
```

**En RL**:
- Resolver Bellman directamente: `V = (I - Î³P)â»Â¹ Â· R`
- Solo para MDPs pequeÃ±os (computacionalmente costoso)

---

## 3. Aplicaciones Directas en RL

### 3.1 RepresentaciÃ³n de Estados como Vectores

**CartPole**:
```python
estado = np.array([
    0.02,   # posiciÃ³n del carro
    0.15,   # velocidad del carro
    -0.05,  # Ã¡ngulo del poste
    0.20    # velocidad angular
])
```

### 3.2 Value Function como Vector

Para n estados, V es un vector de n elementos:
```python
# GridWorld 3Ã—3 = 9 estados
V = np.array([0.0, -1.0, -2.0, -1.0, -2.0, -3.0, -2.0, -3.0, 0.0])
#              sâ‚    sâ‚‚    sâ‚ƒ    sâ‚„    sâ‚…    sâ‚†    sâ‚‡    sâ‚ˆ    sâ‚‰
```

### 3.3 Matriz de TransiciÃ³n P

Para ambiente con n estados:
```python
# P[i][j] = probabilidad de transiciÃ³n s_i â†’ s_j
P = np.array([
    [0.1, 0.7, 0.2],  # Desde estado 0
    [0.3, 0.3, 0.4],  # Desde estado 1
    [0.0, 0.5, 0.5]   # Desde estado 2
])

# Calcular siguiente distribuciÃ³n de estados
estado_actual = np.array([1, 0, 0])  # En estado 0
siguiente_dist = P.T @ estado_actual
# array([0.1, 0.7, 0.2]) - probabilidades de estar en cada estado
```

### 3.4 EcuaciÃ³n de Bellman en Forma Matricial

La ecuaciÃ³n de Bellman:
```
V^Ï€(s) = Î£â‚ Ï€(a|s) Î£â‚›â€² P(sâ€²|s,a)[R(s,a,sâ€²) + Î³V^Ï€(sâ€²)]
```

Se puede escribir como:
```
V = R + Î³PV
```

Donde:
- **V**: vector de valores (n Ã— 1)
- **R**: vector de recompensas (n Ã— 1)
- **P**: matriz de transiciones (n Ã— n)
- **Î³**: escalar de descuento

**SoluciÃ³n directa**:
```
V = (I - Î³P)â»Â¹ R
```

**CÃ³digo completo**:
```python
import numpy as np

# Definir MDP simple
gamma = 0.9
R = np.array([0, 0, 1])  # Recompensas
P = np.array([           # Transiciones
    [0.1, 0.7, 0.2],
    [0.3, 0.3, 0.4],
    [0.0, 0.5, 0.5]
])

# Resolver Bellman
I = np.eye(3)
V = np.linalg.inv(I - gamma * P) @ R
print("Valores Ã³ptimos:", V)
# V â‰ˆ [1.92, 2.62, 10.0]
```

### 3.5 Redes Neuronales (Adelanto)

Una capa de red neuronal es simplemente:
```
h = activation(W Â· x + b)
```

Donde:
- **W**: matriz de pesos
- **x**: vector de entrada (estado)
- **b**: vector de bias
- **h**: vector de salida (valores Q, etc.)

**Ejemplo**:
```python
# Capa simple: 4 entradas â†’ 2 salidas
W = np.random.randn(2, 4)  # Pesos
b = np.zeros(2)             # Bias
x = np.array([1, 2, 3, 4])  # Estado

# Forward pass
z = W @ x + b               # CombinaciÃ³n lineal
h = np.maximum(0, z)        # ReLU activation
print("Salida:", h)
```

---

## 4. Operaciones Ãštiles en NumPy

### 4.1 CreaciÃ³n de Arrays

```python
import numpy as np

# Vectores
v = np.array([1, 2, 3])
ceros = np.zeros(5)              # [0, 0, 0, 0, 0]
unos = np.ones(3)                # [1, 1, 1]
rango = np.arange(0, 10, 2)      # [0, 2, 4, 6, 8]
random = np.random.randn(4)      # 4 nÃºmeros aleatorios N(0,1)

# Matrices
M = np.array([[1, 2], [3, 4]])
ceros_mat = np.zeros((3, 4))     # Matriz 3Ã—4 de ceros
identidad = np.eye(5)            # Identidad 5Ã—5
random_mat = np.random.rand(2, 3) # 2Ã—3 aleatoria [0,1]
```

### 4.2 IndexaciÃ³n y Slicing

```python
v = np.array([10, 20, 30, 40, 50])

v[0]      # 10 (primer elemento)
v[-1]     # 50 (Ãºltimo elemento)
v[1:4]    # array([20, 30, 40]) (slice)
v[::2]    # array([10, 30, 50]) (cada 2 elementos)

# Matrices
M = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])

M[0, 0]   # 1 (fila 0, columna 0)
M[1, :]   # array([4, 5, 6]) (toda la fila 1)
M[:, 2]   # array([3, 6, 9]) (toda la columna 2)
M[1:, 1:] # array([[5, 6], [8, 9]]) (submatriz)
```

### 4.3 Broadcasting

NumPy automÃ¡ticamente "expande" arrays de diferente tamaÃ±o:

```python
v = np.array([1, 2, 3])
M = np.array([[10],
              [20],
              [30]])

# Broadcasting: suma cada fila de M con v
resultado = M + v
# array([[11, 12, 13],
#        [21, 22, 23],
#        [31, 32, 33]])
```

**En RL**: Ãštil para aplicar operaciones a lotes de estados.

### 4.4 ReducciÃ³n

```python
v = np.array([1, 2, 3, 4, 5])

np.sum(v)        # 15 (suma total)
np.mean(v)       # 3.0 (promedio)
np.max(v)        # 5 (mÃ¡ximo)
np.argmax(v)     # 4 (Ã­ndice del mÃ¡ximo)

# Para matrices
M = np.array([[1, 2, 3],
              [4, 5, 6]])

np.sum(M, axis=0)  # array([5, 7, 9]) - suma por columnas
np.sum(M, axis=1)  # array([ 6, 15])  - suma por filas
np.max(M, axis=1)  # array([3, 6])    - mÃ¡ximo por fila
```

---

## 5. Ejercicios PrÃ¡cticos

### Ejercicio 1: Producto Punto
Calcula vâ‚ Â· vâ‚‚ manualmente y verifica con NumPy:
```
vâ‚ = [2, 3, 4]
vâ‚‚ = [1, 0, 2]
```

<details>
<summary>Ver soluciÃ³n</summary>

```python
# Manual
resultado = 2*1 + 3*0 + 4*2 = 2 + 0 + 8 = 10

# NumPy
v1 = np.array([2, 3, 4])
v2 = np.array([1, 0, 2])
print(np.dot(v1, v2))  # 10
```
</details>

### Ejercicio 2: MultiplicaciÃ³n Matriz-Vector
Multiplica manualmente:
```
A = [ 1  2 ]    v = [ 3 ]
    [ 4  5 ]        [ 6 ]
```

<details>
<summary>Ver soluciÃ³n</summary>

```python
# Manual
fila 1: 1*3 + 2*6 = 3 + 12 = 15
fila 2: 4*3 + 5*6 = 12 + 30 = 42

resultado = [15, 42]

# NumPy
A = np.array([[1, 2], [4, 5]])
v = np.array([3, 6])
print(A @ v)  # array([15, 42])
```
</details>

### Ejercicio 3: EcuaciÃ³n de Bellman
Dado Î³=0.5, R=[1, 0, 10], y P=I (identidad), resuelve V = R + Î³PV.

<details>
<summary>Ver soluciÃ³n</summary>

```python
import numpy as np

gamma = 0.5
R = np.array([1, 0, 10])
P = np.eye(3)

# V = R + Î³PV
# V - Î³PV = R
# (I - Î³P)V = R
# V = (I - Î³P)â»Â¹ R

I = np.eye(3)
V = np.linalg.inv(I - gamma * P) @ R
print(V)  # array([ 2.,  0., 20.])
```

**InterpretaciÃ³n**: Con Î³=0.5 y transiciones a mismo estado, V = R/(1-Î³) = 2R.
</details>

### Ejercicio 4: Encontrar Mejor AcciÃ³n
Dado Q-values para 3 acciones, encuentra la mejor acciÃ³n:
```python
Q = np.array([0.2, 0.8, 0.5])
```

<details>
<summary>Ver soluciÃ³n</summary>

```python
Q = np.array([0.2, 0.8, 0.5])
mejor_accion = np.argmax(Q)
print(f"Mejor acciÃ³n: {mejor_accion}")  # 1
print(f"Valor Q: {Q[mejor_accion]}")     # 0.8
```
</details>

---

## 6. Conceptos Avanzados (Opcional)

### 6.1 Eigenvalues y Eigenvectors

Un **eigenvector** v de matriz A satisface:
```
A Â· v = Î» Â· v
```
Donde Î» (lambda) es el **eigenvalue**.

**En RL**:
- Analizar convergencia de algoritmos iterativos
- Matriz de transiciÃ³n P tiene eigenvalue dominante relacionado con tasa de convergencia

```python
A = np.array([[2, 1], [1, 2]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print("Eigenvalues:", eigenvalues)    # [3. 1.]
print("Eigenvectors:\n", eigenvectors)
```

### 6.2 DescomposiciÃ³n SVD

Singular Value Decomposition descompone matriz en:
```
A = U Â· Î£ Â· Váµ€
```

**En Deep RL**: Comprimir representaciones, anÃ¡lisis de redes neuronales.

```python
A = np.array([[1, 2], [3, 4], [5, 6]])
U, S, VT = np.linalg.svd(A)
print("Valores singulares:", S)
```

---

## 7. Cheat Sheet: Operaciones Esenciales

| OperaciÃ³n | SÃ­mbolo | NumPy | Uso en RL |
|-----------|---------|-------|-----------|
| Suma vectores | vâ‚ + vâ‚‚ | `v1 + v2` | Actualizar estados |
| Producto escalar | cÂ·v | `c * v` | Descuento: Î³Â·V |
| Producto punto | vâ‚Â·vâ‚‚ | `np.dot(v1,v2)` | Q = wÂ·Ï† |
| Norma | \|\|v\|\| | `np.linalg.norm(v)` | Magnitud gradiente |
| Mult. matriz-vector | AÂ·v | `A @ v` | V_new = PÂ·V |
| Mult. matriz-matriz | AÂ·B | `A @ B` | ComposiciÃ³n |
| Transpuesta | Aáµ€ | `A.T` | Cambiar dimensiones |
| Inversa | Aâ»Â¹ | `np.linalg.inv(A)` | Bellman directo |
| Identidad | I | `np.eye(n)` | (I-Î³P) |
| Max | max(v) | `np.max(v)` | Bellman optimality |
| Argmax | argmax(v) | `np.argmax(v)` | PolÃ­tica greedy |

---

## 8. Recursos Adicionales

### Videos (EspaÃ±ol)
- [Khan Academy - Ãlgebra Lineal](https://es.khanacademy.org/math/linear-algebra)
- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) (inglÃ©s, subtÃ­tulos)

### Libros
- Strang, Gilbert. "Introduction to Linear Algebra"
- [Deep Learning Book - Math Appendix](https://www.deeplearningbook.org/contents/linear_algebra.html)

### PrÃ¡ctica
- [NumPy Quickstart](https://numpy.org/doc/stable/user/quickstart.html)
- [NumPy for MATLAB users](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html)

---

## 9. AutoevaluaciÃ³n

Â¿Puedes responder estas preguntas?

- [ ] Â¿QuÃ© es un producto punto y cÃ³mo se calcula?
- [ ] Â¿CÃ³mo se multiplica una matriz por un vector?
- [ ] Â¿QuÃ© hace np.argmax()?
- [ ] Â¿Para quÃ© sirve la matriz identidad?
- [ ] Â¿CÃ³mo se escribe la ecuaciÃ³n de Bellman en forma matricial?

Si respondiste todo, Â¡excelente! ContinÃºa con [CÃ¡lculo BÃ¡sico](03_calculo_basico.md).

---

## PrÃ³ximos Pasos

1. **[CÃ¡lculo BÃ¡sico](03_calculo_basico.md)** - Derivadas y gradientes para Deep RL
2. **[Python y NumPy](04_python_numpy.md)** - ProgramaciÃ³n prÃ¡ctica
3. **[OptimizaciÃ³n](05_conceptos_optimizacion.md)** - Gradient descent y mÃ¡s

Â¡Sigue adelante! ğŸš€
