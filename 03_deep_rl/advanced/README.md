# Algoritmos Avanzados de Deep RL

Este directorio contiene implementaciones de algoritmos avanzados de Deep Reinforcement Learning que representan el **estado del arte** en control continuo y aprendizaje por refuerzo.

## üìÅ Archivos Implementados

### 1. `ppo.py` - Proximal Policy Optimization ‚≠ê RECOMENDADO

Implementaci√≥n de PPO (Schulman et al., 2017), el algoritmo on-policy m√°s popular y confiable.

**¬øPor qu√© PPO?**
- Simple y robusto - f√°cil de implementar y tunear
- Sample efficient - reutiliza datos con m√∫ltiples epochs
- Stable - clipping previene updates demasiado grandes
- Versatile - funciona bien en diversos ambientes
- SOTA - usado en OpenAI, DeepMind, etc.

**Caracter√≠sticas clave:**
- **Clipped Surrogate Objective**: Limita cambios de pol√≠tica
  ```
  L^CLIP(Œ∏) = E[min(r_t(Œ∏)A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t)]
  ```
- **GAE (Generalized Advantage Estimation)**: Reduce varianza
- **Mini-batch Training**: M√∫ltiples epochs sobre datos recolectados
- **Value Function Clipping**: Opcional, mejora estabilidad
- **Entropy Bonus**: Fomenta exploraci√≥n
- **Soporte Discrete/Continuous**: Ambos tipos de acciones

**Componentes:**
- `ActorNetwork`: Pol√≠tica œÄ(a|s)
- `CriticNetwork`: Funci√≥n de valor V(s)
- `PPOAgent`: Agente completo con clipping
- Funciones: `train()`, `evaluate_agent()`, `plot_training_results()`

**Hiperpar√°metros importantes:**
- `epsilon_clip`: 0.2 (ratio clipping)
- `value_clip`: 0.2 o None (value function clipping)
- `gae_lambda`: 0.95 (GAE par√°metro)
- `n_epochs`: 10 (epochs de optimizaci√≥n)
- `batch_size`: 64 (tama√±o de mini-batch)
- `update_interval`: 2048 (steps antes de actualizar)

**Cu√°ndo usar PPO:**
- Primera opci√≥n para la mayor√≠a de problemas
- Rob√≥tica, juegos, problemas diversos
- Cuando necesitas algo confiable y estable
- Para aprendizaje con datos limitados

---

### 2. `ddpg.py` - Deep Deterministic Policy Gradient

Implementaci√≥n de DDPG (Lillicrap et al., 2016), un actor-critic off-policy para control continuo.

**Concepto:** Extiende DQN a espacios de acci√≥n continuos usando una pol√≠tica determinista.

**Caracter√≠sticas clave:**
- **Deterministic Policy**: Œº(s) mapea estados a acciones
- **Actor-Critic**: Combina policy gradient con Q-learning
- **Experience Replay**: Buffer para sample efficiency
- **Target Networks**: Para actor y critic (estabilidad)
- **Ornstein-Uhlenbeck Noise**: Exploraci√≥n temporalmente correlacionada
- **Gaussian Noise**: Alternativa m√°s simple

**Arquitectura:**
```
Actor:  s ‚Üí Œº_Œ∏(s) ‚Üí a
Critic: (s,a) ‚Üí Q_œÜ(s,a) ‚Üí Q-value
```

**Componentes:**
- `Actor`: Pol√≠tica determinista
- `Critic`: Q-function Q(s,a)
- `OrnsteinUhlenbeckNoise`: Ruido para exploraci√≥n
- `ReplayBuffer`: Buffer de experiencias
- `DDPGAgent`: Agente completo
- Funciones: `train()`, `evaluate_agent()`, `plot_training_results()`

**Hiperpar√°metros importantes:**
- `actor_lr`: 1e-4
- `critic_lr`: 1e-3
- `tau`: 0.001 (soft update)
- `buffer_size`: 100000
- `batch_size`: 64
- `noise_type`: 'ou' o 'gaussian'
- `noise_std`: 0.2

**Cu√°ndo usar DDPG:**
- Control continuo b√°sico
- Prototipado r√°pido
- Base para aprender algoritmos m√°s avanzados
- Cuando TD3/SAC son overkill

**Limitaciones:**
- Sensible a hiperpar√°metros
- Puede ser inestable
- Superado por TD3 y SAC

---

### 3. `td3.py` - Twin Delayed DDPG ‚≠ê RECOMENDADO

Implementaci√≥n de TD3 (Fujimoto et al., 2018), una mejora significativa sobre DDPG.

**Tres innovaciones clave sobre DDPG:**

1. **Twin Q-networks (Clipped Double Q-learning)**:
   ```python
   Q_target = min(Q1_target, Q2_target)
   ```
   Reduce sobreestimaci√≥n de Q-values

2. **Delayed Policy Updates**:
   ```python
   if step % policy_delay == 0:
       update_actor()
   ```
   Actualiza actor menos frecuentemente que critic

3. **Target Policy Smoothing**:
   ```python
   noise = clip(N(0, œÉ), -c, c)
   a' = clip(Œº(s') + noise, -max, max)
   ```
   Suaviza superficie de Q, m√°s robusto

**Caracter√≠sticas:**
- Todas las de DDPG
- Twin critics para doble robustez
- Policy updates retardados
- Smoothing noise en targets
- M√°s estable que DDPG
- Mismo tipo de acciones (continuous)

**Componentes:**
- `Actor`: Pol√≠tica determinista
- `Critic`: Twin Q-networks (Q1, Q2)
- `ReplayBuffer`: Buffer compartido
- `TD3Agent`: Agente con tres mejoras
- Funciones: `train()`, `evaluate_agent()`, `plot_training_results()`

**Hiperpar√°metros importantes:**
- `actor_lr`: 3e-4
- `critic_lr`: 3e-4
- `tau`: 0.005 (soft update)
- `policy_noise`: 0.2 (target smoothing)
- `noise_clip`: 0.5 (l√≠mite de ruido)
- `policy_delay`: 2 (delayed updates)
- `batch_size`: 256

**Cu√°ndo usar TD3:**
- Control continuo con mejor estabilidad que DDPG
- Cuando DDPG es inestable
- Problemas que requieren determinismo
- Alternativa a SAC m√°s simple

**Ventajas sobre DDPG:**
- Mucho m√°s estable
- Menor sobreestimaci√≥n
- Mejor rendimiento general
- M√≠nimo overhead computacional

---

### 4. `sac.py` - Soft Actor-Critic ‚≠ê‚≠ê ESTADO DEL ARTE

Implementaci√≥n de SAC (Haarnoja et al., 2018-2019), el estado del arte para control continuo.

**Concepto:** Maximum Entropy RL - maximiza recompensas **y** entrop√≠a de la pol√≠tica.

```
J(œÄ) = E[Œ£ r_t + Œ± H(œÄ(¬∑|s_t))]
```

**Cinco caracter√≠sticas clave:**

1. **Maximum Entropy Framework**:
   - Fomenta exploraci√≥n naturalmente
   - Aprende pol√≠ticas robustas y multimodales
   - Balance entre exploraci√≥n y explotaci√≥n

2. **Automatic Temperature Tuning**:
   - Œ± se ajusta autom√°ticamente
   - No requiere tuning manual
   - Mantiene entrop√≠a objetivo

3. **Twin Q-networks**:
   - Como TD3, reduce sobreestimaci√≥n
   - Dos critics independientes

4. **Stochastic Policy**:
   - Gaussian policy: œÄ(a|s) = N(Œº(s), œÉ(s))
   - Reparameterization trick para gradientes
   - M√°s robusta que deterministic

5. **Off-policy**:
   - Sample efficient con replay buffer
   - Reutiliza experiencias pasadas

**Arquitectura:**
```
Actor:  s ‚Üí œÄ_Œ∏(a|s) ~ N(Œº, œÉ) ‚Üí a (stochastic)
Critic: (s,a) ‚Üí Q_œÜ(s,a) ‚Üí Q-value (twin)
Alpha:  Œ± adaptativo (learnable parameter)
```

**Componentes:**
- `GaussianActor`: Pol√≠tica estoc√°stica con reparameterization
- `Critic`: Twin Q-networks
- `ReplayBuffer`: Buffer de experiencias
- `SACAgent`: Agente completo con auto-tuning
- Funciones: `train()`, `evaluate_agent()`, `plot_training_results()`

**Hiperpar√°metros importantes:**
- `actor_lr`: 3e-4
- `critic_lr`: 3e-4
- `alpha_lr`: 3e-4 (si auto_tune=True)
- `tau`: 0.005
- `auto_tune`: True (recomendado)
- `target_entropy`: -action_dim (heur√≠stica)
- `batch_size`: 256
- `updates_per_step`: 1

**Cu√°ndo usar SAC:**
- **Primera opci√≥n** para control continuo
- Rob√≥tica real
- Problemas complejos
- Cuando sample efficiency importa
- Cuando exploraci√≥n es cr√≠tica

**Ventajas:**
- Estado del arte en continuous control
- Muy robusto y estable
- Explora mejor que TD3/DDPG
- Auto-tuning reduce hyperparameter search
- Aprende m√∫ltiples modos de soluci√≥n

---

## üìä Comparaci√≥n de Algoritmos

### Tabla Comparativa

| Caracter√≠stica | PPO | DDPG | TD3 | SAC |
|----------------|-----|------|-----|-----|
| **Tipo** | On-policy | Off-policy | Off-policy | Off-policy |
| **Pol√≠tica** | Stochastic | Deterministic | Deterministic | Stochastic |
| **Acciones** | Disc/Cont | Continuous | Continuous | Continuous |
| **Sample Efficiency** | Media | Alta | Alta | Alta |
| **Estabilidad** | Alta | Media | Alta | Muy Alta |
| **Simplicidad** | Media | Alta | Media | Media |
| **Exploraci√≥n** | Entropy bonus | Noise | Noise | Maximum Entropy |
| **Hyperparameters** | Medio | Alto | Medio | Bajo (auto-tune) |
| **Uso en Producci√≥n** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Performance Esperada

**Pendulum-v1** (200 episodios):
- PPO: ~-200 reward
- DDPG: ~-200 reward
- TD3: ~-150 reward
- SAC: ~-150 reward (m√°s r√°pido)

**MountainCarContinuous-v0** (500 episodios):
- PPO: ~90 reward
- DDPG: ~85 reward (inestable)
- TD3: ~90 reward (estable)
- SAC: ~90 reward (m√°s consistente)

---

## üöÄ Gu√≠a de Uso

### Quick Start

```python
import gymnasium as gym
from advanced import PPOAgent, TD3Agent, SACAgent

# 1. PPO para CartPole (discreto)
env = gym.make('CartPole-v1')
agent = PPOAgent(
    state_dim=4,
    action_dim=2,
    continuous=False
)
agent.train(env, n_episodes=300)

# 2. TD3 para Pendulum (continuo)
env = gym.make('Pendulum-v1')
agent = TD3Agent(
    state_dim=3,
    action_dim=1,
    max_action=2.0
)
agent.train(env, n_episodes=200)

# 3. SAC para control continuo (SOTA)
env = gym.make('Pendulum-v1')
agent = SACAgent(
    state_dim=3,
    action_dim=1,
    auto_tune=True
)
agent.train(env, n_episodes=150)
```

### Workflow Completo

```python
import gymnasium as gym
from advanced.sac import SACAgent, evaluate_agent, plot_training_results

# 1. Crear ambiente
env = gym.make('Pendulum-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]

# 2. Crear agente
agent = SACAgent(
    state_dim=state_dim,
    action_dim=action_dim,
    auto_tune=True,
    hidden_dims=[256, 256]
)

# 3. Entrenar
history = agent.train(
    env=env,
    n_episodes=200,
    warmup_steps=1000,
    print_every=10,
    save_path='sac_pendulum.pth'
)

# 4. Visualizar
plot_training_results(history, 'training.png')

# 5. Evaluar
mean_reward, std_reward = evaluate_agent(agent, env, n_episodes=50)
print(f"Reward: {mean_reward:.2f} ¬± {std_reward:.2f}")

# 6. Guardar/Cargar
agent.save('final_model.pth')
agent.load('final_model.pth')

env.close()
```

---

## üéØ Decisi√≥n: ¬øQu√© Algoritmo Usar?

### Diagrama de Decisi√≥n

```
¬øAcciones continuas o discretas?
‚îú‚îÄ Discretas ‚Üí PPO
‚îî‚îÄ Continuas ‚Üí ¬øNecesitas SOTA?
              ‚îú‚îÄ S√≠ ‚Üí SAC (primera opci√≥n)
              ‚îî‚îÄ No ‚Üí ¬øNecesitas determinismo?
                     ‚îú‚îÄ S√≠ ‚Üí TD3
                     ‚îî‚îÄ No ‚Üí PPO o SAC
```

### Recomendaciones por Caso de Uso

**Rob√≥tica Real:**
- Primera opci√≥n: **SAC** (robusto, explora bien, stochastic)
- Alternativa: **PPO** (si on-policy es aceptable)

**Juegos:**
- Acciones discretas: **PPO**
- Acciones continuas: **SAC** o **TD3**

**Simulaci√≥n / Investigaci√≥n:**
- Baseline: **PPO** (f√°cil de implementar)
- SOTA: **SAC** (mejores resultados)
- Alternativa: **TD3** (buen balance)

**Prototipado R√°pido:**
- **DDPG** (simple, r√°pido de entrenar)
- **PPO** (si necesitas estabilidad)

**Control de Procesos Industriales:**
- **TD3** o **SAC** (continuos, robustos)

---

## üí° Tips de Entrenamiento

### 1. Hyperparameters Generales

**Learning Rates:**
- Actor: 3e-4 (PPO, TD3, SAC)
- Critic: 3e-4 o 1e-3 (TD3, SAC)
- Empieza con estos, ajusta si inestable

**Network Size:**
- Default: [256, 256] funciona bien
- Problemas simples: [64, 64]
- Problemas complejos: [400, 300] o [512, 512]

**Batch Size:**
- PPO: 64 (on-policy, menos datos)
- DDPG/TD3/SAC: 256 (off-policy, m√°s estable)

**Buffer Size:**
- Problemas simples: 100K
- Problemas complejos: 1M

### 2. Debugging

**Si entrenamiento no converge:**
1. Verifica que ambiente funcione correctamente
2. Reduce learning rate (x0.3)
3. Aumenta warmup steps
4. Revisa que recompensas est√©n normalizadas

**Si es inestable:**
1. Aumenta batch size
2. Reduce learning rate
3. Activa gradient clipping (ya incluido)
4. Para PPO: reduce epsilon_clip

**Si explora poco:**
1. PPO: aumenta entropy_coef
2. SAC: verifica que auto_tune=True
3. DDPG/TD3: aumenta exploration_noise

### 3. Mejores Pr√°cticas

‚úÖ **Hacer:**
- Usa warmup period para llenar buffer
- Normaliza observaciones si tienen escalas muy diferentes
- Normaliza rewards si son muy variables
- Guarda checkpoints regularmente
- Eval√∫a sin ruido/deterministic
- Usa tensorboard para monitorear m√©tricas

‚ùå **Evitar:**
- Cambiar muchos hyperparameters a la vez
- Entrenar sin warmup (off-policy)
- Ignorar warnings de NaN/Inf
- Usar batch size muy peque√±o (off-policy)
- Updates demasiado frecuentes (PPO)

---

## üìö Referencias

### Papers Originales

1. **PPO**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
   Schulman et al., 2017

2. **DDPG**: [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971)
   Lillicrap et al., 2016

3. **TD3**: [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)
   Fujimoto et al., 2018

4. **SAC**: [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905)
   Haarnoja et al., 2019

### Recursos Adicionales

- **Spinning Up in Deep RL** (OpenAI): Excelente tutorial
- **Stable-Baselines3**: Implementaciones de referencia
- **CleanRL**: Implementaciones simples y limpias
- **RLlib** (Ray): Para entrenamiento distribuido

---

## üß™ Testing

Para verificar que las implementaciones funcionan:

```bash
# Test PPO
python 03_deep_rl/advanced/ppo.py

# Test DDPG
python 03_deep_rl/advanced/ddpg.py

# Test TD3
python 03_deep_rl/advanced/td3.py

# Test SAC
python 03_deep_rl/advanced/sac.py
```

Cada script incluye ejemplos completos en su funci√≥n `main()`.

---

## üìù Notas Finales

Estos algoritmos son **production-ready** y siguen las mejores pr√°cticas:

‚úÖ Type hints completos
‚úÖ Docstrings en espa√±ol
‚úÖ Gradient clipping
‚úÖ Soft/hard updates
‚úÖ Save/load functionality
‚úÖ Plotting utilities
‚úÖ Evaluation functions
‚úÖ Comprehensive examples

**Total:** ~3700 l√≠neas de c√≥digo de calidad profesional.

---

**Autor:** MARK-126
**Versi√≥n:** 1.0.0
**√öltima actualizaci√≥n:** 2025
