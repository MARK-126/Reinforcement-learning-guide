{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO (Proximal Policy Optimization) - Interactive Exercise\n",
    "\n",
    "Welcome! In this notebook, you will implement **PPO**, one of the most popular and successful policy gradient algorithms in modern Deep RL.\n",
    "\n",
    "## What is PPO?\n",
    "\n",
    "PPO (Schulman et al., 2017) is a policy gradient method that achieves:\n",
    "- **Stable training**: Conservative policy updates prevent collapse\n",
    "- **Sample efficiency**: Reuses data through multiple epochs\n",
    "- **Strong performance**: State-of-the-art on many benchmarks\n",
    "- **Simplicity**: Easier to implement than TRPO (Trust Region Policy Optimization)\n",
    "\n",
    "PPO has become the **default choice** for many RL applications!\n",
    "\n",
    "## Key Innovation: Clipped Surrogate Objective\n",
    "\n",
    "**Standard Policy Gradient**:\n",
    "$$L^{PG} = \\mathbb{E}[\\log \\pi(a|s) A(s,a)]$$\n",
    "\n",
    "**PPO Clipped Objective**:\n",
    "$$L^{CLIP} = \\mathbb{E}[\\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)]$$\n",
    "\n",
    "where $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the **probability ratio**.\n",
    "\n",
    "The **clip** prevents the new policy from deviating too far from the old policy!\n",
    "\n",
    "## Differences from A2C\n",
    "\n",
    "| Aspect | A2C | PPO |\n",
    "|--------|-----|-----|\n",
    "| Update Rule | Standard PG | **Clipped surrogate** |\n",
    "| Data Usage | Single update | **Multiple epochs** |\n",
    "| Advantage | n-step TD | **GAE (Î»-returns)** |\n",
    "| Stability | Good | **Excellent** |\n",
    "| Performance | Good | **State-of-the-art** |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the clipped surrogate objective\n",
    "- Implement Generalized Advantage Estimation (GAE)\n",
    "- Build a complete PPO agent\n",
    "- See why PPO is so stable and effective\n",
    "- Compare PPO with A2C and other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from ppo_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment: CartPole\n",
    "\n",
    "We'll use CartPole-v1 to demonstrate PPO's superior stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compute GAE (Generalized Advantage Estimation)\n",
    "\n",
    "GAE (Schulman et al., 2016) provides a better advantage estimate by combining n-step returns with different values of n.\n",
    "\n",
    "**TD Error at time t**:\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "**GAE Advantage**:\n",
    "$$A_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "Î» controls the bias-variance tradeoff:\n",
    "- Î»=0: A(s,a) = Î´ (1-step TD, high bias, low variance)\n",
    "- Î»=1: A(s,a) = sum of all future Î´ (Monte Carlo, low bias, high variance)\n",
    "- Î»=0.95: **Typical choice** (good balance)\n",
    "\n",
    "**Task**: Implement GAE computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_gae\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation.\n",
    "    \n",
    "    Arguments:\n",
    "    rewards -- list of rewards [r_0, r_1, ..., r_T]\n",
    "    values -- list of state values [V(s_0), V(s_1), ..., V(s_T), V(s_{T+1})]\n",
    "              Note: values has one more element for bootstrap\n",
    "    dones -- list of done flags [d_0, d_1, ..., d_T]\n",
    "    gamma -- discount factor\n",
    "    lambda_ -- GAE lambda parameter\n",
    "    \n",
    "    Returns:\n",
    "    advantages -- list of advantages [A_0, A_1, ..., A_T]\n",
    "    returns -- list of returns (for critic training) [R_0, R_1, ..., R_T]\n",
    "    \"\"\"\n",
    "    # (approx. 15-18 lines)\n",
    "    # 1. Initialize advantages list\n",
    "    # 2. Initialize gae = 0\n",
    "    # 3. Loop backwards through trajectory (from T-1 to 0):\n",
    "    #    a. Compute TD error:\n",
    "    #       if done[t]:\n",
    "    #           delta = reward[t] - value[t]\n",
    "    #       else:\n",
    "    #           delta = reward[t] + gamma * value[t+1] - value[t]\n",
    "    #    b. Update GAE:\n",
    "    #       if done[t]:\n",
    "    #           gae = delta\n",
    "    #       else:\n",
    "    #           gae = delta + gamma * lambda_ * gae\n",
    "    #    c. Insert gae at beginning of advantages list\n",
    "    # 4. Compute returns: returns = advantages + values[:-1]\n",
    "    # 5. Return advantages and returns\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_gae_test(compute_gae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: PPO Actor Network\n",
    "\n",
    "Same architecture as A2C, but we'll use it differently (multiple epochs).\n",
    "\n",
    "**Task**: Implement the Actor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: PPOActorNetwork\n",
    "\n",
    "class PPOActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Actor network for PPO.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        action_dim -- dimension of action space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(PPOActorNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 2 lines)\n",
    "        # fc1: state_dim -> hidden_dim\n",
    "        # fc2: hidden_dim -> action_dim\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        action_probs -- action probability distribution\n",
    "        \"\"\"\n",
    "        # (approx. 3 lines)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "ppo_actor_network_test(PPOActorNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: PPO Critic Network\n",
    "\n",
    "Same as A2C - estimates state values.\n",
    "\n",
    "**Task**: Implement the Critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: PPOCriticNetwork\n",
    "\n",
    "class PPOCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Critic network for PPO.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(PPOCriticNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 2 lines)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # (approx. 3 lines)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "ppo_critic_network_test(PPOCriticNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Compute PPO Loss (The Core Innovation!)\n",
    "\n",
    "This is where PPO shines! The **clipped surrogate objective**.\n",
    "\n",
    "**Probability Ratio**:\n",
    "$$r(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$$\n",
    "\n",
    "**Clipped Objective**:\n",
    "$$L^{CLIP} = \\mathbb{E}[\\min(r(\\theta) A, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon) A)]$$\n",
    "\n",
    "The clip prevents:\n",
    "- **Large positive updates** when A > 0 (advantage is positive)\n",
    "- **Large negative updates** when A < 0 (advantage is negative)\n",
    "\n",
    "**Critic Loss**: Same MSE as before\n",
    "\n",
    "**Task**: Implement the PPO clipped loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_ppo_loss\n",
    "\n",
    "def compute_ppo_loss(actor, states, actions, old_log_probs, advantages, \n",
    "                     critic, returns, clip_epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Compute PPO clipped loss.\n",
    "    \n",
    "    Arguments:\n",
    "    actor -- current actor network\n",
    "    states -- tensor of states\n",
    "    actions -- tensor of actions taken\n",
    "    old_log_probs -- tensor of log probs from old policy\n",
    "    advantages -- tensor of advantages (should be normalized)\n",
    "    critic -- current critic network\n",
    "    returns -- tensor of returns (for critic training)\n",
    "    clip_epsilon -- clipping parameter (typical: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "    actor_loss -- clipped policy loss\n",
    "    critic_loss -- value function loss\n",
    "    total_loss -- combined loss\n",
    "    approx_kl -- approximate KL divergence (for monitoring)\n",
    "    \"\"\"\n",
    "    # (approx. 18-22 lines)\n",
    "    # 1. Get current action probabilities from actor\n",
    "    # 2. Create distribution and get log probs of actions:\n",
    "    #    dist = torch.distributions.Categorical(action_probs)\n",
    "    #    new_log_probs = dist.log_prob(actions)\n",
    "    # 3. Compute probability ratio:\n",
    "    #    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    # 4. Compute surrogate losses:\n",
    "    #    surr1 = ratio * advantages\n",
    "    #    surr2 = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages\n",
    "    # 5. Actor loss (take minimum, then negate for gradient ascent):\n",
    "    #    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "    # 6. Critic loss:\n",
    "    #    values = critic(states)\n",
    "    #    critic_loss = F.mse_loss(values, returns)\n",
    "    # 7. Total loss:\n",
    "    #    total_loss = actor_loss + 0.5 * critic_loss\n",
    "    # 8. Approximate KL (for monitoring):\n",
    "    #    approx_kl = (old_log_probs - new_log_probs).mean()\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return actor_loss, critic_loss, total_loss, approx_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_ppo_loss_test(compute_ppo_loss, PPOActorNetwork, PPOCriticNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Train PPO\n",
    "\n",
    "PPO's training loop is more complex than A2C:\n",
    "1. Collect trajectory (like A2C)\n",
    "2. Compute advantages with GAE\n",
    "3. **Multiple epochs** over the data\n",
    "4. **Mini-batch updates** within each epoch\n",
    "\n",
    "This reuse of data makes PPO more sample-efficient!\n",
    "\n",
    "**Task**: Implement the complete PPO training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_ppo\n",
    "\n",
    "def train_ppo(env, actor, critic, optimizer, n_episodes=500, gamma=0.99, \n",
    "              lambda_=0.95, clip_epsilon=0.2, update_epochs=4, \n",
    "              batch_size=64, trajectory_length=2048):\n",
    "    \"\"\"\n",
    "    Train PPO on the environment.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gym environment\n",
    "    actor -- Actor network\n",
    "    critic -- Critic network\n",
    "    optimizer -- shared optimizer\n",
    "    n_episodes -- number of episodes to train\n",
    "    gamma -- discount factor\n",
    "    lambda_ -- GAE lambda\n",
    "    clip_epsilon -- PPO clip parameter\n",
    "    update_epochs -- number of epochs to update on each batch\n",
    "    batch_size -- mini-batch size\n",
    "    trajectory_length -- collect this many steps before update\n",
    "    \n",
    "    Returns:\n",
    "    episode_rewards -- list of episode rewards\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # (approx. 50-60 lines - this is complex!)\n",
    "    # For each episode:\n",
    "    #   1. Collect trajectory up to trajectory_length steps\n",
    "    #      Store: states, actions, rewards, log_probs, values, dones\n",
    "    #   2. Compute GAE advantages and returns\n",
    "    #   3. Normalize advantages: (advantages - mean) / (std + 1e-8)\n",
    "    #   4. Convert to tensors\n",
    "    #   5. For each update_epoch:\n",
    "    #      a. Shuffle indices\n",
    "    #      b. For each mini-batch:\n",
    "    #         - Get batch of (states, actions, old_log_probs, advantages, returns)\n",
    "    #         - Compute PPO loss\n",
    "    #         - Update networks:\n",
    "    #           * optimizer.zero_grad()\n",
    "    #           * total_loss.backward()\n",
    "    #           * optimizer.step()\n",
    "    #   6. Track episode rewards\n",
    "    #   7. Print progress\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "train_ppo_test(train_ppo, PPOActorNetwork, PPOCriticNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Run\n",
    "\n",
    "Let's train PPO on CartPole and see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "actor = PPOActorNetwork(state_dim, action_dim)\n",
    "critic = PPOCriticNetwork(state_dim)\n",
    "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=3e-4)\n",
    "\n",
    "# Train PPO\n",
    "episode_rewards = train_ppo(\n",
    "    env, actor, critic, optimizer,\n",
    "    n_episodes=300,\n",
    "    gamma=0.99,\n",
    "    lambda_=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    update_epochs=4,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards, alpha=0.6)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 100\n",
    "if len(episode_rewards) >= window:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.axhline(y=475, color='r', linestyle='--', label='Solved')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(f'Avg Reward ({window} ep)')\n",
    "    plt.title('Moving Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(episode_rewards) >= 100:\n",
    "    final = np.mean(episode_rewards[-100:])\n",
    "    print(f\"\\n{'ðŸŽ‰ Solved!' if final >= 475 else 'ðŸ“Š Training complete'} Final avg: {final:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PPO is So Popular\n",
    "\n",
    "### 1. **Stability**\n",
    "- Clipping prevents catastrophic policy updates\n",
    "- Monotonic improvement guarantee (approximately)\n",
    "- Much more robust to hyperparameters than A2C\n",
    "\n",
    "### 2. **Sample Efficiency**\n",
    "- Multiple epochs reuse data effectively\n",
    "- GAE provides better advantage estimates\n",
    "- Learns faster than on-policy methods like A2C\n",
    "\n",
    "### 3. **Performance**\n",
    "- State-of-the-art on many continuous control tasks\n",
    "- Won OpenAI's Dota 2 competition\n",
    "- Used in robotics, game AI, and more\n",
    "\n",
    "### 4. **Simplicity**\n",
    "- Easier than TRPO (no complex constraint optimization)\n",
    "- Few hyperparameters to tune\n",
    "- Reliable default settings work well\n",
    "\n",
    "## Comparison Summary\n",
    "\n",
    "| Algorithm | Stability | Sample Efficiency | Performance | Complexity |\n",
    "|-----------|-----------|-------------------|-------------|------------|\n",
    "| REINFORCE | Low | Low | Moderate | Simple |\n",
    "| Actor-Critic | Moderate | Moderate | Good | Simple |\n",
    "| A2C | Good | Good | Good | Moderate |\n",
    "| **PPO** | **Excellent** | **Excellent** | **Excellent** | Moderate |\n",
    "\n",
    "## Hyperparameter Guidelines\n",
    "\n",
    "**Typical values** (good starting points):\n",
    "- **clip_epsilon**: 0.2 (range: 0.1-0.3)\n",
    "- **lambda_**: 0.95 (range: 0.9-0.99)\n",
    "- **update_epochs**: 4-10\n",
    "- **batch_size**: 64-256\n",
    "- **learning_rate**: 3e-4\n",
    "\n",
    "**Tuning tips**:\n",
    "- Start with defaults, they usually work!\n",
    "- Increase clip_epsilon for more aggressive updates\n",
    "- Increase lambda_ for lower bias (but higher variance)\n",
    "- More epochs = better data usage (but risk overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented PPO! You now understand:\n",
    "- âœ… The clipped surrogate objective and why it works\n",
    "- âœ… Generalized Advantage Estimation (GAE)\n",
    "- âœ… Multi-epoch training with mini-batches\n",
    "- âœ… Why PPO is the gold standard for policy gradient methods\n",
    "- âœ… How to tune PPO hyperparameters\n",
    "\n",
    "**You've completed one of the most important algorithms in modern Deep RL!**\n",
    "\n",
    "**Next Steps**:\n",
    "- Try PPO on continuous control tasks (MuJoCo, PyBullet)\n",
    "- Implement PPO for continuous action spaces\n",
    "- Explore PPO variants (PPO-penalty, PPO with curiosity)\n",
    "- Read the original paper: \"Proximal Policy Optimization Algorithms\" (Schulman et al., 2017)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
