{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic - Interactive Exercise\n",
    "\n",
    "Welcome! In this notebook, you will implement **Actor-Critic**, a powerful policy gradient method that combines the best of both worlds.\n",
    "\n",
    "## What is Actor-Critic?\n",
    "\n",
    "Actor-Critic methods use two neural networks:\n",
    "- **Actor**: Learns the policy œÄ(a|s) - what action to take\n",
    "- **Critic**: Learns the value function V(s) - how good is the current state\n",
    "\n",
    "The critic helps reduce variance in the actor's gradient updates by providing a **baseline**.\n",
    "\n",
    "## Key Differences from REINFORCE\n",
    "\n",
    "| Aspect | REINFORCE | Actor-Critic |\n",
    "|--------|-----------|---------------|\n",
    "| Learning | Monte Carlo (full episodes) | Temporal Difference (step-by-step) |\n",
    "| Baseline | Fixed or moving average | Learned value function V(s) |\n",
    "| Variance | High variance | Lower variance |\n",
    "| Update Frequency | End of episode | Every step (online) |\n",
    "| Sample Efficiency | Lower | Higher |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the Actor-Critic architecture\n",
    "- Implement both Actor and Critic networks\n",
    "- Compute advantages using TD error\n",
    "- Combine policy and value losses\n",
    "- Compare Actor-Critic with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from actor_critic_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment: CartPole\n",
    "\n",
    "We'll use CartPole-v1, where the goal is to balance a pole on a moving cart.\n",
    "\n",
    "- **State**: [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "- **Actions**: 0 (push left), 1 (push right)\n",
    "- **Reward**: +1 for each timestep the pole stays upright\n",
    "- **Success**: Average reward > 475 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Actor Network\n",
    "\n",
    "The **Actor** network outputs action probabilities. It's similar to REINFORCE's PolicyNetwork.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input (state) ‚Üí FC1 (128) ‚Üí ReLU ‚Üí FC2 (action_dim) ‚Üí Softmax ‚Üí Action Probabilities\n",
    "```\n",
    "\n",
    "**Task**: Implement the Actor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ActorNetwork\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Actor network that outputs action probabilities.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        action_dim -- dimension of action space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 2 lines)\n",
    "        # Define two fully connected layers:\n",
    "        # fc1: state_dim -> hidden_dim\n",
    "        # fc2: hidden_dim -> action_dim\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass to get action probabilities.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        action_probs -- probability distribution over actions\n",
    "        \"\"\"\n",
    "        # (approx. 3 lines)\n",
    "        # 1. Pass through fc1 and apply ReLU\n",
    "        # 2. Pass through fc2\n",
    "        # 3. Apply softmax to get probabilities\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "actor_network_test(ActorNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Critic Network\n",
    "\n",
    "The **Critic** network estimates the value function V(s). It helps the actor by providing a baseline.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input (state) ‚Üí FC1 (128) ‚Üí ReLU ‚Üí FC2 (1) ‚Üí State Value\n",
    "```\n",
    "\n",
    "Note: Output is a single value (not a probability distribution).\n",
    "\n",
    "**Task**: Implement the Critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: CriticNetwork\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Critic network that estimates state value V(s).\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 2 lines)\n",
    "        # Define two fully connected layers:\n",
    "        # fc1: state_dim -> hidden_dim\n",
    "        # fc2: hidden_dim -> 1 (single value output)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass to get state value.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        value -- estimated value of the state\n",
    "        \"\"\"\n",
    "        # (approx. 3 lines)\n",
    "        # 1. Pass through fc1 and apply ReLU\n",
    "        # 2. Pass through fc2 to get value\n",
    "        # 3. Squeeze to remove extra dimensions\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "critic_network_test(CriticNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Select Action\n",
    "\n",
    "Similar to REINFORCE, we sample actions from the policy distribution.\n",
    "\n",
    "**Task**: Implement action selection with log probability tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: select_action\n",
    "\n",
    "def select_action(actor, state):\n",
    "    \"\"\"\n",
    "    Select action from policy and compute log probability.\n",
    "    \n",
    "    Arguments:\n",
    "    actor -- Actor network\n",
    "    state -- current state (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "    action -- selected action (int)\n",
    "    log_prob -- log probability of the action\n",
    "    \"\"\"\n",
    "    # (approx. 5-6 lines)\n",
    "    # 1. Convert state to tensor\n",
    "    # 2. Get action probabilities from actor\n",
    "    # 3. Create categorical distribution\n",
    "    # 4. Sample action\n",
    "    # 5. Get log probability\n",
    "    # 6. Return action as int and log_prob\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "select_action_test(select_action, ActorNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Compute Actor-Critic Loss\n",
    "\n",
    "The Actor-Critic algorithm uses two losses:\n",
    "\n",
    "**Actor Loss** (Policy Gradient with advantage):\n",
    "$$L_{actor} = -\\log \\pi(a|s) \\cdot A(s,a)$$\n",
    "\n",
    "where the **advantage** is:\n",
    "$$A(s,a) = r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "This is the **TD error** Œ¥!\n",
    "\n",
    "**Critic Loss** (Mean Squared Error):\n",
    "$$L_{critic} = [r + \\gamma V(s') - V(s)]^2 = \\delta^2$$\n",
    "\n",
    "**Task**: Implement both losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_ac_loss\n",
    "\n",
    "def compute_ac_loss(log_prob, value, next_value, reward, done, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute Actor-Critic loss.\n",
    "    \n",
    "    Arguments:\n",
    "    log_prob -- log probability of action taken\n",
    "    value -- V(s) from critic\n",
    "    next_value -- V(s') from critic\n",
    "    reward -- reward received\n",
    "    done -- whether episode ended\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    actor_loss -- loss for actor\n",
    "    critic_loss -- loss for critic\n",
    "    \"\"\"\n",
    "    # (approx. 7-8 lines)\n",
    "    # 1. Compute TD target:\n",
    "    #    - If done: target = reward\n",
    "    #    - Else: target = reward + gamma * next_value\n",
    "    # 2. Compute advantage (TD error): delta = target - value\n",
    "    # 3. Detach advantage for actor (don't backprop through critic to actor)\n",
    "    # 4. Compute actor loss: -log_prob * advantage\n",
    "    # 5. Compute critic loss: delta^2 (or use F.mse_loss)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_ac_loss_test(compute_ac_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Train Actor-Critic\n",
    "\n",
    "Now let's combine everything into the training loop!\n",
    "\n",
    "**Algorithm** (per step):\n",
    "1. Select action using actor\n",
    "2. Take action, get reward and next state\n",
    "3. Compute V(s) and V(s') using critic\n",
    "4. Compute actor and critic losses\n",
    "5. Update both networks\n",
    "\n",
    "**Key difference from REINFORCE**: We update **every step**, not at the end of episodes!\n",
    "\n",
    "**Task**: Implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_actor_critic\n",
    "\n",
    "def train_actor_critic(env, actor, critic, actor_optimizer, critic_optimizer, \n",
    "                       n_episodes=500, gamma=0.99, max_steps=500):\n",
    "    \"\"\"\n",
    "    Train Actor-Critic on the environment.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gym environment\n",
    "    actor -- Actor network\n",
    "    critic -- Critic network\n",
    "    actor_optimizer -- optimizer for actor\n",
    "    critic_optimizer -- optimizer for critic\n",
    "    n_episodes -- number of episodes to train\n",
    "    gamma -- discount factor\n",
    "    max_steps -- max steps per episode\n",
    "    \n",
    "    Returns:\n",
    "    episode_rewards -- list of total rewards per episode\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # (approx. 25-30 lines)\n",
    "    # For each episode:\n",
    "    #   1. Reset environment, get initial state\n",
    "    #   2. total_reward = 0\n",
    "    #   3. For each step (up to max_steps):\n",
    "    #      a. Select action using select_action()\n",
    "    #      b. Take action: next_state, reward, done, _, _ = env.step(action)\n",
    "    #      c. Get value estimates: value = critic(state), next_value = critic(next_state)\n",
    "    #      d. Compute losses: compute_ac_loss(...)\n",
    "    #      e. Update actor:\n",
    "    #         - actor_optimizer.zero_grad()\n",
    "    #         - actor_loss.backward()\n",
    "    #         - actor_optimizer.step()\n",
    "    #      f. Update critic:\n",
    "    #         - critic_optimizer.zero_grad()\n",
    "    #         - critic_loss.backward()\n",
    "    #         - critic_optimizer.step()\n",
    "    #      g. Update state and total_reward\n",
    "    #      h. If done: break\n",
    "    #   4. Append total_reward to episode_rewards\n",
    "    #   5. Print progress every 50 episodes\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "train_actor_critic_test(train_actor_critic, ActorNetwork, CriticNetwork, select_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Run\n",
    "\n",
    "Let's train Actor-Critic on CartPole and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks and optimizers\n",
    "actor = ActorNetwork(state_dim, action_dim)\n",
    "critic = CriticNetwork(state_dim)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# Train\n",
    "episode_rewards = train_actor_critic(env, actor, critic, actor_optimizer, critic_optimizer, n_episodes=500)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards, alpha=0.6)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Actor-Critic Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 100\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(moving_avg)\n",
    "plt.axhline(y=475, color='r', linestyle='--', label='Solved threshold (475)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(f'Average Reward (last {window} episodes)')\n",
    "plt.title('Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if solved\n",
    "if len(moving_avg) > 0 and moving_avg[-1] >= 475:\n",
    "    print(f\"\\nüéâ Environment solved! Final average reward: {moving_avg[-1]:.2f}\")\n",
    "else:\n",
    "    print(f\"\\nüìä Training completed. Final average reward: {moving_avg[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Actor-Critic vs REINFORCE\n",
    "\n",
    "**Advantages of Actor-Critic**:\n",
    "- ‚úÖ **Lower variance**: Critic provides better baseline than simple average\n",
    "- ‚úÖ **Online learning**: Updates every step (don't need to wait for episode end)\n",
    "- ‚úÖ **Faster convergence**: More frequent updates lead to faster learning\n",
    "- ‚úÖ **Works for continuing tasks**: Doesn't require episodic structure\n",
    "\n",
    "**Disadvantages**:\n",
    "- ‚ùå **More complex**: Two networks instead of one\n",
    "- ‚ùå **Bias-variance tradeoff**: Bootstrapping (using V(s')) introduces bias\n",
    "- ‚ùå **Hyperparameter sensitivity**: Need to balance actor and critic learning rates\n",
    "\n",
    "**When to use**:\n",
    "- Use **Actor-Critic** for most practical applications (better sample efficiency)\n",
    "- Use **REINFORCE** when you want simplicity or unbiased estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Actor-Critic! You now understand:\n",
    "- ‚úÖ The Actor-Critic architecture (two networks working together)\n",
    "- ‚úÖ How the Critic reduces variance by providing a learned baseline\n",
    "- ‚úÖ TD error as the advantage function\n",
    "- ‚úÖ Online learning with step-by-step updates\n",
    "- ‚úÖ Differences from REINFORCE\n",
    "\n",
    "**Next Steps**: \n",
    "- Try **Advantage Actor-Critic (A2C)** with n-step returns\n",
    "- Explore **A3C** (Asynchronous Actor-Critic) for parallel training\n",
    "- Learn **PPO** (Proximal Policy Optimization) for more stable training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
