{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C (Advantage Actor-Critic) - Interactive Exercise\n",
    "\n",
    "Welcome! In this notebook, you will implement **A2C (Advantage Actor-Critic)**, an improved version of the basic Actor-Critic algorithm.\n",
    "\n",
    "## What is A2C?\n",
    "\n",
    "A2C is the **synchronous** version of A3C (Asynchronous Advantage Actor-Critic). It improves upon basic Actor-Critic by:\n",
    "- Using **n-step returns** instead of 1-step TD\n",
    "- Explicitly computing **advantages** (not just TD error)\n",
    "- Better **bias-variance tradeoff**\n",
    "\n",
    "## Key Differences from Actor-Critic\n",
    "\n",
    "| Aspect | Actor-Critic (1-step) | A2C |\n",
    "|--------|----------------------|-----|\n",
    "| Returns | 1-step TD: r + Î³V(s') | **n-step**: râ‚ + Î³râ‚‚ + ... + Î³â¿V(s_{t+n}) |\n",
    "| Advantage | TD error: Î´ = r + Î³V(s') - V(s) | **n-step advantage**: A(s,a) = R_n - V(s) |\n",
    "| Bias-Variance | Higher bias, lower variance | **Better tradeoff** |\n",
    "| Sample Efficiency | Lower | **Higher** |\n",
    "| Stability | Good | **Better** |\n",
    "\n",
    "## N-Step Returns\n",
    "\n",
    "The key innovation is using **n-step returns**:\n",
    "\n",
    "$$R_t^{(n)} = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^{n-1} r_{t+n-1} + \\gamma^n V(s_{t+n})$$\n",
    "\n",
    "This provides a better estimate than 1-step TD by looking further ahead!\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand n-step returns and why they help\n",
    "- Implement n-step advantage calculation\n",
    "- Build a proper A2C agent\n",
    "- Compare A2C with basic Actor-Critic\n",
    "- See the bias-variance tradeoff in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from a2c_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment: CartPole\n",
    "\n",
    "We'll use CartPole-v1 to compare A2C with basic Actor-Critic.\n",
    "\n",
    "- **State**: [position, velocity, angle, angular velocity]\n",
    "- **Actions**: 0 (left), 1 (right)\n",
    "- **Reward**: +1 per timestep\n",
    "- **Success**: Average reward > 475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compute N-Step Returns\n",
    "\n",
    "The core of A2C is computing **n-step returns**. Given a trajectory of experiences, we need to compute the discounted sum of rewards looking n steps ahead.\n",
    "\n",
    "**Formula**:\n",
    "$$R_t^{(n)} = \\sum_{i=0}^{n-1} \\gamma^i r_{t+i} + \\gamma^n V(s_{t+n})$$\n",
    "\n",
    "If the episode ends before n steps, we only sum up to the terminal state.\n",
    "\n",
    "**Task**: Implement n-step returns calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_n_step_returns\n",
    "\n",
    "def compute_n_step_returns(rewards, values, dones, gamma=0.99, n_steps=5):\n",
    "    \"\"\"\n",
    "    Compute n-step returns for a batch of experiences.\n",
    "    \n",
    "    Arguments:\n",
    "    rewards -- list of rewards [r_0, r_1, ..., r_T]\n",
    "    values -- list of state values [V(s_0), V(s_1), ..., V(s_T), V(s_{T+1})]\n",
    "              Note: values has one more element (bootstrap value)\n",
    "    dones -- list of done flags [d_0, d_1, ..., d_T]\n",
    "    gamma -- discount factor\n",
    "    n_steps -- number of steps to look ahead\n",
    "    \n",
    "    Returns:\n",
    "    returns -- list of n-step returns [R_0, R_1, ..., R_T]\n",
    "    \"\"\"\n",
    "    # (approx. 15-18 lines)\n",
    "    # 1. Initialize returns list\n",
    "    # 2. For each timestep t:\n",
    "    #    a. Initialize n_step_return = 0\n",
    "    #    b. For i in range(n_steps):\n",
    "    #       - Check if t+i is within bounds\n",
    "    #       - If done[t+i] is True:\n",
    "    #         * Add gamma^i * reward[t+i]\n",
    "    #         * Break (episode ended)\n",
    "    #       - Else:\n",
    "    #         * Add gamma^i * reward[t+i]\n",
    "    #       - If reached n_steps and not done:\n",
    "    #         * Add gamma^n * V(s_{t+n}) as bootstrap\n",
    "    #    c. Append n_step_return to returns\n",
    "    # 3. Return returns list\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_n_step_returns_test(compute_n_step_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Actor Network\n",
    "\n",
    "The Actor network is similar to basic Actor-Critic, but we'll prepare it for better integration with advantages.\n",
    "\n",
    "**Architecture**: Same as before - outputs action probabilities.\n",
    "\n",
    "**Task**: Implement the Actor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: A2CActorNetwork\n",
    "\n",
    "class A2CActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Actor network for A2C.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        action_dim -- dimension of action space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(A2CActorNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 2 lines)\n",
    "        # Define two fully connected layers:\n",
    "        # fc1: state_dim -> hidden_dim\n",
    "        # fc2: hidden_dim -> action_dim\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass to get action probabilities.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        action_probs -- probability distribution over actions\n",
    "        \"\"\"\n",
    "        # (approx. 3 lines)\n",
    "        # 1. Pass through fc1 with ReLU\n",
    "        # 2. Pass through fc2\n",
    "        # 3. Apply softmax for probabilities\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "a2c_actor_network_test(A2CActorNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Critic Network\n",
    "\n",
    "The Critic estimates state values V(s). Same as basic Actor-Critic.\n",
    "\n",
    "**Task**: Implement the Critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: A2CCriticNetwork\n",
    "\n",
    "class A2CCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Critic network for A2C.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(A2CCriticNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 2 lines)\n",
    "        # Define two fully connected layers:\n",
    "        # fc1: state_dim -> hidden_dim\n",
    "        # fc2: hidden_dim -> 1 (value output)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass to get state value.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        value -- estimated value of the state\n",
    "        \"\"\"\n",
    "        # (approx. 3 lines)\n",
    "        # 1. Pass through fc1 with ReLU\n",
    "        # 2. Pass through fc2\n",
    "        # 3. Squeeze to scalar\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "a2c_critic_network_test(A2CCriticNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Compute A2C Loss\n",
    "\n",
    "A2C uses **n-step advantages** instead of 1-step TD error:\n",
    "\n",
    "**Advantage**:\n",
    "$$A(s_t, a_t) = R_t^{(n)} - V(s_t)$$\n",
    "\n",
    "where $R_t^{(n)}$ is the n-step return we computed earlier.\n",
    "\n",
    "**Actor Loss** (Policy Gradient):\n",
    "$$L_{actor} = -\\log \\pi(a|s) \\cdot A(s,a)$$\n",
    "\n",
    "**Critic Loss** (MSE):\n",
    "$$L_{critic} = [R_t^{(n)} - V(s_t)]^2 = A(s,a)^2$$\n",
    "\n",
    "**Optional Entropy Bonus** (for exploration):\n",
    "$$L_{entropy} = -\\beta \\sum_a \\pi(a|s) \\log \\pi(a|s)$$\n",
    "\n",
    "**Task**: Implement the A2C loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_a2c_loss\n",
    "\n",
    "def compute_a2c_loss(log_probs, values, n_step_returns, entropy_coef=0.01):\n",
    "    \"\"\"\n",
    "    Compute A2C loss with entropy bonus.\n",
    "    \n",
    "    Arguments:\n",
    "    log_probs -- list of log probabilities of actions taken\n",
    "    values -- list of state values V(s) from critic\n",
    "    n_step_returns -- list of n-step returns R^(n)\n",
    "    entropy_coef -- coefficient for entropy bonus (encourages exploration)\n",
    "    \n",
    "    Returns:\n",
    "    actor_loss -- policy gradient loss\n",
    "    critic_loss -- value function loss\n",
    "    entropy_loss -- entropy bonus (for logging)\n",
    "    total_loss -- combined loss for backprop\n",
    "    \"\"\"\n",
    "    # (approx. 12-15 lines)\n",
    "    # 1. Stack tensors:\n",
    "    #    log_probs_tensor = torch.stack(log_probs)\n",
    "    #    values_tensor = torch.stack(values)\n",
    "    #    returns_tensor = torch.tensor(n_step_returns)\n",
    "    # 2. Compute advantages:\n",
    "    #    advantages = returns_tensor - values_tensor.detach()\n",
    "    #    Note: detach advantages for actor (don't backprop through critic to actor)\n",
    "    # 3. Actor loss:\n",
    "    #    actor_loss = -(log_probs_tensor * advantages).mean()\n",
    "    # 4. Critic loss:\n",
    "    #    critic_loss = F.mse_loss(values_tensor, returns_tensor)\n",
    "    # 5. Entropy (optional, for exploration):\n",
    "    #    Can compute from log_probs or just set to 0 for simplicity\n",
    "    # 6. Total loss:\n",
    "    #    total_loss = actor_loss + critic_loss\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return actor_loss, critic_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_a2c_loss_test(compute_a2c_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Train A2C\n",
    "\n",
    "Now let's put it all together! A2C training collects n-step trajectories and updates both networks.\n",
    "\n",
    "**Algorithm** (per episode):\n",
    "1. Collect trajectory of n steps (or until done)\n",
    "2. Compute n-step returns for all states\n",
    "3. Compute A2C loss\n",
    "4. Update both actor and critic\n",
    "5. Repeat from next state\n",
    "\n",
    "**Key difference from basic Actor-Critic**: We update every n steps, not every step!\n",
    "\n",
    "**Task**: Implement the A2C training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_a2c\n",
    "\n",
    "def train_a2c(env, actor, critic, optimizer, n_episodes=500, gamma=0.99, \n",
    "              n_steps=5, max_steps_per_episode=500):\n",
    "    \"\"\"\n",
    "    Train A2C on the environment.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gym environment\n",
    "    actor -- Actor network\n",
    "    critic -- Critic network\n",
    "    optimizer -- shared optimizer for both networks\n",
    "    n_episodes -- number of episodes to train\n",
    "    gamma -- discount factor\n",
    "    n_steps -- number of steps for n-step returns\n",
    "    max_steps_per_episode -- max steps per episode\n",
    "    \n",
    "    Returns:\n",
    "    episode_rewards -- list of total rewards per episode\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # (approx. 40-45 lines)\n",
    "    # For each episode:\n",
    "    #   1. Reset environment\n",
    "    #   2. Initialize trajectory buffers:\n",
    "    #      - states, actions, rewards, log_probs, values, dones\n",
    "    #   3. total_reward = 0\n",
    "    #   4. For each step:\n",
    "    #      a. Get action from actor (with log_prob)\n",
    "    #      b. Get value from critic\n",
    "    #      c. Take action in environment\n",
    "    #      d. Store experience in buffers\n",
    "    #      e. If collected n_steps experiences OR done:\n",
    "    #         - Get final bootstrap value (if not done)\n",
    "    #         - Compute n-step returns\n",
    "    #         - Compute A2C loss\n",
    "    #         - Update networks:\n",
    "    #           * optimizer.zero_grad()\n",
    "    #           * total_loss.backward()\n",
    "    #           * optimizer.step()\n",
    "    #         - Clear buffers (but keep last state if not done)\n",
    "    #      f. Update state and total_reward\n",
    "    #      g. If done: break\n",
    "    #   5. Append total_reward\n",
    "    #   6. Print progress every 50 episodes\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "train_a2c_test(train_a2c, A2CActorNetwork, A2CCriticNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Run\n",
    "\n",
    "Let's train A2C on CartPole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "actor = A2CActorNetwork(state_dim, action_dim)\n",
    "critic = A2CCriticNetwork(state_dim)\n",
    "\n",
    "# Use shared optimizer (common practice in A2C)\n",
    "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=1e-3)\n",
    "\n",
    "# Train\n",
    "episode_rewards = train_a2c(\n",
    "    env, actor, critic, optimizer,\n",
    "    n_episodes=500,\n",
    "    gamma=0.99,\n",
    "    n_steps=5\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards, alpha=0.6)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('A2C Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 100\n",
    "if len(episode_rewards) >= window:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.axhline(y=475, color='r', linestyle='--', label='Solved (475)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(f'Average Reward (last {window})')\n",
    "    plt.title('Moving Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(episode_rewards) >= 100:\n",
    "    final_avg = np.mean(episode_rewards[-100:])\n",
    "    if final_avg >= 475:\n",
    "        print(f\"\\nðŸŽ‰ Environment solved! Final average: {final_avg:.2f}\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ“Š Training completed. Final average: {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Actor-Critic vs A2C\n",
    "\n",
    "### Why A2C is Better\n",
    "\n",
    "**1. Bias-Variance Tradeoff**:\n",
    "- **1-step TD** (Actor-Critic): Lower variance, higher bias\n",
    "- **Monte Carlo** (REINFORCE): Higher variance, no bias\n",
    "- **n-step TD** (A2C): **Best of both worlds!**\n",
    "\n",
    "**2. Credit Assignment**:\n",
    "- A2C looks n steps ahead, better at assigning credit for actions\n",
    "- Especially useful in environments with delayed rewards\n",
    "\n",
    "**3. Sample Efficiency**:\n",
    "- More informative updates lead to faster learning\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **Basic Actor-Critic**: Simple environments, online learning priority\n",
    "- **A2C**: Most practical applications (better performance)\n",
    "- **A3C**: Parallel training with multiple workers (asynchronous)\n",
    "\n",
    "### Hyperparameter: n_steps\n",
    "\n",
    "- **Small n (2-5)**: Lower variance, higher bias, faster updates\n",
    "- **Large n (20-50)**: Higher variance, lower bias, closer to Monte Carlo\n",
    "- **Typical**: n=5 is a good default for many environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C: Asynchronous Version\n",
    "\n",
    "**A3C** is A2C with multiple parallel workers:\n",
    "- Each worker runs independently with its own environment\n",
    "- Workers asynchronously update shared network parameters\n",
    "- Better exploration due to diverse experiences\n",
    "- Faster training with parallelization\n",
    "\n",
    "**A2C vs A3C**:\n",
    "- A2C is synchronous (easier to implement, more stable)\n",
    "- A3C is asynchronous (faster with multi-core CPUs)\n",
    "- Modern practice often uses A2C with vectorized environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented A2C! You now understand:\n",
    "- âœ… N-step returns and why they improve learning\n",
    "- âœ… The bias-variance tradeoff in RL\n",
    "- âœ… How to compute proper advantages\n",
    "- âœ… Batch updates with trajectory collection\n",
    "- âœ… The relationship between A2C and A3C\n",
    "\n",
    "**Next Steps**:\n",
    "- Try **PPO** (Proximal Policy Optimization) for even more stable training\n",
    "- Experiment with different n_steps values\n",
    "- Implement **A3C** with multiprocessing for parallel training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
