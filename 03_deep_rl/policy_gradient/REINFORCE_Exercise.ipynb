{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm (Policy Gradient)\n",
    "\n",
    "Welcome to REINFORCE! This is the first **policy gradient** algorithm that directly optimizes the policy using neural networks. By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Understand the difference between value-based and policy-based methods\n",
    "* Implement a policy network that outputs action probabilities\n",
    "* Calculate the policy gradient using the REINFORCE algorithm\n",
    "* Train an agent using gradient ascent on expected return\n",
    "\n",
    "## Policy Gradient: A Paradigm Shift\n",
    "\n",
    "**Value-Based (Q-Learning, DQN)**:\n",
    "- Learn Q(s,a)\n",
    "- Extract policy: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "- Indirect optimization\n",
    "\n",
    "**Policy-Based (REINFORCE, PPO)**:\n",
    "- Learn $\\pi_\\theta(a|s)$ directly\n",
    "- Optimize policy parameters $\\theta$\n",
    "- Direct optimization\n",
    "\n",
    "## REINFORCE: Key Idea\n",
    "\n",
    "**Objective**: Maximize expected return\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "**Policy Gradient Theorem**:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t\\right]$$\n",
    "\n",
    "**REINFORCE Update**:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$$\n",
    "\n",
    "**Intuition**: Increase probability of actions with high returns!\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Please ensure:\n",
    "1. No extra print statements\n",
    "2. No extra code cells\n",
    "3. Function parameters unchanged\n",
    "4. No global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Policy Network](#2)\n",
    "    - [Exercise 1 - PolicyNetwork](#ex-1)\n",
    "- [3 - Action Sampling](#3)\n",
    "    - [Exercise 2 - select_action](#ex-2)\n",
    "- [4 - Policy Gradient Loss](#4)\n",
    "    - [Exercise 3 - compute_policy_loss](#ex-3)\n",
    "- [5 - Complete REINFORCE](#5)\n",
    "    - [Exercise 4 - train_reinforce](#ex-4)\n",
    "- [6 - Testing on CartPole](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from reinforce_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Policy Network\n",
    "\n",
    "The policy network $\\pi_\\theta(a|s)$ outputs a **probability distribution** over actions.\n",
    "\n",
    "**Architecture for CartPole**:\n",
    "```\n",
    "Input: State (4 values)\n",
    "Hidden: 128 neurons (ReLU)\n",
    "Output: Action probabilities (2 actions, Softmax)\n",
    "```\n",
    "\n",
    "**Key difference from DQN**: Output is probabilities, not Q-values!\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - PolicyNetwork\n",
    "\n",
    "Implement the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: PolicyNetwork\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network for REINFORCE.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 3-4 lines)\n",
    "        # Build network:\n",
    "        # 1. fc1: Linear(state_dim, hidden_dim)\n",
    "        # 2. fc2: Linear(hidden_dim, action_dim)\n",
    "        # Note: No softmax layer - we'll apply it in forward()\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass returns action probabilities.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        action_probs -- probability distribution over actions\n",
    "        \"\"\"\n",
    "        # (approx. 3-4 lines)\n",
    "        # 1. x = ReLU(fc1(state))\n",
    "        # 2. x = fc2(x)\n",
    "        # 3. action_probs = Softmax(x, dim=-1)\n",
    "        # Hint: Use F.relu() and F.softmax()\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "policy_net = PolicyNetwork(state_dim=4, action_dim=2)\n",
    "print(\"Policy Network:\")\n",
    "print(policy_net)\n",
    "\n",
    "test_state = torch.randn(1, 4)\n",
    "action_probs = policy_net(test_state)\n",
    "print(f\"\\nInput shape: {test_state.shape}\")\n",
    "print(f\"Output shape: {action_probs.shape}\")\n",
    "print(f\"Action probabilities: {action_probs.detach().numpy()}\")\n",
    "print(f\"Sum of probabilities: {action_probs.sum().item():.4f} (should be 1.0)\")\n",
    "\n",
    "policy_network_test(PolicyNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Action Sampling\n",
    "\n",
    "Given action probabilities, we need to:\n",
    "1. **Sample** an action from the distribution\n",
    "2. **Compute** log probability $\\log \\pi_\\theta(a|s)$ (needed for gradient)\n",
    "\n",
    "**Using PyTorch Categorical**:\n",
    "```python\n",
    "dist = Categorical(action_probs)\n",
    "action = dist.sample()  # Sample action\n",
    "log_prob = dist.log_prob(action)  # Get log probability\n",
    "```\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - select_action\n",
    "\n",
    "Sample action and get log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: select_action\n",
    "\n",
    "def select_action(policy_net, state):\n",
    "    \"\"\"\n",
    "    Select action from policy and return log probability.\n",
    "    \n",
    "    Arguments:\n",
    "    policy_net -- PolicyNetwork\n",
    "    state -- current state (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "    action -- sampled action (integer)\n",
    "    log_prob -- log probability of action\n",
    "    \"\"\"\n",
    "    # (approx. 5-7 lines)\n",
    "    # 1. Convert state to tensor and add batch dimension\n",
    "    # 2. Get action probabilities from policy_net\n",
    "    # 3. Create Categorical distribution\n",
    "    # 4. Sample action from distribution\n",
    "    # 5. Get log probability\n",
    "    # 6. Return action.item() and log_prob\n",
    "    \n",
    "    # Hint: Categorical is already imported\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "policy_net = PolicyNetwork(4, 2)\n",
    "test_state = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "action, log_prob = select_action(policy_net, test_state)\n",
    "print(f\"Sampled action: {action}\")\n",
    "print(f\"Log probability: {log_prob.item():.4f}\")\n",
    "print(f\"Action type: {type(action)}\")\n",
    "print(f\"Log prob requires grad: {log_prob.requires_grad}\")\n",
    "\n",
    "select_action_test(select_action, PolicyNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Policy Gradient Loss\n",
    "\n",
    "The REINFORCE update is:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t\\right]$$\n",
    "\n",
    "**PyTorch implementation**:\n",
    "```python\n",
    "loss = -sum(log_probs[t] * returns[t] for t in episode)\n",
    "```\n",
    "\n",
    "**Why negative?** PyTorch minimizes loss, we want to maximize reward!\n",
    "\n",
    "**Baseline (optional)**: Subtract mean to reduce variance\n",
    "$$loss = -\\sum_t \\log\\pi(a_t|s_t)(G_t - b)$$\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - compute_policy_loss\n",
    "\n",
    "Compute the policy gradient loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_policy_loss\n",
    "\n",
    "def compute_policy_loss(log_probs, returns):\n",
    "    \"\"\"\n",
    "    Compute policy gradient loss.\n",
    "    \n",
    "    Arguments:\n",
    "    log_probs -- list of log probabilities for each action\n",
    "    returns -- list of returns for each timestep\n",
    "    \n",
    "    Returns:\n",
    "    loss -- policy gradient loss (negative for gradient ascent)\n",
    "    \"\"\"\n",
    "    # (approx. 5-7 lines)\n",
    "    # 1. Convert returns to tensor\n",
    "    # 2. Normalize returns (subtract mean, divide by std + eps)\n",
    "    #    This is the baseline trick for variance reduction\n",
    "    # 3. Compute policy_loss = -sum(log_probs[t] * returns[t])\n",
    "    # 4. Return loss\n",
    "    \n",
    "    # Hint: torch.stack(log_probs) to convert list to tensor\n",
    "    # Hint: (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "test_log_probs = [torch.tensor(-0.5, requires_grad=True), \n",
    "                  torch.tensor(-0.3, requires_grad=True),\n",
    "                  torch.tensor(-0.7, requires_grad=True)]\n",
    "test_returns = torch.tensor([1.0, 2.0, 0.5])\n",
    "\n",
    "loss = compute_policy_loss(test_log_probs, test_returns)\n",
    "print(f\"Policy loss: {loss.item():.4f}\")\n",
    "print(f\"Loss requires grad: {loss.requires_grad}\")\n",
    "\n",
    "compute_policy_loss_test(compute_policy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Complete REINFORCE\n",
    "\n",
    "**REINFORCE Algorithm**:\n",
    "```\n",
    "Initialize policy network π_θ\n",
    "For each episode:\n",
    "    Generate episode using π_θ\n",
    "    For each timestep t:\n",
    "        Calculate return G_t\n",
    "    Compute loss = -Σ log π_θ(a_t|s_t) * G_t\n",
    "    Update θ using gradient descent on loss\n",
    "```\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - train_reinforce\n",
    "\n",
    "Implement complete REINFORCE training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_reinforce\n",
    "\n",
    "def train_reinforce(env, n_episodes=1000, lr=0.01, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train policy using REINFORCE.\n",
    "    \n",
    "    Returns:\n",
    "    policy_net -- trained policy network\n",
    "    rewards_history -- episode rewards\n",
    "    \"\"\"\n",
    "    # (approx. 25-30 lines)\n",
    "    # 1. Initialize policy network and optimizer\n",
    "    # 2. For each episode:\n",
    "    #    a. Reset environment\n",
    "    #    b. Generate episode:\n",
    "    #       - For each step:\n",
    "    #         * Select action using select_action\n",
    "    #         * Store log_prob and reward\n",
    "    #         * Take action in environment\n",
    "    #    c. Calculate returns (discounted cumulative rewards)\n",
    "    #    d. Compute loss using compute_policy_loss\n",
    "    #    e. Backprop and update policy\n",
    "    #    f. Store total reward\n",
    "    # 3. Return policy_net and rewards_history\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return policy_net, rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Testing on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"Training REINFORCE on CartPole...\\n\")\n",
    "policy_net, rewards = train_reinforce(env, n_episodes=1000, lr=0.01, gamma=0.99)\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Average reward (last 100): {np.mean(rewards[-100:]):.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, alpha=0.3, label='Episode reward')\n",
    "\n",
    "window = 20\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg,\n",
    "             label=f'Moving average ({window})', linewidth=2)\n",
    "\n",
    "plt.axhline(y=195, color='r', linestyle='--', label='Solved (195)', alpha=0.7)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('REINFORCE on CartPole')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've implemented REINFORCE, the foundation of policy gradient methods! Here's what you learned:\n",
    "\n",
    "✅ Policy networks that output action probabilities\n",
    "\n",
    "✅ Sampling actions from learned distributions\n",
    "\n",
    "✅ Policy gradient theorem and loss computation\n",
    "\n",
    "✅ Complete REINFORCE training loop\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Direct policy optimization**: No Q-function needed!\n",
    "2. **Stochastic policies**: Naturally handles exploration\n",
    "3. **High variance**: Returns vary a lot → slower convergence\n",
    "4. **Baseline trick**: Reduces variance without introducing bias\n",
    "5. **On-policy**: Must collect new data after each update\n",
    "\n",
    "### REINFORCE vs DQN:\n",
    "\n",
    "| Aspect | REINFORCE | DQN |\n",
    "|--------|-----------|-----|\n",
    "| Type | Policy-based | Value-based |\n",
    "| Output | π(a\\|s) | Q(s,a) |\n",
    "| Learning | Policy gradient | TD learning |\n",
    "| Exploration | Stochastic policy | ε-greedy |\n",
    "| Variance | High | Low |\n",
    "| Continuous actions | Easy | Hard |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Learn **Actor-Critic** (combines value and policy)\n",
    "- Explore **PPO** (more stable policy gradients)\n",
    "- Understand **advantage functions** A(s,a)\n",
    "- Try **continuous action spaces** (Gaussian policies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
