{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN - Interactive Exercise\n",
    "\n",
    "Welcome! In this notebook, you will implement **Dueling DQN**, an architectural improvement over standard DQN and Double DQN.\n",
    "\n",
    "## What is Dueling DQN?\n",
    "\n",
    "Dueling DQN (Wang et al., 2016) changes the **network architecture** to separately estimate:\n",
    "- **State Value** V(s): How good is this state?\n",
    "- **Advantage** A(s,a): How much better is each action?\n",
    "\n",
    "Then combines them to get Q(s,a)!\n",
    "\n",
    "## The Key Insight\n",
    "\n",
    "In many states, the **value of the state** matters more than **which specific action** you take.\n",
    "\n",
    "**Example** (Driving):\n",
    "- **Bad state**: About to crash â†’ V(s) very low, all actions are bad\n",
    "- **Good state**: Open highway â†’ V(s) high, most actions are similar\n",
    "- **Critical state**: Intersection â†’ V(s) moderate, action choice matters!\n",
    "\n",
    "Dueling DQN learns V(s) and A(s,a) separately, which helps in states where action choice doesn't matter much.\n",
    "\n",
    "## Architecture Comparison\n",
    "\n",
    "**Standard DQN**:\n",
    "```\n",
    "State â†’ FC â†’ FC â†’ Q(s,aâ‚), Q(s,aâ‚‚), ..., Q(s,aâ‚™)\n",
    "```\n",
    "\n",
    "**Dueling DQN**:\n",
    "```\n",
    "State â†’ Shared FC â†’ â”¬â”€â†’ Value Stream â†’ V(s)\n",
    "                     â””â”€â†’ Advantage Stream â†’ A(s,aâ‚), A(s,aâ‚‚), ..., A(s,aâ‚™)\n",
    "                     \n",
    "Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))\n",
    "```\n",
    "\n",
    "## Key Differences from DQN/Double DQN\n",
    "\n",
    "| Aspect | DQN/Double DQN | Dueling DQN |\n",
    "|--------|----------------|-------------|\n",
    "| Architecture | Single stream | **Dual stream** (V + A) |\n",
    "| Q-value Estimate | Direct | **Decomposed**: V(s) + A(s,a) |\n",
    "| Learning | All actions together | **Separate value and advantage** |\n",
    "| Performance | Good | **Better** (especially in sparse rewards) |\n",
    "| Generalization | Moderate | **Better** (V(s) shared across actions) |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the dueling architecture\n",
    "- Implement value and advantage streams\n",
    "- Combine them correctly to get Q-values\n",
    "- See why this decomposition helps learning\n",
    "- Compare Dueling DQN with standard DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from dueling_dqn_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment: CartPole\n",
    "\n",
    "We'll use CartPole-v1 to compare Dueling DQN with standard DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Dueling Q-Network Architecture\n",
    "\n",
    "The dueling architecture has three parts:\n",
    "1. **Shared layers**: Common feature extraction\n",
    "2. **Value stream**: Estimates V(s) â†’ single output\n",
    "3. **Advantage stream**: Estimates A(s,a) â†’ one output per action\n",
    "\n",
    "**Combining Formula** (with mean subtraction for identifiability):\n",
    "$$Q(s,a) = V(s) + \\left(A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a')\\right)$$\n",
    "\n",
    "Why subtract mean? To make V and A **identifiable** (unique). Without it, you could add a constant to V and subtract it from A without changing Q!\n",
    "\n",
    "**Task**: Implement the Dueling Q-Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DuelingQNetwork\n",
    "\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Dueling Q-Network with separate value and advantage streams.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        action_dim -- dimension of action space  \n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 6-7 lines)\n",
    "        # 1. Shared layers:\n",
    "        #    fc1: state_dim -> hidden_dim\n",
    "        # 2. Value stream:\n",
    "        #    value_fc: hidden_dim -> hidden_dim//2\n",
    "        #    value_out: hidden_dim//2 -> 1 (single value)\n",
    "        # 3. Advantage stream:\n",
    "        #    advantage_fc: hidden_dim -> hidden_dim//2\n",
    "        #    advantage_out: hidden_dim//2 -> action_dim\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through dueling architecture.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        q_values -- combined Q-values for each action\n",
    "        \"\"\"\n",
    "        # (approx. 12-15 lines)\n",
    "        # 1. Shared features:\n",
    "        #    x = F.relu(self.fc1(state))\n",
    "        # 2. Value stream:\n",
    "        #    v = F.relu(self.value_fc(x))\n",
    "        #    value = self.value_out(v)  # Shape: (batch, 1)\n",
    "        # 3. Advantage stream:\n",
    "        #    a = F.relu(self.advantage_fc(x))\n",
    "        #    advantages = self.advantage_out(a)  # Shape: (batch, action_dim)\n",
    "        # 4. Combine with mean subtraction:\n",
    "        #    q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        #    This ensures Q(s,a) = V(s) + A(s,a) - mean(A(s,:))\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "dueling_qnetwork_test(DuelingQNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Replay Buffer\n",
    "\n",
    "Same as DQN and Double DQN - we need experience replay.\n",
    "\n",
    "**Task**: Implement the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DuelingReplayBuffer\n",
    "\n",
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class DuelingReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        \"\"\"Experience replay buffer.\"\"\"\n",
    "        # (approx. 1 line)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        # (approx. 1 line)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        # (approx. 1 line)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "dueling_replay_buffer_test(DuelingReplayBuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute Dueling DQN Loss\n",
    "\n",
    "The loss function can use either:\n",
    "- **Standard DQN**: max over target network\n",
    "- **Double DQN**: action selection from online, evaluation from target\n",
    "\n",
    "We'll use **Double DQN style** (it's better!).\n",
    "\n",
    "The key difference from before: The network architecture is dueling, but the loss is the same!\n",
    "\n",
    "**Task**: Implement the loss function for Dueling DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_dueling_dqn_loss\n",
    "\n",
    "def compute_dueling_dqn_loss(batch, online_net, target_net, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute Dueling DQN loss (using Double DQN approach).\n",
    "    \n",
    "    Arguments:\n",
    "    batch -- list of Transition namedtuples\n",
    "    online_net -- online Dueling Q-network\n",
    "    target_net -- target Dueling Q-network\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    loss -- scalar loss value\n",
    "    \"\"\"\n",
    "    # (approx. 15-18 lines)\n",
    "    # This is the same as Double DQN loss, but using DuelingQNetwork!\n",
    "    # 1. Unpack batch into tensors\n",
    "    # 2. Get current Q-values: online_net(states).gather(1, actions)\n",
    "    # 3. Double DQN target:\n",
    "    #    a. Select actions: next_actions = online_net(next_states).argmax(1, keepdim=True)\n",
    "    #    b. Evaluate: next_q = target_net(next_states).gather(1, next_actions)\n",
    "    #    c. Target: reward + gamma * next_q * (1 - done)\n",
    "    # 4. Compute loss: F.mse_loss or F.smooth_l1_loss\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_dueling_dqn_loss_test(compute_dueling_dqn_loss, DuelingQNetwork, DuelingReplayBuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Update Target Network\n",
    "\n",
    "Same as Double DQN - periodically copy weights.\n",
    "\n",
    "**Task**: Implement target network update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_dueling_target\n",
    "\n",
    "def update_dueling_target(online_net, target_net):\n",
    "    \"\"\"\n",
    "    Copy weights from online to target network.\n",
    "    \n",
    "    Arguments:\n",
    "    online_net -- online Dueling Q-network\n",
    "    target_net -- target Dueling Q-network\n",
    "    \"\"\"\n",
    "    # (approx. 1 line)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "update_dueling_target_test(update_dueling_target, DuelingQNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Train Dueling DQN\n",
    "\n",
    "Training loop is the same as Double DQN, but using the dueling architecture!\n",
    "\n",
    "**Task**: Implement the complete training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_dueling_dqn\n",
    "\n",
    "def train_dueling_dqn(env, n_episodes=500, gamma=0.99, epsilon_start=1.0,\n",
    "                      epsilon_end=0.01, epsilon_decay=0.995, lr=1e-3,\n",
    "                      batch_size=64, target_update_freq=10):\n",
    "    \"\"\"\n",
    "    Train Dueling DQN on the environment.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gym environment\n",
    "    n_episodes -- number of episodes\n",
    "    gamma -- discount factor\n",
    "    epsilon_start -- initial epsilon\n",
    "    epsilon_end -- minimum epsilon\n",
    "    epsilon_decay -- epsilon decay rate\n",
    "    lr -- learning rate\n",
    "    batch_size -- batch size for training\n",
    "    target_update_freq -- update target network every N episodes\n",
    "    \n",
    "    Returns:\n",
    "    episode_rewards -- list of rewards per episode\n",
    "    online_net -- trained network\n",
    "    \"\"\"\n",
    "    # (approx. 35-40 lines)\n",
    "    # Same structure as Double DQN training:\n",
    "    # 1. Initialize online and target DuelingQNetwork\n",
    "    # 2. Initialize optimizer and replay buffer\n",
    "    # 3. For each episode:\n",
    "    #    a. Epsilon-greedy action selection\n",
    "    #    b. Store transitions\n",
    "    #    c. Sample and train when buffer is ready\n",
    "    #    d. Update target network periodically\n",
    "    #    e. Decay epsilon\n",
    "    #    f. Track rewards\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return episode_rewards, online_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "train_dueling_dqn_test(train_dueling_dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Run\n",
    "\n",
    "Let's train Dueling DQN and see the benefits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dueling DQN\n",
    "episode_rewards, trained_net = train_dueling_dqn(\n",
    "    env,\n",
    "    n_episodes=500,\n",
    "    gamma=0.99,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=10\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards, alpha=0.6)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Dueling DQN Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 100\n",
    "if len(episode_rewards) >= window:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.axhline(y=475, color='r', linestyle='--', label='Solved')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(f'Avg ({window} ep)')\n",
    "    plt.title('Moving Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(episode_rewards) >= 100:\n",
    "    final = np.mean(episode_rewards[-100:])\n",
    "    print(f\"\\n{'ðŸŽ‰ Solved!' if final >= 475 else 'ðŸ“Š Complete'} Final: {final:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Dueling DQN Works Better\n",
    "\n",
    "### 1. **Better Value Generalization**\n",
    "- V(s) is shared across all actions\n",
    "- Learning about state value helps estimate Q for all actions\n",
    "- Especially useful when some actions don't matter much\n",
    "\n",
    "### 2. **More Frequent Value Updates**\n",
    "- Every transition updates V(s), regardless of which action was taken\n",
    "- In standard DQN, only Q(s, a_taken) is updated\n",
    "- Dueling updates V(s) for **every** experience!\n",
    "\n",
    "### 3. **Better in Sparse Reward Environments**\n",
    "- Can learn V(s) even when actions don't lead to rewards\n",
    "- Helps with credit assignment\n",
    "\n",
    "### 4. **Identifiability Through Mean Subtraction**\n",
    "- Without mean subtraction: Q = V + A is not unique\n",
    "- With mean subtraction: Forces V to represent state value, A to represent relative advantage\n",
    "\n",
    "## Comparison: DQN vs Double DQN vs Dueling DQN\n",
    "\n",
    "| Aspect | DQN | Double DQN | Dueling DQN |\n",
    "|--------|-----|------------|-------------|\n",
    "| Architecture | Single stream | Single stream | **Dual stream** |\n",
    "| Q Estimation | Direct | Decoupled selection/eval | **Decomposed V + A** |\n",
    "| Overestimation | High | Reduced | Reduced |\n",
    "| Value Learning | Slow | Moderate | **Fast** (V shared) |\n",
    "| Sparse Rewards | Moderate | Moderate | **Good** |\n",
    "| Performance | Good | Better | **Best** |\n",
    "\n",
    "## Best Practice: Combine Them!\n",
    "\n",
    "**Dueling Double DQN** = Dueling architecture + Double DQN loss\n",
    "\n",
    "This is what we implemented here! It combines:\n",
    "- Dueling architecture â†’ better value learning\n",
    "- Double DQN loss â†’ reduced overestimation\n",
    "\n",
    "Result: **State-of-the-art value-based RL** (before Rainbow)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dueling Architecture\n",
    "\n",
    "Let's inspect what the network learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample state\n",
    "state, _ = env.reset()\n",
    "state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "# Forward pass through dueling network\n",
    "with torch.no_grad():\n",
    "    # Get Q-values\n",
    "    q_values = trained_net(state_tensor)\n",
    "    \n",
    "    # Also compute V and A separately for visualization\n",
    "    x = F.relu(trained_net.fc1(state_tensor))\n",
    "    \n",
    "    v = F.relu(trained_net.value_fc(x))\n",
    "    value = trained_net.value_out(v)\n",
    "    \n",
    "    a = F.relu(trained_net.advantage_fc(x))\n",
    "    advantages = trained_net.advantage_out(a)\n",
    "\n",
    "print(\"\\nDueling Architecture Decomposition:\")\n",
    "print(f\"State Value V(s): {value.item():.4f}\")\n",
    "print(f\"Advantages A(s,a): {advantages.numpy()[0]}\")\n",
    "print(f\"Final Q-values Q(s,a): {q_values.numpy()[0]}\")\n",
    "print(f\"\\nVerify: Q = V + (A - mean(A))\")\n",
    "print(f\"  V + (A - mean(A)) = {(value + (advantages - advantages.mean())).numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Dueling DQN! You now understand:\n",
    "- âœ… The dueling architecture with value and advantage streams\n",
    "- âœ… Why decomposing Q into V and A helps learning\n",
    "- âœ… The importance of mean subtraction for identifiability\n",
    "- âœ… How to combine Dueling with Double DQN\n",
    "- âœ… When and why Dueling DQN outperforms standard DQN\n",
    "\n",
    "**You've now mastered the core DQN family of algorithms!**\n",
    "\n",
    "**Next Steps**:\n",
    "- Implement **Rainbow DQN** (combines 6+ improvements)\n",
    "- Try Dueling DQN on Atari games\n",
    "- Explore **Noisy Nets** for better exploration\n",
    "- Read the original paper: \"Dueling Network Architectures for Deep RL\" (Wang et al., 2016)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
