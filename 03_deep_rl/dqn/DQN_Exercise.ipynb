{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "\n",
    "Welcome to the DQN assignment! This is where Reinforcement Learning meets Deep Learning. By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Understand why we need function approximation in RL\n",
    "* Implement a neural network for Q-value approximation\n",
    "* Build an experience replay buffer\n",
    "* Implement the DQN algorithm with target networks\n",
    "* Train an agent on environments with continuous state spaces\n",
    "\n",
    "## From Q-Learning to DQN\n",
    "\n",
    "**Problem with Q-Tables:**\n",
    "- CartPole has ~10^20 possible states (continuous)\n",
    "- Atari has 256^(84×84×4) ≈ 10^67,000 states!\n",
    "- Q-tables are impossible for large/continuous state spaces\n",
    "\n",
    "**Solution: Function Approximation**\n",
    "- Instead of a table, use a neural network: $Q(s,a; \\theta)$\n",
    "- The network learns to **approximate** Q-values\n",
    "- Can generalize to unseen states!\n",
    "\n",
    "## DQN Key Innovations\n",
    "\n",
    "1. **Neural Network**: Replace Q-table with neural network\n",
    "2. **Experience Replay**: Store and reuse past experiences\n",
    "3. **Target Network**: Stabilize training with separate target Q-network\n",
    "4. **Reward Clipping**: Normalize rewards for stability\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*w5GuxedZ9ivRYhQCv8kVZQ.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "## Important Note on Submission\n",
    "\n",
    "Please ensure:\n",
    "1. No extra print statements\n",
    "2. No extra code cells  \n",
    "3. Function parameters unchanged\n",
    "4. No global variables in graded functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Q-Network Architecture](#2)\n",
    "    - [Exercise 1 - build_q_network](#ex-1)\n",
    "- [3 - Experience Replay Buffer](#3)\n",
    "    - [Exercise 2 - ReplayBuffer](#ex-2)\n",
    "- [4 - DQN Training](#4)\n",
    "    - [Exercise 3 - compute_td_loss](#ex-3)\n",
    "    - [Exercise 4 - train_dqn](#ex-4)\n",
    "- [5 - Testing on CartPole](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from dqn_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Q-Network Architecture\n",
    "\n",
    "The Q-Network is a neural network that takes a state as input and outputs Q-values for each action.\n",
    "\n",
    "**Architecture for CartPole:**\n",
    "```\n",
    "Input: State (4 values) → [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "Hidden Layer 1: 128 neurons (ReLU)\n",
    "Hidden Layer 2: 128 neurons (ReLU)\n",
    "Output: Q-values (2 actions) → [Q(s, left), Q(s, right)]\n",
    "```\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - build_q_network\n",
    "\n",
    "Implement a Q-Network using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: QNetwork\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Network for DQN.\n",
    "    \n",
    "    Arguments:\n",
    "    state_dim -- dimension of state space\n",
    "    action_dim -- dimension of action space\n",
    "    hidden_dim -- number of neurons in hidden layers (default: 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 5-7 lines)\n",
    "        # Build a neural network with:\n",
    "        # 1. Input layer: state_dim\n",
    "        # 2. Hidden layer 1: Linear(state_dim, hidden_dim) + ReLU\n",
    "        # 3. Hidden layer 2: Linear(hidden_dim, hidden_dim) + ReLU\n",
    "        # 4. Output layer: Linear(hidden_dim, action_dim)\n",
    "        # \n",
    "        # Hint: Use nn.Sequential to combine layers\n",
    "        # Hint: ReLU activation: nn.ReLU()\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor of shape (batch_size, state_dim)\n",
    "        \n",
    "        Returns:\n",
    "        q_values -- Q-values for each action, shape (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        # (approx. 1 line)\n",
    "        # Pass state through the network\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "q_net = QNetwork(state_dim=4, action_dim=2, hidden_dim=128)\n",
    "print(\"Q-Network architecture:\")\n",
    "print(q_net)\n",
    "\n",
    "# Test forward pass\n",
    "test_state = torch.randn(1, 4)\n",
    "q_values = q_net(test_state)\n",
    "print(f\"\\nInput shape: {test_state.shape}\")\n",
    "print(f\"Output shape: {q_values.shape}\")\n",
    "print(f\"Q-values: {q_values.detach().numpy()}\")\n",
    "\n",
    "# Run the grader\n",
    "qnetwork_test(QNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Experience Replay Buffer\n",
    "\n",
    "**Why Experience Replay?**\n",
    "\n",
    "Problem with online learning:\n",
    "- Consecutive samples are highly correlated\n",
    "- Network can forget previous experiences (catastrophic forgetting)\n",
    "- Inefficient use of data\n",
    "\n",
    "**Solution: Replay Buffer**\n",
    "1. Store experiences (s, a, r, s', done) in a buffer\n",
    "2. Sample random mini-batches for training\n",
    "3. Breaks correlation, improves stability and sample efficiency\n",
    "\n",
    "```python\n",
    "# Store experience\n",
    "buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "# Sample mini-batch\n",
    "batch = buffer.sample(batch_size=32)\n",
    "```\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - ReplayBuffer\n",
    "\n",
    "Implement an experience replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experience tuple\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ReplayBuffer\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer.\n",
    "    \n",
    "    Arguments:\n",
    "    capacity -- maximum number of experiences to store\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        # (approx. 1 line)\n",
    "        # Use deque with maxlen=capacity to store experiences\n",
    "        # Hint: deque automatically removes oldest items when full\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add experience to buffer.\n",
    "        \"\"\"\n",
    "        # (approx. 1-2 lines)\n",
    "        # Create Experience tuple and append to buffer\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample random batch of experiences.\n",
    "        \n",
    "        Arguments:\n",
    "        batch_size -- number of experiences to sample\n",
    "        \n",
    "        Returns:\n",
    "        Tuple of batched (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        # (approx. 8-10 lines)\n",
    "        # 1. Randomly sample batch_size experiences from buffer\n",
    "        #    Hint: Use np.random.choice to sample indices\n",
    "        # 2. Extract and stack each component into numpy arrays\n",
    "        # 3. Return tuple of (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current size of buffer.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Add some experiences\n",
    "for i in range(10):\n",
    "    state = np.array([i, i+1, i+2, i+3])\n",
    "    action = i % 2\n",
    "    reward = float(i)\n",
    "    next_state = state + 1\n",
    "    done = (i == 9)\n",
    "    buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "\n",
    "# Sample a batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample(batch_size=3)\n",
    "print(f\"\\nSampled batch shapes:\")\n",
    "print(f\"States: {states.shape}\")\n",
    "print(f\"Actions: {actions.shape}\")\n",
    "print(f\"Rewards: {rewards.shape}\")\n",
    "\n",
    "# Run the grader\n",
    "replay_buffer_test(ReplayBuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - DQN Training\n",
    "\n",
    "DQN uses **two networks**:\n",
    "1. **Q-Network** (θ): Updated every step, used to select actions\n",
    "2. **Target Network** (θ⁻): Updated periodically, used to compute targets\n",
    "\n",
    "**Why Target Network?**\n",
    "- Prevents moving target problem\n",
    "- Stabilizes training\n",
    "- Updated every C steps: θ⁻ ← θ\n",
    "\n",
    "**DQN Loss Function:**\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s')\\sim Buffer}\\left[\\left(r + \\gamma \\max_{a'} Q(s',a'; \\theta^-) - Q(s,a;\\theta)\\right)^2\\right]$$\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - compute_td_loss\n",
    "\n",
    "Implement the DQN loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_td_loss\n",
    "\n",
    "def compute_td_loss(q_network, target_network, batch, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute TD loss for DQN.\n",
    "    \n",
    "    Arguments:\n",
    "    q_network -- current Q-network\n",
    "    target_network -- target Q-network  \n",
    "    batch -- tuple of (states, actions, rewards, next_states, dones)\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    loss -- mean squared TD error\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    \n",
    "    # Convert to tensors (approx. 5 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Compute Q-values (approx. 5-7 lines)\n",
    "    # 1. Get current Q-values: Q(s, a) for actions taken\n",
    "    #    Hint: Use gather() to select Q-values for specific actions\n",
    "    # 2. Get target Q-values: max Q(s', a') from target network\n",
    "    #    Hint: Use torch.no_grad() for target network\n",
    "    # 3. Compute TD targets: r + gamma * max Q(s', a') * (1 - done)\n",
    "    # 4. Compute loss: MSE between current Q and TD targets\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - train_dqn\n",
    "\n",
    "Now implement the complete DQN training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def select_action(q_network, state, epsilon, n_actions):\n",
    "    \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = q_network(state_tensor)\n",
    "            return q_values.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_dqn\n",
    "\n",
    "def train_dqn(env, n_episodes=500, batch_size=32, gamma=0.99,\n",
    "              epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "              lr=0.001, target_update=10, buffer_size=10000):\n",
    "    \"\"\"\n",
    "    Train DQN agent.\n",
    "    \n",
    "    Returns:\n",
    "    q_network -- trained Q-network\n",
    "    rewards_history -- list of episode rewards\n",
    "    \"\"\"\n",
    "    # Initialize (approx. 8 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    rewards_history = []\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Training loop (approx. 25-30 lines)\n",
    "    # For each episode:\n",
    "    #   1. Reset environment\n",
    "    #   2. For each step:\n",
    "    #      a. Select action using epsilon-greedy\n",
    "    #      b. Take action, observe reward and next_state\n",
    "    #      c. Store experience in buffer\n",
    "    #      d. If buffer has enough samples:\n",
    "    #         - Sample batch from buffer\n",
    "    #         - Compute loss\n",
    "    #         - Update Q-network\n",
    "    #      e. If done, break\n",
    "    #   3. Decay epsilon\n",
    "    #   4. Every target_update episodes, copy Q-network to target_network\n",
    "    #   5. Store episode reward\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return q_network, rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Testing on CartPole\n",
    "\n",
    "Let's train DQN on the classic CartPole environment!\n",
    "\n",
    "**Goal**: Balance a pole on a cart for as long as possible.\n",
    "- **State**: [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "- **Actions**: [push left, push right]\n",
    "- **Reward**: +1 for each timestep the pole stays up\n",
    "- **Success**: Average reward ≥ 195 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"Training DQN on CartPole...\")\n",
    "print(f\"State space: {env.observation_space.shape[0]}\")\n",
    "print(f\"Action space: {env.action_space.n}\")\n",
    "print(\"\\nThis may take a few minutes...\\n\")\n",
    "\n",
    "# Train\n",
    "q_network, rewards_history = train_dqn(\n",
    "    env,\n",
    "    n_episodes=500,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    lr=0.001,\n",
    "    target_update=10,\n",
    "    buffer_size=10000\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final average reward (last 100 eps): {np.mean(rewards_history[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(rewards_history, alpha=0.3, label='Episode reward')\n",
    "\n",
    "window = 20\n",
    "if len(rewards_history) >= window:\n",
    "    moving_avg = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards_history)), moving_avg,\n",
    "            label=f'Moving average ({window} episodes)', linewidth=2)\n",
    "\n",
    "ax.axhline(y=195, color='r', linestyle='--', label='Success threshold (195)', alpha=0.7)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('DQN Training on CartPole')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Deep Q-Network (DQN)! This is a major milestone in your RL journey.\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "✅ Function approximation with neural networks\n",
    "\n",
    "✅ Experience replay for stable training\n",
    "\n",
    "✅ Target networks to prevent moving targets\n",
    "\n",
    "✅ How to train agents on continuous state spaces\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Neural networks** can approximate Q-functions for large/continuous state spaces\n",
    "2. **Experience replay** breaks correlation and improves sample efficiency\n",
    "3. **Target networks** stabilize training by fixing targets temporarily\n",
    "4. **Hyperparameters matter**: learning rate, buffer size, target update frequency\n",
    "\n",
    "### DQN vs Q-Learning:\n",
    "\n",
    "| Aspect | Q-Learning | DQN |\n",
    "|--------|-----------|-----|\n",
    "| State space | Small, discrete | Large, continuous |\n",
    "| Q-function | Table | Neural network |\n",
    "| Update | Online | Replay buffer |\n",
    "| Stability | Stable | Needs target network |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try DQN on other environments (MountainCar, LunarLander)\n",
    "- Learn about improvements: Double DQN, Dueling DQN, Rainbow\n",
    "- Explore Policy Gradient methods (A2C, PPO)\n",
    "- Try DQN on Atari games!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
