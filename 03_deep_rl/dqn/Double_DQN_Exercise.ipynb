{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN - Interactive Exercise\n",
    "\n",
    "Welcome! In this notebook, you will implement **Double DQN**, an improvement over standard DQN that reduces overestimation of Q-values.\n",
    "\n",
    "## What is Double DQN?\n",
    "\n",
    "Standard DQN often **overestimates** action values due to the max operator in the Bellman equation. Double DQN fixes this by **decoupling action selection from action evaluation**.\n",
    "\n",
    "## The Overestimation Problem\n",
    "\n",
    "**Standard DQN** uses:\n",
    "$$Q_{target} = r + \\gamma \\max_{a'} Q_{target}(s', a')$$\n",
    "\n",
    "Problem: The **same network** selects AND evaluates the action, leading to overestimation.\n",
    "\n",
    "**Double DQN** uses:\n",
    "$$Q_{target} = r + \\gamma Q_{target}(s', \\arg\\max_{a'} Q_{online}(s', a'))$$\n",
    "\n",
    "Solution: **Online network** selects the action, **Target network** evaluates it.\n",
    "\n",
    "## Key Differences from DQN\n",
    "\n",
    "| Aspect | DQN | Double DQN |\n",
    "|--------|-----|------------|\n",
    "| Action Selection | Target network | **Online network** |\n",
    "| Action Evaluation | Target network | Target network |\n",
    "| Q-value Estimates | Overestimated | More accurate |\n",
    "| Implementation | Simple max | Two-step: argmax then index |\n",
    "| Performance | Good | **Better** (more stable) |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why DQN overestimates Q-values\n",
    "- Implement the Double DQN loss function\n",
    "- See the difference between DQN and Double DQN in practice\n",
    "- Learn when to use Double DQN over standard DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from double_dqn_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment: CartPole\n",
    "\n",
    "We'll use CartPole-v1 to compare DQN and Double DQN.\n",
    "\n",
    "- **State**: [position, velocity, angle, angular velocity]\n",
    "- **Actions**: 0 (left), 1 (right)\n",
    "- **Reward**: +1 per timestep\n",
    "- **Success**: Average reward > 475 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Q-Network\n",
    "\n",
    "The Q-network architecture is the same as standard DQN.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input (state) â†’ FC1 (128) â†’ ReLU â†’ FC2 (128) â†’ ReLU â†’ FC3 (action_dim) â†’ Q-values\n",
    "```\n",
    "\n",
    "**Task**: Implement the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: QNetwork\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Q-Network for Double DQN.\n",
    "        \n",
    "        Arguments:\n",
    "        state_dim -- dimension of state space\n",
    "        action_dim -- dimension of action space\n",
    "        hidden_dim -- number of hidden units\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        # (approx. 3 lines)\n",
    "        # Define three fully connected layers:\n",
    "        # fc1: state_dim -> hidden_dim\n",
    "        # fc2: hidden_dim -> hidden_dim\n",
    "        # fc3: hidden_dim -> action_dim\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass to get Q-values.\n",
    "        \n",
    "        Arguments:\n",
    "        state -- state tensor\n",
    "        \n",
    "        Returns:\n",
    "        q_values -- Q-value for each action\n",
    "        \"\"\"\n",
    "        # (approx. 4 lines)\n",
    "        # 1. Pass through fc1 and apply ReLU\n",
    "        # 2. Pass through fc2 and apply ReLU\n",
    "        # 3. Pass through fc3 (no activation)\n",
    "        # 4. Return Q-values\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "qnetwork_test(QNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Replay Buffer\n",
    "\n",
    "Experience replay is essential for stable DQN training. Same as standard DQN.\n",
    "\n",
    "**Task**: Implement the replay buffer with store and sample methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ReplayBuffer\n",
    "\n",
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        \"\"\"\n",
    "        Experience replay buffer.\n",
    "        \n",
    "        Arguments:\n",
    "        capacity -- maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        # (approx. 1 line)\n",
    "        # Initialize a deque with maxlen=capacity\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a transition.\n",
    "        \"\"\"\n",
    "        # (approx. 1 line)\n",
    "        # Append Transition namedtuple to buffer\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions.\n",
    "        \n",
    "        Arguments:\n",
    "        batch_size -- number of transitions to sample\n",
    "        \n",
    "        Returns:\n",
    "        batch -- list of Transition namedtuples\n",
    "        \"\"\"\n",
    "        # (approx. 1 line)\n",
    "        # Use random.sample to get batch_size transitions\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "replay_buffer_test(ReplayBuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Double DQN Loss\n",
    "\n",
    "This is where Double DQN differs from standard DQN!\n",
    "\n",
    "**Standard DQN**:\n",
    "```python\n",
    "target_q = reward + gamma * target_net(next_state).max(dim=1)[0]\n",
    "```\n",
    "\n",
    "**Double DQN**:\n",
    "```python\n",
    "# Step 1: Select action using online network\n",
    "best_actions = online_net(next_state).argmax(dim=1)\n",
    "\n",
    "# Step 2: Evaluate action using target network\n",
    "target_q = reward + gamma * target_net(next_state).gather(1, best_actions)\n",
    "```\n",
    "\n",
    "**Task**: Implement the Double DQN loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_double_dqn_loss\n",
    "\n",
    "def compute_double_dqn_loss(batch, online_net, target_net, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute Double DQN loss.\n",
    "    \n",
    "    Arguments:\n",
    "    batch -- list of Transition namedtuples\n",
    "    online_net -- online Q-network (being trained)\n",
    "    target_net -- target Q-network (for stability)\n",
    "    gamma -- discount factor\n",
    "    \n",
    "    Returns:\n",
    "    loss -- scalar loss value\n",
    "    \"\"\"\n",
    "    # (approx. 15-18 lines)\n",
    "    # 1. Unpack batch into separate tensors:\n",
    "    #    states, actions, rewards, next_states, dones\n",
    "    # 2. Get current Q-values: online_net(states).gather(1, actions)\n",
    "    # 3. Compute target Q-values (Double DQN way):\n",
    "    #    a. Get best actions from online network:\n",
    "    #       next_actions = online_net(next_states).argmax(dim=1, keepdim=True)\n",
    "    #    b. Evaluate these actions using target network:\n",
    "    #       next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "    #    c. Compute target: reward + gamma * next_q_values * (1 - done)\n",
    "    # 4. Compute loss: F.mse_loss or F.smooth_l1_loss\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "compute_double_dqn_loss_test(compute_double_dqn_loss, QNetwork, ReplayBuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Update Target Network\n",
    "\n",
    "Periodically copy weights from online network to target network for stability.\n",
    "\n",
    "**Task**: Implement target network update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_target_network\n",
    "\n",
    "def update_target_network(online_net, target_net):\n",
    "    \"\"\"\n",
    "    Copy weights from online network to target network.\n",
    "    \n",
    "    Arguments:\n",
    "    online_net -- online Q-network\n",
    "    target_net -- target Q-network\n",
    "    \"\"\"\n",
    "    # (approx. 1 line)\n",
    "    # Use load_state_dict to copy online_net parameters to target_net\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "update_target_network_test(update_target_network, QNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Train Double DQN\n",
    "\n",
    "Now let's put everything together!\n",
    "\n",
    "**Algorithm**:\n",
    "1. Initialize online and target networks\n",
    "2. For each episode:\n",
    "   - Reset environment\n",
    "   - For each step:\n",
    "     - Select action (epsilon-greedy)\n",
    "     - Take action, observe reward and next state\n",
    "     - Store transition in replay buffer\n",
    "     - Sample batch and compute Double DQN loss\n",
    "     - Update online network\n",
    "     - Periodically update target network\n",
    "     - Decay epsilon\n",
    "\n",
    "**Task**: Implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_double_dqn\n",
    "\n",
    "def train_double_dqn(env, n_episodes=500, gamma=0.99, epsilon_start=1.0, \n",
    "                     epsilon_end=0.01, epsilon_decay=0.995, lr=1e-3, \n",
    "                     batch_size=64, target_update_freq=10):\n",
    "    \"\"\"\n",
    "    Train Double DQN on the environment.\n",
    "    \n",
    "    Arguments:\n",
    "    env -- Gym environment\n",
    "    n_episodes -- number of episodes to train\n",
    "    gamma -- discount factor\n",
    "    epsilon_start -- initial epsilon for epsilon-greedy\n",
    "    epsilon_end -- minimum epsilon\n",
    "    epsilon_decay -- epsilon decay rate\n",
    "    lr -- learning rate\n",
    "    batch_size -- batch size for training\n",
    "    target_update_freq -- how often to update target network\n",
    "    \n",
    "    Returns:\n",
    "    episode_rewards -- list of total rewards per episode\n",
    "    online_net -- trained online network\n",
    "    \"\"\"\n",
    "    # (approx. 35-40 lines)\n",
    "    # 1. Initialize networks, optimizer, replay buffer\n",
    "    # 2. epsilon = epsilon_start\n",
    "    # 3. For each episode:\n",
    "    #    a. Reset environment\n",
    "    #    b. total_reward = 0\n",
    "    #    c. For each step:\n",
    "    #       - Epsilon-greedy action selection\n",
    "    #       - Take action, get next_state, reward, done\n",
    "    #       - Store in replay buffer\n",
    "    #       - If buffer has enough samples:\n",
    "    #         * Sample batch\n",
    "    #         * Compute loss using compute_double_dqn_loss\n",
    "    #         * Update online network\n",
    "    #       - Update state and total_reward\n",
    "    #       - If done: break\n",
    "    #    d. Update target network every target_update_freq episodes\n",
    "    #    e. Decay epsilon: epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    #    f. Append total_reward to episode_rewards\n",
    "    #    g. Print progress every 50 episodes\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return episode_rewards, online_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "train_double_dqn_test(train_double_dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Run\n",
    "\n",
    "Let's train Double DQN on CartPole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Double DQN\n",
    "episode_rewards, trained_net = train_double_dqn(\n",
    "    env, \n",
    "    n_episodes=500,\n",
    "    gamma=0.99,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=10\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards, alpha=0.6)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Double DQN Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 100\n",
    "if len(episode_rewards) >= window:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.axhline(y=475, color='r', linestyle='--', label='Solved threshold (475)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(f'Average Reward (last {window} episodes)')\n",
    "    plt.title('Moving Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if solved\n",
    "if len(episode_rewards) >= 100:\n",
    "    final_avg = np.mean(episode_rewards[-100:])\n",
    "    if final_avg >= 475:\n",
    "        print(f\"\\nðŸŽ‰ Environment solved! Final average: {final_avg:.2f}\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ“Š Training completed. Final average: {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Standard DQN vs Double DQN\n",
    "\n",
    "Let's visualize the key difference in Q-value estimation.\n",
    "\n",
    "**Overestimation Analysis**:\n",
    "- **Standard DQN**: Tends to overestimate Q-values, especially early in training\n",
    "- **Double DQN**: More conservative estimates, leading to more stable learning\n",
    "\n",
    "**When to use Double DQN**:\n",
    "- âœ… **Always** - It's almost always better than standard DQN\n",
    "- âœ… When you notice training instability in standard DQN\n",
    "- âœ… When you need more accurate value estimates\n",
    "- âœ… Environments where overestimation causes poor policies\n",
    "\n",
    "**Computational Cost**:\n",
    "- Virtually the same as standard DQN (just one extra argmax operation)\n",
    "- No additional memory requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "**Why Double DQN Works**:\n",
    "1. **Decoupling**: Separates action selection from action evaluation\n",
    "2. **Reduces bias**: Online network errors don't directly affect target values\n",
    "3. **More stable**: Prevents runaway overestimation\n",
    "\n",
    "**Implementation Tips**:\n",
    "- Start with Double DQN as your default (not standard DQN)\n",
    "- Combine with other improvements: Dueling DQN, Prioritized Replay\n",
    "- Monitor Q-values during training to check for overestimation\n",
    "\n",
    "**Further Improvements**:\n",
    "- **Dueling DQN**: Separate value and advantage streams\n",
    "- **Prioritized Experience Replay**: Sample important transitions more often\n",
    "- **Rainbow DQN**: Combine all improvements for state-of-the-art performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully implemented Double DQN! You now understand:\n",
    "- âœ… Why standard DQN overestimates Q-values\n",
    "- âœ… How Double DQN fixes this with action decoupling\n",
    "- âœ… The simple but powerful modification to the loss function\n",
    "- âœ… When and why to use Double DQN\n",
    "- âœ… How to combine it with other DQN improvements\n",
    "\n",
    "**Next Steps**: \n",
    "- Try **Dueling DQN** for better value estimation\n",
    "- Implement **Prioritized Experience Replay** for better sampling\n",
    "- Explore **Rainbow DQN** that combines 6+ improvements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
