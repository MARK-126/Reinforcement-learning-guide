# DQN y sus Variantes

Este directorio contiene implementaciones de Deep Q-Networks (DQN) y sus variantes m√°s importantes para Deep Reinforcement Learning.

## üìÅ Archivos Implementados

### 1. `dqn_basic.py` - DQN B√°sico
Implementaci√≥n del DQN original (Mnih et al., 2015).

**Caracter√≠sticas:**
- Q-Network con arquitectura MLP
- Experience Replay Buffer
- Target Network con actualizaciones peri√≥dicas
- Exploraci√≥n Œµ-greedy con decaimiento
- Ejemplo con CartPole-v1

**Componentes:**
- `DQN`: Red neuronal para Q-values
- `ReplayBuffer`: Buffer de experiencias
- `DQNAgent`: Agente con algoritmo DQN
- `train_dqn()`: Funci√≥n de entrenamiento
- `plot_training()`: Visualizaci√≥n de resultados

### 2. `double_dqn.py` - Double DQN ‚≠ê NUEVO
Implementaci√≥n de Double DQN (van Hasselt et al., 2015).

**Mejora clave:** Reduce la sobreestimaci√≥n de Q-values al separar la selecci√≥n y evaluaci√≥n de acciones.

**Diferencia con DQN est√°ndar:**
```python
# DQN est√°ndar
target = r + Œ≥ * max_a' Q_target(s', a')

# Double DQN
a* = argmax_a' Q_online(s', a')  # Seleccionar con online network
target = r + Œ≥ * Q_target(s', a*)  # Evaluar con target network
```

**Caracter√≠sticas:**
- Arquitectura id√©ntica a DQN b√°sico
- Double Q-learning update
- Soporte para hard y soft updates
- Compatibilidad con tau para actualizaci√≥n suave
- Ejemplos con CartPole y LunarLander

**Componentes:**
- `DQN`: Red neuronal (misma que b√°sico)
- `ReplayBuffer`: Buffer de experiencias
- `DoubleDQNAgent`: Agente con Double DQN
- `train_double_dqn()`: Entrenamiento
- `evaluate_agent()`: Evaluaci√≥n sin exploraci√≥n
- `plot_training()`: Visualizaci√≥n
- `compare_with_standard_dqn()`: Comparaci√≥n con DQN est√°ndar

**Cu√°ndo usar:**
- Cuando DQN est√°ndar sobreestima Q-values
- En ambientes con recompensas ruidosas
- Para aprendizaje m√°s estable
- Mismo costo computacional que DQN est√°ndar

### 3. `dueling_dqn.py` - Dueling DQN ‚≠ê NUEVO
Implementaci√≥n de Dueling DQN (Wang et al., 2016).

**Mejora clave:** Separa la estimaci√≥n del valor del estado de las ventajas de las acciones.

**Arquitectura:**
```
Estado ‚Üí Features (compartidas)
         ‚Üì
         ‚îú‚Üí Value Stream ‚Üí V(s)
         ‚îî‚Üí Advantage Stream ‚Üí A(s,a)

Q(s,a) = V(s) + (A(s,a) - mean_a(A(s,a)))
```

**Caracter√≠sticas:**
- Arquitectura Dueling con value y advantage streams
- Se puede combinar con Double DQN (recomendado)
- M√©todo `analyze_value_advantage()` para inspecci√≥n
- Soporte para hard y soft updates
- Ejemplos con CartPole y LunarLander

**Componentes:**
- `DuelingDQN`: Red neuronal con arquitectura Dueling
- `ReplayBuffer`: Buffer de experiencias
- `DuelingDQNAgent`: Agente con opci√≥n de Double DQN
- `train_dueling_dqn()`: Entrenamiento
- `evaluate_agent()`: Evaluaci√≥n con an√°lisis opcional
- `plot_training()`: Visualizaci√≥n de entrenamiento
- `visualize_value_advantage()`: An√°lisis de V(s) y A(s,a)

**Cu√°ndo usar:**
- Cuando el valor del estado es importante independiente de la acci√≥n
- En espacios de acci√≥n grandes
- Cuando muchas acciones tienen efectos similares
- Para mejor generalizaci√≥n
- Combinar con Double DQN para mejores resultados

## üöÄ Uso R√°pido

### Ejecutar ejemplos individuales

```bash
# DQN b√°sico en CartPole
cd 03_deep_rl/dqn
python dqn_basic.py

# Double DQN en LunarLander
python double_dqn.py

# Dueling DQN en LunarLander
python dueling_dqn.py
```

### Usar como librer√≠a

```python
# Importar desde el paquete
from dqn import DoubleDQNAgent, DuelingDQNAgent

# O importar directamente
from dqn.double_dqn import DoubleDQNAgent
from dqn.dueling_dqn import DuelingDQNAgent

# Crear ambiente
import gymnasium as gym
env = gym.make('LunarLander-v2')

# Double DQN
agent = DoubleDQNAgent(
    state_dim=8,
    action_dim=4,
    learning_rate=5e-4,
    gamma=0.99,
    epsilon_decay=1000,
    tau=0.005  # Soft update
)

# Dueling DQN + Double DQN
agent = DuelingDQNAgent(
    state_dim=8,
    action_dim=4,
    learning_rate=5e-4,
    gamma=0.99,
    use_double_dqn=True,  # Combinar con Double DQN
    tau=0.005
)

# Entrenar
from dqn.double_dqn import train_double_dqn
rewards, losses = train_double_dqn(env, agent, n_episodes=500)
```

## üìä Comparaci√≥n de Variantes

| Caracter√≠stica | DQN B√°sico | Double DQN | Dueling DQN |
|----------------|------------|------------|-------------|
| **Arquitectura** | MLP simple | MLP simple | Value + Advantage streams |
| **Update** | max Q_target | Decouple select/eval | Igual que Double |
| **Par√°metros** | ~10K | ~10K | ~12K |
| **Problema que resuelve** | Q-learning + DL | Sobreestimaci√≥n | Generalizaci√≥n |
| **Costo computacional** | Base | Igual | +20% |
| **Mejora t√≠pica** | Base | +10-15% | +15-25% |
| **Combinable** | - | Con Dueling | Con Double |

## üîß Caracter√≠sticas Comunes

Todas las implementaciones incluyen:

### Componentes
- ‚úÖ PyTorch neural networks
- ‚úÖ Experience replay buffer
- ‚úÖ Œµ-greedy exploration con decaimiento exponencial
- ‚úÖ Target network con hard/soft updates
- ‚úÖ Type hints completos
- ‚úÖ Docstrings en espa√±ol

### Funcionalidades
- ‚úÖ Training loops completos
- ‚úÖ Evaluaci√≥n sin exploraci√≥n
- ‚úÖ Visualizaci√≥n y tracking de m√©tricas
- ‚úÖ Save/load de modelos
- ‚úÖ `__main__` con ejemplos completos
- ‚úÖ Logging detallado

### Hiperpar√°metros Configurables
- `learning_rate`: Tasa de aprendizaje (1e-4 a 1e-3)
- `gamma`: Factor de descuento (0.95 a 0.99)
- `epsilon_start/end/decay`: Exploraci√≥n
- `buffer_size`: Tama√±o del replay buffer
- `batch_size`: Tama√±o del mini-batch
- `target_update`: Frecuencia de actualizaci√≥n (episodios)
- `tau`: Factor de soft update (None = hard update)
- `hidden_dim`: Dimensi√≥n de capas ocultas

## üéØ Configuraciones Recomendadas

### CartPole-v1 (simple)
```python
config = {
    'learning_rate': 1e-3,
    'gamma': 0.99,
    'epsilon_decay': 500,
    'buffer_size': 10000,
    'batch_size': 64,
    'target_update': 10,
    'tau': None,  # Hard update
    'hidden_dim': 128,
    'n_episodes': 300
}
```

### LunarLander-v2 (complejo)
```python
config = {
    'learning_rate': 5e-4,
    'gamma': 0.99,
    'epsilon_decay': 1000,
    'buffer_size': 50000,
    'batch_size': 128,
    'target_update': 5,
    'tau': 0.005,  # Soft update
    'hidden_dim': 256,
    'n_episodes': 600
}
```

## üìà Resultados Esperados

### CartPole-v1
- **DQN B√°sico**: ~200 reward en 200 episodios
- **Double DQN**: ~250 reward en 150 episodios
- **Dueling DQN**: ~300 reward en 100 episodios

### LunarLander-v2
- **DQN B√°sico**: ~150 reward en 500 episodios
- **Double DQN**: ~180 reward en 450 episodios
- **Dueling DQN + Double**: ~200 reward en 400 episodios

## üß™ Testing

Ejecutar el script de verificaci√≥n:

```bash
# Requiere: torch, gymnasium, numpy, matplotlib
python test_dqn_variants.py
```

Tests incluidos:
- ‚úì Arquitecturas de redes
- ‚úì Agentes y m√©todos
- ‚úì Training loops
- ‚úì Save/load de modelos
- ‚úì Comparaci√≥n de arquitecturas

## üìö Referencias

1. **DQN**: Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." Nature.

2. **Double DQN**: van Hasselt, H., Guez, A., & Silver, D. (2016). "Deep Reinforcement Learning with Double Q-learning." AAAI.

3. **Dueling DQN**: Wang, Z., et al. (2016). "Dueling Network Architectures for Deep Reinforcement Learning." ICML.

## üéì Conceptos Educativos

### Experience Replay
Almacena transiciones (s, a, r, s', done) y muestrea mini-batches aleatorios para:
- Romper correlaci√≥n temporal
- Reutilizar experiencias
- Estabilizar entrenamiento

### Target Network
Red separada para calcular targets, actualizada peri√≥dicamente:
- **Hard update**: Copia completa cada N episodios
- **Soft update**: Actualizaci√≥n gradual con œÑ cada step

### Œµ-greedy Exploration
Balance exploraci√≥n/explotaci√≥n:
- Œµ alto al inicio ‚Üí explorar
- Œµ bajo al final ‚Üí explotar
- Decaimiento exponencial

### Double Q-learning
Reduce sobreestimaci√≥n:
- Seleccionar: argmax sobre online network
- Evaluar: Q-value de target network

### Dueling Architecture
Separa valor y ventajas:
- V(s): Cu√°n bueno es el estado
- A(s,a): Ventaja relativa de cada acci√≥n
- Q(s,a) = V(s) + (A(s,a) - mean(A))

## üîÑ Pr√≥ximos Pasos

Para continuar aprendiendo Deep RL:

1. **Mejoras a DQN**:
   - Prioritized Experience Replay
   - Noisy Networks
   - Distributional RL (C51, QR-DQN)
   - Rainbow (combina todas las mejoras)

2. **Policy Gradient Methods**:
   - REINFORCE
   - Actor-Critic
   - A3C/A2C
   - PPO

3. **Continuous Control**:
   - DDPG
   - TD3
   - SAC

4. **Meta-Learning**:
   - MAML
   - Reptile
   - Model-Agnostic Meta-Learning

## üí° Tips de Uso

1. **Empezar simple**: Probar primero con CartPole antes de LunarLander
2. **Monitorear epsilon**: Asegurar que decae apropiadamente
3. **Tama√±o del buffer**: M√°s grande es mejor, pero usa m√°s memoria
4. **Learning rate**: Si diverge, reducir; si aprende lento, aumentar
5. **Combinar t√©cnicas**: Dueling + Double DQN suele funcionar mejor
6. **Guardar modelos**: Usar `save_every` para checkpoints regulares
7. **Visualizar**: Las gr√°ficas ayudan a detectar problemas

## ‚öôÔ∏è Dependencias

```bash
pip install torch gymnasium numpy matplotlib
```

Versiones recomendadas:
- Python >= 3.8
- PyTorch >= 2.0
- Gymnasium >= 0.28
- NumPy >= 1.20
- Matplotlib >= 3.5

---

**Nota**: Estas implementaciones est√°n dise√±adas para ser educativas y entender los fundamentos de Deep RL antes de pasar a meta-learning. El c√≥digo est√° bien documentado en espa√±ol con comentarios explicativos.
